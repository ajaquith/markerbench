<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Passwords-O-Plenty | Markerbench</title>

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Passwords-O-Plenty | Markerbench</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Passwords-O-Plenty" />
<meta name="author" content="arj" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Before the holidays I ran a quick, three-question, survey of the securitymetrics.org mailing list membership about the number of passwords people use. Here are the results, drawn from 51 responses (not bad, considering the list membership is about 400 people). I’d promised the respondents that I’d share the results… so here they are." />
<meta property="og:description" content="Before the holidays I ran a quick, three-question, survey of the securitymetrics.org mailing list membership about the number of passwords people use. Here are the results, drawn from 51 responses (not bad, considering the list membership is about 400 people). I’d promised the respondents that I’d share the results… so here they are." />
<link rel="canonical" href="http://localhost:4000/blog/2008/02/05/Passwords-O-Plenty/" />
<meta property="og:url" content="http://localhost:4000/blog/2008/02/05/Passwords-O-Plenty/" />
<meta property="og:site_name" content="Markerbench" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2008-02-05T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2008/02/05/Passwords-O-Plenty/"},"url":"http://localhost:4000/blog/2008/02/05/Passwords-O-Plenty/","headline":"Passwords-O-Plenty","dateModified":"2008-02-05T00:00:00-05:00","datePublished":"2008-02-05T00:00:00-05:00","author":{"@type":"Person","name":"arj"},"description":"Before the holidays I ran a quick, three-question, survey of the securitymetrics.org mailing list membership about the number of passwords people use. Here are the results, drawn from 51 responses (not bad, considering the list membership is about 400 people). I’d promised the respondents that I’d share the results… so here they are.","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"arj"},"@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" type="text/css" href="/assets/main-60e8a3a1cc9a058dfd2d2a8fd911e7caf7b6260ec5c339a66b9ae5f2f47d84df.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" type="text/css" href="/assets/theme-dd80c3f2c50689478db75a54a0209b0e499bde6cd8492f76ce2091fd807b5d36.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 

  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37546435-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>




</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Markerbench</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/lunr-19890ee2999a289dbc36809e25f7af7f038b633e23c41e143da1a507c83327d4.js" type="text/javascript"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made with by Sal @wowthemesnet. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-andrew-jaquith.html",
    "title": "Andrew Jaquith",
    "body": "                        {{page. title}} Follow:         {{ site. authors. arj. site }}         {{ site. authors. arj. bio }}                     {% asset  {{ site. authors. arj. avatar }}  alt= {{ site. authors. arj. name }}  class= rounded-circle  magick:resize=100x100 @optim %}              Posts by {{page. title}}:       {% assign posts = site. posts | where: author , arj  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                      {% asset  {{ author[1]. avatar }}  alt= {{ author[1]. name }}  class= rounded-circle  magick:resize=80x80 @optim %}                   {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                          &nbsp;       &nbsp;                                    {% endfor %}  "
    }, {
    "id": 4,
    "url": "http://localhost:4000/buy-me-a-coffee.html",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 5,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 6,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "http://localhost:4000/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 8,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 9,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 10,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 11,
    "url": "http://localhost:4000/page2/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 12,
    "url": "http://localhost:4000/page3/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 13,
    "url": "http://localhost:4000/page4/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 14,
    "url": "http://localhost:4000/page5/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 15,
    "url": "http://localhost:4000/page6/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 16,
    "url": "http://localhost:4000/page7/",
    "title": "Welcome",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div style= background-image: url({% asset  {{ latest_post. image }}  @path magick:resize=x200 @optim %}); height: 200px; background-size: cover; background-repeat: no-repeat; &gt;&lt;/div&gt;        {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         {% asset '{{ second_post. image }}' class='w-100' alt='{{ second_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         {% asset '{{ third_post. image }}' class='w-100' alt='{{ third_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                         {% asset '{{ fourth_post. image }}' class='w-100' alt='{{ fourth_post. title }}' magick:resize=200x @optim %}                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            &lt;div class= col-md-6 d-none d-md-block pr-0  style= background-size: cover; background-image: url({% asset  {{ post. image }}  @path %}) &gt;            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 17,
    "url": "http://localhost:4000/blog/2019/06/05/metrics-engineering/",
    "title": "SRE Metrics and Security Measurement",
    "body": "2019/06/05 - Why can’t IT and security get along better? Disciplined technology teams uses data and metrics strategically. But security and risk teams think about metrics differently than the rest of IT. In this post, I’ll describe how the discipline of Site Reliably Engineering (SRE) stresses the importance of service levels, discuss the slightly different terminology used by security and risk teams, and then suggest how using the language of SRE is can help security and risk teams to build bridges to technologists and across the business. Some background. For various reasons, I’ve been lately “sharpening the saw,” one of the seven habits Steven Covey described in his famous book. The saw that I am sharpening is The Cloud; more specifically, deepening my understanding of some of the core architectural patters that underpin modern cloud stacks such as Google and Amazon. One of Google’s well-known patterns is something called Site Reliability Engineering, or SRE for short. I had a pretty good idea of what SRE was about, but wanted a more formal indoctrination. Google folk have written a book on SRE. The chapter on Service Level Objectives is invigorating because it delivers an opinionated jolt about metrics, and their importance to reliability:  It’s impossible to manage a service correctly, let alone well, without understanding which behaviors really matter for that service and how to measure and evaluate those behaviors. To this end, we would like to define and deliver a given level of service to our users, whether they use an internal API or a public product.  We use intuition, experience, and an understanding of what users want to define service level indicators (SLIs), objectives (SLOs), and agreements (SLAs). These measurements describe basic properties of metrics that matter, what values we want those metrics to have, and how we’ll react if we can’t provide the expected service. Ultimately, choosing appropriate metrics helps to drive the right action if something goes wrong, and also gives an SRE team confidence that a service is healthy. Preaching to the converted! With a simple find-and-replace operation one could easily substitute security for service, and the result would closely capture the spirit of what I have been doing for the last five years inside several large financial institutions: namely, building measurement systems for security. In order to explain the parallels between security measurement and SRE metrics, I’ll first explain what I mean by “security measurement. ” Security measurement tend to measure two things: risks, using Key Risk Indicators, and controls, using Key Performance Indicators. Key Risk Indicators attempt to quantify risk. They are often produced by estimation, using a defined methodology such as FAIR, which produces dollars and cents. They can also be based on objective measurements that we know are good proxies for risk. For example, many compromises are the result of attackers using software exploits to gain access to unpatched systems. So, the “percentage of Internet-facing systems that are missing high-severity security patches” is a pretty good proxy for risk. “High severity,” in this case, means that the patch fixes a vulnerability that is remotely exploitable without needing the target to take any active steps, and is being actively exploited by bad guys right now. There is healthy debate in the security community about whether economic estimates or “proxy” performance indicators is a better KRI strategy. I have always preferred the latter approach (using selected KPIs as proxies) because they are simpler, and scale well. Key Performance Indicators, by contrast, measure the effectiveness of activities. There is a rich and varied set of definitions about what KPIs are, but in security, these are generally “control processes” that discover, constrain, or reduce risk. The controls are generally drawn from frameworks such as NIST Cyber-Security Framework, COSO, ISO 27001, PCI-DSS or other régimes. In the vulnerability example described above, the “control” activity involves patching vulnerable systems. Many companies that have developed patch KPIs express them as the “percentage of systems patched on time,” with different definitions of “on time” (the service level agreement) depending on the importance of each system, and the importance of the patch. Personally, I like three buckets: patch it now (72 hours or less), patch it soon (within 30 days), and patch it later (everything else), along with a glossary that describes in plain English what goes into each bucket. KPIs and KRIs can have thresholds — upper and lower bounds — that separate goodness from not-so-goodness, and not-so-goodness from badness. They are essentially the numeric boundaries between green, red, and yellow. The collective set of KRIs, considered together with their thresholds, can form the basis of an oganization’s risk appetite. One institution I am familiar with expresses its cyber-security risk appetite with a small set of well-defined, compact “proxy” indicators as KRIs. The institution has two thresholds for each KRI: an “internal limit” (yellow!) that triggers immediate remedial action, and a “board limit” (red!) that absolutely nobody wants to get outside of, because it requires a command performance in front of the board. With that background, let’s compare and contrast these security measurement concepts with Google’s SRE service level objectives. As quoted from Site Reliability Engineering:  Indicators. An SLI is a service level indicator — a carefully defined quantitative measure of some aspect of the level of service that is provided. Most services consider request latency — how long it takes to return a response to a request — as a key SLI. Other common SLIs include the error rate, often expressed as a fraction of all requests received, and system throughput, typically measured in requests per second. The measurements are often aggregated: i. e. , raw data is collected over a measurement window and then turned into a rate, average, or percentile… Ideally, the SLI directly measures a service level of interest, but sometimes only a proxy is available because the desired measure may be hard to obtain or interpret. For example, client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.  Objectives. An SLO is a service level objective: a target value or range of values for a service level that is measured by an SLI. A natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound. For example, we might decide that we will return Shakespeare search results “quickly,” adopting an SLO that our average search request latency should be less than 100 milliseconds… Choosing and publishing SLOs to users sets expectations about how a service will perform. This strategy can reduce unfounded complaints to service owners about, for example, the service being slow. Without an explicit SLO, users often develop their own beliefs about desired performance, which may be unrelated to the beliefs held by the people designing and operating the service.  Agreements. Finally, SLAs are service level agreements: an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain. The consequences are most easily recognized when they are financial — a rebate or a penalty — but they can take other forms. An easy way to tell the difference between an SLO and an SLA is to ask “what happens if the SLOs aren’t met?”: if there is no explicit consequence, then you are almost certainly looking at an SLO. As you can see, one can easily draw an analogy from security measurement to SRE service level metrics. Security KRIs and KPIs neatly map to Service Level Indicators. Security measurement thresholds (and what security teams often call “SLAs”) are very similar to Service Level Objectives. And while Service Level Agreements (as defined in SRE) don’t have a perfect analog, the arrangements many GRC teams make about what to do when their they hit their thresholds (for example, report to the board) are essentially agreements.       SRE concept   Security measurement concept         Service level indicators (SLIs)   Key Performance Indicators, Key Risk Indicators       Service level objectives (SLOs)   Indicator thresholds, SLAs       Service level agreements (SLAs)   Escalation standards, board limits   The concepts are close enough that many organizations may wish to adopt SRE terminology across the board, instead of clinging to special jargon that only the security and risk teams, and (regrettably) the regulators, speak. Why? I know this verges on heresy, but the advantages are plain enough.    Although “SRE” originated at Google, having explicit service level indicators, objectives and agreements is a modern twist on a classic IT operations discipline. It is well-understood by the business.     Because most technology organizations are several orders of magnitude larger than their corresponding security and risk teams, having common language increases the understanding each team has of the other’s mission.     Last: embracing SRE-aligned definitions of measurement helps clarify what security teams mean when they say “measurement. ” If you prefer that it refer explicitly to empiricial observations of “ground truth,” and not to other kinds of numbers — such as ordinal 1–5 scales or risk indices made using mystery math — the SRE philosophy should suit you just fine.  Bluntly: software is eating the world. Security and risk folks, you are surrounded by technologists. Why not use their language? Image: Google Pipes Datacenter, Jorge Jorquera via Flickr (CC BY-NC-ND 2. 0) "
    }, {
    "id": 18,
    "url": "http://localhost:4000/blog/2019/03/21/metricon-x-opening/",
    "title": "Five Things the Last Decade Taught Me About Security Metrics",
    "body": "2019/03/21 - This is the nominal text of my opening remarks for Metricon X, delivered on March 21, 2019. It has been lightly edited for clarity and a few identities have been slightly disguised. The views expressed in this speech do not necessarily reflect those of my present or past employers. WelcomeI appreciate everybody coming today. It’s a great turnout for a conference that we rather deliberately did not advertise. If you’re here, it’s because you wanted to be here. You’ve self-selected. The theme of the conference is “plus ça change…,” the second half of which is “plus c’est la même chose. ” Colloquially: “the more things change, the more they stay the same. ” So what we’re really here to talk about are the constants and the change. But because I suspect that we will have ample time to reheat some of the old chestnuts (the constants), I’d like to offer a few remarks on the changes — that is, notable happenings in the world of security metrics over the last 12 years. Data-driven security took rootOne of the most gratifying things to emerge in security over the last 10-plus years is the increased fluency and comfort people have with real security data. This is not completely new. Bill Cheswick’s work at Bell Labs in the late 1990s on network mapping, for example, helped create a company (Lumeta) that specialized in analyzing networks, and developed a specialty in analytics for use in M&amp;A situations. Jim Cowie, formerly CTO of Renesys, as another example, was doing large-scale analytics on BGP routes at the turn of the millennium. The last dozen years has brought many more examples, notably:  The Verizon Data Breach Investigations Report (DBIR), which fused together law-enforcement data and private sources to paint a data-rich picture of what data breaches look like, are caused by, and cost. The DBIR, and publications such as Larry Ponemon’s eponymous studies on breach costs, helped popularize a metric known as “cost per record. ” As a result, we now have relatively well-accepted currency for calculating potential and actual consumer information exposures.  Observables and ratings. Spurred on, in part, by the challenges of the the questionnaire-based approach to evaluating vendor security, vendors such as BitSight and Security Scorecard have focused on inferring the security of companies based on what they can empirically observe. If your MX and DNS records are messed up, or if spam is coming from IP address space you control, or if externally-facing systems appear to be compromised, then the rest of your security program probably isn’t any good either. Ratings are derived from how spotless one’s external presence is. Data about your supply base, for example, can help you make a decision about when need to dispatch the goon squad to interrogate a high-risk vendor.  The increased use of statistical and data science tools to analyze large security data sets. These include Python (eg PANDAS and NumPy), and the R ecosystem, the HadleyVerse and so on. There are a healthy number of “R-heads” in the security metrics community, such as Jay Jacobs, Bob Rudis and many others. I count myself among them. Although many of the studies are custom-made, the prevailing attitude is to practice reproducible science using a tool-driven analysis and workflow. Find interesting problems and data sets. Explore them. Publish findings. Repeat!And also, somewhere along the line, data science became a Thing. Some of us used to call it “statistics. ” Speaking of which… “AI” has come to security, with uneven results“AI” has come to security, with uneven results. I say “AI” in quotes because what we call AI in the popular press is not about endowing computing machines with cognition. I must tell you, every time I see that Microsoft commercial with the rapper Common extolling the virtues of “AI,” I feel like Marvin Minsky spins another turn in his grave, and that Douglas Hofstadter rips up and crumples one of his piano compositions and weeps. Once you get beyond the commercials, “AI” is primarily about creating models to make better predictions, using a bag of tricks that includes supervised and unsupervised learning, neural networks, bayesian strategies, Markov networks, bootstrapping, anomaly detection, and a whole set of other buzzwords that many of our attendees have better first-hand experience with than I. In security, many of these “AI” techniques are being put to use to help solve some very real operational security problems, for example, making a security operations team more efficient. Consider an enterprise-class SOC with dozens of analysts. The sensor grid will ingest daily log volumes in the tens of millions, extract tens of thousands of potentially suspicious activities, and then reduce these down to dozens of cases to put in front of human analysts. As a rule of thumb, it’s about roughly one million pieces of straw in every haystack, for each needle found in it. Financial services and national agencies are two types of organizations that have the threat volume, funding and organizational capability to fund vendor and internal efforts in this space. They have big haystacks and lots of needles to find. A large focus of research and vendor efforts is in increasing the signal-to-noise ratio. From a measurement perspective, this means using “AI” to correctly classify genuine intrusions (true positives) and non-intrusions (true negatives), and reduce the false-positive and false-negative rate. But results have been “uneven” because it’s a tough problem space. Many vendors will tell you that they’ve got bulletproof, universal techniques that solve all sorts of superficially related problems. For example, network intrusion detection and insurance fraud are both anomaly detection problems, right? I’ve heard a vendor say, “well, our AI/neural net/ML engine solves both of these problems. ” Actually, they are in different domains and have very different characteristics in terms of variety of data sources, completeness, and outlier detection strategies. There is no one size fits all. I’m inherently suspicious of generalizable AI in security. But every time I see a well-bounded, domain specific strategy, I’m happy. In addition, there is lots of low-hanging fruit that can be harvested by simply fusing data together at the presentation level to make investigations more efficient. SOC labor optimization is more like an operations research problem than an “AI” problem. With respect to making SOCs more efficient, there’s plenty of room for experimentation at both ends of the funnel, by attacking the top and middle of the funnel to present the truest and most accurate incidents; and then, improving the efficiency of the investigations of the cases that fall through to the bottom of the funnel. Success disasters are great teachersDr Dan Geer first introduced me to the concept of a “success disaster”; something that goes so well that it creates painful side-effects. Here in New York, you could argue that the cronut craze that began in 2013 was a success disaster for the Dominique Ansell Bakery. Sure, there were lines around of the block, but it led to a black market in resellable cronuts, counterfeit cronuts, quotas for cronuts, and I am sure, staff burnout and ingredient shortages. It was also a disaster for ordinary customers. If, for you, the Ansell Bakery had been a lovely place to have your morning French roast while leisurely enjoying a croissant, reading Le Monde and chain-smoking Galois cigarettes, it is no longer. That dream was trampled by all of the marauding tourists. In security metrics, it’s been gratifying to see a lot more focus on data, analytics and metrics. And many of the metrics I’ve been seeing are much better than the stuff that drove me batty when I wrote my book twelve years ago. You know, stuff like turning highs, mediums, lows into cardinal numbers like 5, 3, and 1, or (worse) 9, 3 and 1, and then doing math on them and claiming the results are “quanty. ” Or creating an “index” that uses mystery math to jam a bunch of semi-related indicators into a score that can’t be easily explained, on the theory that because the Dow Jones Industrial average is an index, and we all know that a higher means we’re richer, then our security metric needs to be an index too. These are mistakes anybody can make, and usually do when they start off. Many organizations have matured their thinking and have gotten religion about measuring things. At a bank I’m familiar with, for example, the GRC team produces a 100-page monthly pack of metrics that cover all areas of technology risk. Many of the metrics count things things that risk or control owners consider important, typically trailing indicators, often with breakdowns by organizational units, and almost always with commentary and correct attribution about sources. The 1,000 or so metrics in this pack are assiduously collected every month and assembled into a polished report. This is wonderful. It is a success. It is also a disaster, because the quantity of data is challenging to assimilate. It is challenging to see the forest for the trees. Here’s another success disaster: vulnerability management. Everybody in the audience knows what a vulnerability scan is, and what it does. It finds weaknesses and exposures in technology assets, typically on endpoints such as servers and desktops. The tools have gotten very good and produce few false positives. What’s more, there’s a general consensus on an industry-wide rating scheme for measuring severity: the Common Vulnerability Scoring System (CVSS). The market is mature, with well-established vendors such as Qualys, Rapid7 and Tenable. What’s not to like about vulnerability scanners? They have a consistent measurement system, are accurate and pervasive. If the scanner says something is bad, it must be right? We should fix all “critical” vulnerabilities right away, shouldn’t we? Sounds great. But the problem is that there are too many darned vulnerabilities: millions in the typical large enterprise. What do you fix first? This is very much a success disaster. These kinds of problems are excellent teachers, because they force you to think differently about the problems. In the vulnerability management space, for example, one must begin with the concession that not all vulnerabilities are cost-effective to fix. Some matter more than others. How important is the asset they are on? And is the vulnerability weaponized? Are attackers actively exploiting the vulnerability in the wild? Both of these are tedious and error-prone processes to do as one-offs, but can be attacked with a bit of engineering. So now you have vendors such as Kenna (founded by one of securitymetrics. org’s early members, Ed Bellis), applying logic over-the-top of the scanners you’re already using. Maybe you don’t need to fix 1 million vulnerabilities. Maybe this week, the only thing you worry about is the one-half of one percent of the vulns, or 5,000 patches relating to a single CVE that other companies are seeing abused by scripted attacks. That is a nice win, even better than the proverbial 80/20. For coping with success disasters in areas such as risk and control issues, I tend to worry less about the overall numbers of issues, and focus more on the pockets of risk “debt” that aren’t being paid down. Suppose you’ve got 10,000 risk issues and control breaks on the books, across the whole company. That sounds like a lot, but only 250 of them are in your highest-severity bracket. What’s the best way to figure out which ones to attack? There are many ways to look at the data — for example, finding who has the largest number of high-severity issues, or those with the largest number of longest-aged ones. Mean-time-to-close is another. Personally, I like “velocity” as the right way to look at the problem. Who’s paying down debt fastest, and who’s letting it sit? I stole a metric from the warehousing industry called “turnover,” which is defined as the number of SKUs flowing through a warehouse, divided into the average inventory.  For example, Apple’s inventory turnover in 2017 was 60, meaning it sold through everything in its warehouses every 6 days. When adapted for issue turnover, we define it as the number of closed issues divided into the average inventory. You don’t get credit for issues you postpone or renew. So for example, if you start with 100 issues on Feb 1, and end with 120 on Feb 28th, that’s bad, right? But what if you closed 65, and added 85? That’s pretty good, because you closed half of your issues during the month. Your issue turnover is 0. 5, or when expressed as an annualized figure means your inventory would turn over 6 times per year. That’s actually quite outstanding. Now imagine computing issue turnover by organizational unit and severity of issue. You’d see the high and low performers right away. This issue turnover metric works well because it is easy to understand and rewards the behaviors we want to see: paydown of issue debt. This is another example of how a success disaster causes us to evolve our thinking, and allows us to prioritize better. Controls instrumentation offers terrific bang for the buckWhen I joined a large investment bank as the MD for technology risk measurement and analytics, I was excited that I’d be able to put some of my ideas about security metrics into practice. I’d done a fair bit of metrics work on a smaller scale in prior roles, but the bank had both the commitment and the resources to do it properly. But what I found out quite quickly after coming in was that the primary use of “metrics” was in demonstrating controls conformance, chiefly for Sarbanes-Oxley and assurance régimes such as SSAE-18. Our biggest customer wasn’t the security organization — it was our external auditors. They needed our data to be able to show quantitatively that the key controls were working. Our second biggest customer was the finance organization, because they ran SOX, although they were less interested in the data than the results. The “sweet spot” for the continuous controls monitoring program was identity and authorization, which lies at the heart of technology risk management. “No privilege without identity. Approve all privileges. Remove them in a timely manner when roles change or someone leaves the firm. ” These were well-instrumented operational processes with well-defined systems to tap for the data. Because we calculated control effectiveness at a very granular level, we could state with confidence whether a particular control was effective or not. We had the data to prove it. No arguments. A key insight the team had was to being applying a similar approach to a large annual process that many of you are intimately familiar with, the Risk and Control Self-Assessment (RCSA, or as my contact from the Fed calls it, the “ricksa”). If you’ve had the pleasure of doing one, it’s usually an annual exercise that touches the entire enterprise. Both business-managed and control-function-managed controls are included. Everybody does it a little differently, but the basic steps are similar: (1) define “assessment units” that will perform the risk and control assessments; (2) set up the ratings scales for assessing inherent and residual risk; (3) have each assessment unit assess their inherent risk; (4) have each control owner assess the controls that help reduce these risks; (5) synthesize the results, calibrate them, determine residual risk and roll everything up. All of this sounds nice in theory, but the defects in practice are known.  Because so many people are involved, RCSAs can’t be done regularly; at most, most organizations will do them once a year.  Because the ratings are subjective, a lot of time is spent “calibrating” and “challenging” to try to ensure that nobody lied particularly egregiously. And, Because of time constraints and the lack of detailed empirical facts about the control environment, assessors must evaluate in a very coarse-grained way, perhaps, at a sub-line of business level at best. What this means is that a significant risk or control weakness affecting a particular asset is steamrollered over by the tyranny of averages. In short, these RCSA exercises aren’t timely, objective or precise. So what good are they? Based on comments from practitioners, not much good at all. And the regulators know it, which is why they are quite openly fishing for alternative approaches. What we found was that applying the continuous controls monitoring strategy to RCSA offered a terrific bang for the buck. The key was to do it in a commercial way. For example, consider Dorian’s wonderful Unified Compliance Framework, which offers a consistent and universal taxonomy of controls that can be mapped to every technology or cyber framework, regulation or statute. If you pick just three of these mandates, for example ISO 27000, the EU’s General Data Protection Regulation (GDPR) and the NIST Cyber-Security Framework, UCF will tell you that you need something like 600 controls, with another 300–400 implied. You would never want to automate the measurement of that number of controls. That would not be commercial, and you’d never be done. Instead, why not pick the 50 technology controls that we know from experience offer the biggest risk reduction potential, and instrument just those? We developed a playbook, which went more-or-less like this: “hey subject matter experts, we think change management, software lifecycle, data quality, tech ops, asset management, intrusion detection etc etc are the most important risk areas. How would you define ‘success’ in these areas? What metrics can we agree on that describe success? Who owns the data?” And then: defining a project plan for sourcing, loading, transforming and refining the data, in waves, so that we can compute the metrics we agreed constitute success. As a sweetener, we bribed the data owners with free labor to get their data into the computing plant. There are some caveats:  The data is never complete, but that’s ok, because it’s good enough to be indicative… and certainly better than “1-5 scales” that are based mostly on opinions leavened with a few facts.  The early results are always ugly, but that’s ok, because un-instrumented controls are always ugly the first time one sees the data. But nobody ought to get fired if the data’s all new and the control implementers haven’t been given time to fully adopt or get their performance in shape.  And it takes time, but that’s ok, especially if one sequences the plan to deliver quick wins firstIn short, having a rigorous plan to delivery incremental value of a small number of representative metrics makes assessment processes more timely, precise and objective. It’s important to keep the exercise limited to key controls that you can tangibly measure. And it is critical to keep reminding everybody about all of the cost and complexity that’s being removed — typically, millions of dollars of labor that is largely guesswork. Audience is everythingPeople want data for different reasons. And people consume data differently. What might seem good to you might be Greek to someone else. As a rule, I believe that when we build exhibits and reports, we tend to condescend to the reader. We assume that if we don’t lard exhibits with lots of reds, yellows, and greens, the person who is reading it won’t get it. Or we use simple pie and bar charts that waste space and are not data-dense. I ranted about this in my book a long time ago, but it’s still true. I rarely see information graphics related to security metrics that are more complicated than one-dimensional, for example, categorical data displayed as a bar chart. This is understandable in many ways, because most information graphs used in high-volume reports don’t need to do too much. They’re not there because they provide a lot of diagnostic power. They are meant to just get a simple message out. But is the message even right? If you don’t know who your audience is and what they want, it can’t possibly be — and so you are forced to keep it simple. If you knew your audience better, you could take them along much further, with more relevant and powerful metrics. When I look at published metrics and exhibits, I ask five questions that have a simple mnemonic: A-B-C-D-E.  A is for Audience. Do we know who we’re putting our metrics in front of? Do we know what they want? B is for Behaviors. If you’re looking at a chart of exhibit, what behaviors do I want the audience to change based on the inferences or conclusions in the data? C: can I Concisely and clearly communicate, in the simplest way possible, the data I that the audience will need to make… D: …the Decisions based on the data I put in front of them? E: Lastly, does my data include commentary with an Editorial voice that showcases my expertise and provides context to guide the audience to the decision?Because Audience is everything, you have to start there. That’s a key lesson I’ve learned personally over the last dozen years. Outside of the security field, I two relatively new disciplines have emerged as Things that people specialize in that relate to the question of Audience. The first is data visualization as a discrete field of study, and a sub-field related to information dashboard design. For data visualization (or “data vis”), toolsets such as Tableau, D3 and GGplot have turned visualization into a rich grammar that can be programmed, layered and reused. And websites like Information Is Beautiful and Flowing Data celebrate novel ways of mashing up and showcasing data. Stephen Few has been doing pathbreaking work on dashboard design — I can’t recommend his work highly enough, because of the rigor with which he approaches make-overs of the sorts of dashboards that we are all showing our bosses. As security and risk professionals, we all benefit from the increasing formalism of the field of data visualization, and from efforts to promote more “visualization literacy. ” Data journalism is the second Thing I’ve been following that benefits our field, and it too relates to Audience. Made mainstream by Nate Silver’s FiveThirtyEight election prediction work, nearly every premier news publication has invested in what is now called data journalism. Data journalists are either quants like Nate who happen to write persuasively, or data-curious journalists that got their Nerd on and developed a niche. The essence of data journalism is telling stories with data. Notable publications that are doing this really well include the New York Times, which has been doing some extraordinary data journalism over the last ten years; the Economist, which has always had excellent, honest, sound data graphics but has recently gone much deeper into analytics; and of course, the now-ESPN-owned fivethirtyeight. com.  And academics such as Alberto Cairo are also doing incredible work in this space. A few years ago I made a highly speculative hire — I hired the head of the data journalism team from a major business publication. The theory was, we’ve got lots of data, but we’re doing a crap job telling the story. Let’s see if we can bring in someone with a hybrid skillset. She writes well, and fast — is used to writing on deadline. As a reporter, she’s got a nose for the headline. And she’s got data chops. Maybe not like a full-on data scientist would, but hey, give it time. It turned out she was exactly what we needed. It was a true win-win… the bank got a massive upgrade in clarity and impact. And my new team member was happy as a clam because by making the jump into financial services, we were also able to raise her compensation by a very healthy amount. The point I’m trying to make here is that the skills that made our data journalist such a valuable member of the team was, more-or-less, ABCDE. In short: knowing your audience, what they want, and what you want out of them. And then, constructing the simplest and most efficient narrative that encourages inquiry, while also making setting the stage for decisions that shape behavior. This talk was meant as a retrospective, so I could have talked about any number of things. I mentioned these five trends…  data-driven security “AI” in security success disasters as teachers controls instrumentation audience focus…because they represented topics that I’ve learned a lot about, and that have benefited the industry. Thanks for listening to this rather old-school speech — no slides — and I look forward to seeing what Metricon XX will bring. "
    }, {
    "id": 19,
    "url": "http://localhost:4000/blog/2015/06/06/gartner-speech/",
    "title": "The Twenty-Year War on Cybercrime",
    "body": "2015/06/06 - This is the text of a speech I delivered at the Gartner Group Security and Risk Management Summit in June 2015. I originally wrote the speech for Sir Roger Carr, the Chairman of BAE Systems, to use at one of his public appearances. But it was too good not to re-use for myself as the BAE Applied Intelligence’ strategy lead. I felt no shame in doing so, seeing that I’d written it… IntroductionGood afternoon. Thank you for coming. It is a privilege to speak with you today. I’ve been asked to speak to you about digital crime: its rise, its significance, and what can be done about it. But I also know that I am the last thing between you and beer, so I will keep this talk as short and sweet as I can. Certainly, “cyber security” (I hate that phrase, but there we are) is a topic that can be treated lightly, and it is ambitious to try and cover the whole subject in 20 minutes. Nonetheless. I will discuss the rise of digital crime: how criminal enterprises, state-sponsored actors, and other parties are robbing the industrialized world of its secrets and personal information. I’ll discuss the impact that these activities have on businesses, citizens and governments. And I’ll discuss what can be done from our perspective as BAE Systems, one of the world’s largest defense contractors and providers of digital crime solutions. Introduce Self: But first, allow me to introduce myself and BAE Systems. I am the strategy officer for BAE Systems Applied Intelligence. I’m a recovering analyst; you might know me from, as they say on late-night TV, “another network,” in this case Forrester, where I covered data security and mobile security, and advised hundreds of enterprise clients on these topics, and on security strategy. I also wrote a fairly well-regarded book on security metrics called, funnily enough, “Security Metrics. ” Introduce BAE: Most of you probably know BAE Systems because of the work we do with the UK government and the Ministry of Defense. BAE’s role is to safeguard and enhance our customers’ vital interests. We have a robust defense business: we build aircraft such as the the Typhoon; we build, service and repair naval ships; we make land-based armaments, such as the Bradley Fighting Vehicle; and we are a key supplier to aerospace and defense companies worldwide. Most of you probably do not know that we have a billion-dollar risk and security business, which we call Applied Intelligence. We are probably the largest cyber-security company that you have never heard of. We have over 5,000 customers in many industries in three continents, with a concentration in financial services. We secure our customers’ intellectual property and their email; we detect fraud and reduce the cost of compliance; we help them identify and reduce their financial and reputational risks; we host their key collaboration services; and we monitor and defend their networks from intrusions. All of these activities give us a unique vantage point on the challenges of cyber-security, and on the problem of digital crime. The Rise of Digital CrimeFirst, let’s talk about the rise of digital crime: what it is, and what it means. When we speak about “digital crime” we mean the use of computers either as the main component of, or as an accessory to, criminal activities that result in financial gain or in competitive advantage. Broadly speaking, “digital crime” includes all dastardly deeds that span cyber-crime, financial crime, fraud, and insider activity. The common element is that unlike purely physical crimes — for example, pickpockets on a crowded subway car, these crimes rely on technology in some way. Increasingly, we see significant interplay between the different types of digital crime. Cyber is a key enabler of financial fraud, of healthcare fraud, and of the theft of industrial secrets. As reported by Scotland Yard in April 2014, seven out of ten financial fraud offenses involve cyber in some way. And because every part of society is becoming increasingly automated, instrumented, and network-connected, we expect that cyber will be involved in an increasingly large proportion of crimes over the next few years. Two types of threat actors: nation-states and criminal enterprises: Today, digital crime is perpetrated by two main types of actors: nation-states and criminal enterprises. Many of the most important cyber incidents that you have no doubt read about over the last five years have involved nation-states. These nation-states are engaged in state-sponsored hacking and industrial espionage on a grand scale. Two years ago, for example, US forensics firm Mandiant revealed that an elite hacking unit of the People’s Liberation Army was responsible for stealing industrial secrets from the U. S. defense industrial base, leading security software firms, and other businesses. More recently, North Korea stands accused of penetrating the networks of Sony Pictures to embarrass executives and steal intellectual property. The goal of these types of state-sponsored cyber activities is to obtain industrial secrets for sovereign advantage. The adversaries are advanced, persistent, and most certainly a threat. Criminal enterprises present a danger of a different sort. Their goals are to obtain what one might call “toxic data”: payment card details; personal health information; and personally identifying information, such as pension and other government identifiers. This information is fungible, and can be sold on black markets for profit, or to commit identity theft — at which point it is used for fraudulent financial purposes. Some examples. Last year, the U. S. retailer Target suffered from a data breach that caused the payment card details of over 40 million customers to be stolen, plus the personal details of over 70 million additional customers. And last month, the healthcare company Anthem was breached, exposing millions of healthcare records. A Bloomberg report suggested that the real target of the Anthem breach were the employees of its customers, which included Northrop Grumman and Boeing. Attackers were in effect using weaknesses found in Anthem’s defenses to get to these other companies. The advantages attackers have over defenders: Although both classes of attacker — state-sponsored actors and organized criminal enterprises — have different objectives, they have several things in common, which give them advantages over their targets, who must defend themselves:    First, both classes of adversary are supported by an integrated criminal supply chain. The supply chain is fully stratified, with loose networks of cyber weapons suppliers, middle-men, intermediaries, distributors, and 24 x 7 support providers. The wheels of this supply chain are helpfully greased by digital currencies such as BitCoin, which enable the anonymous exchange of funds between buyers and sellers.     Second: both classes of adversary are highly creative, willing to use all means at their disposal. These means include hacking, lying, fraud, identity theft, infiltration, and compromising trusted suppliers. They also include the use of any and all channels: phone, cyber, wi-fi, in-person and physical.     Third: both depend on the fact that their victims’ networks are increasingly far-flung, cloud based, and porous. With the advent of mobile, cloud, social networking, consumerization, and extended digital supply chains, companies must deal with exponentially more complexity in their networks than they did just ten years ago.  But it gets worse. You may not know that that the lingua franca of the Internet, the TCP and IP protocols, were never designed to be secure. They were designed to make the Internet resilient, to allow packets to flow to their destinations even when parts of the infrastructure were damaged. Every security protocol we have, was written — after the fact — to flow on top of those resilient, but insecure, protocols. Because security was never woven into the basic building blocks of the Internet, attackers inevitably find flaws in the ones we’ve fitted on top of them. Against such a backdrop, the adversary is always assured of asymmetric advantage. Defenders have to get it right all the time. Attackers, just once. To use a colloquial phrase, one might expect that for attackers, this should be rather like shooting fish in a barrel. And indeed it has been. The Impact of Digital CrimeThe impact of digital crime is significant no matter how one chooses to measure it.    The cost of digital crime begins with the direct costs; the cost of cleanup, notifications to customers, and fines. Target stores has spent almost $150 million cleaning up after their data breach. Heartland Payments Systems, a payment processor, was breached in 2008 and had over a hundred million payment card details stolen, with direct costs from the breach totaling nearly $150 million, only 30 million of which was covered by insurance. In general, industry analysts estimate that breaches of customer information can cost victims — companies and customers — millions of dollars. But the criminals nearly always make a mint: the gangs that broke into Target, for example, may have made over 675 million dollars of profit.     The cost of digital crime includes the damage to the victim’s reputation. A significant breach can cause significant personal embarrassment to executives and to customers. The co-chairman of Sony Pictures, for example, was forced resign last month because her company’s security was so poor. The CEO of Target stores resigned because of its hack. Security is indeed becoming a board level issue in the sense that people are getting fired because they don’t have enough of it.     The cost of digital crime includes changes in stock price and profits in the wake of a security breach, although these are usually temporary. Often overlooked are the inevitable class-action lawsuits that arise against public companies after data breaches. The management of Heartland Payment Systems has spent over five years defending itself against 27 separate consumer and institutional class-action lawsuits.     Finally, the cost of digital crime includes the loss of trust of one’s customers. Once lost, it is often difficult to regain. This is particularly challenging with firms that sell to other businesses. In the Heartland case, after years of growing its merchant base at double-digit rates between 10 and 20 percent, in the 2 years following the breach, merchant growth went into reverse, dropping 2%.  (more examples here…) These costs — direct costs, damage to reputation, stock price and profit drops, lawsuits, and loss of trust — are significant costs for any individual organization to bear. Taken in aggregate, the near-continuous stream of bad news leads to a gradual erosion of trust in digital business in general. What can be doneThe problems associated with digital crime are complex. So are the solutions, but that is in part because of the way we as customers, suppliers and national governments have been thinking about the problem of digital crime. We need to think differently. We need to think deeply. And we need to think quickly. Systems thinking, not silo thinking: First, we need to think about systems as a whole, and not about silos. To use an analogy, consider the West’s responses to various failed and successful hijacks of aircraft by terrorists. The first plots were revealed in 2006. A plot was foiled to detonate liquid explosives on 7 airplanes over the Atlantic. These explosives were peroxide-based and easily disguised in drinks bottles. After foiling the plot, the US and UK airline authorities duly banned bringing liquids through airport security. In September 2001, the 9/11 hijackers took control of airline cockpits using knives and box-cutters. Authorities duly prohibited knives and box-cutters on flights. Then, in December, show-bomber Richard Reid tried to set off a PETN-based bomb embedded in his shoe; the plot was foiled. Authorities duly forced passengers to remove their shoes. Security expert Bruce Schneier argues that none of these things have made any difference in minimizing the risk of hijackings. Only two things have: reinforcement of cockpit doors, and the fact that passengers are willing to fight back against attackers. Whether you agree with Bruce or not on this point, you can surely agree that the pattern used for preventing hijackings is “silo thinking”: looking for the artifacts used in the last attack and hoping that strategy will be effective in preventing the next one. Enumerating the things that are bad, rather than spotting the patterns that are bad. In cyber, we have been following a similar script. Consider the case of Target stores. Target suffered a horrendous breach; most people can appreciate the seriousness of that. What is less appreciated is that Target was compliant with the industry standard for security at the time of the hack: the Payment Card Industry’s Data Security Standard (PCI-DSS). By definition, Target owned and operated:  anti-virus software firewalls intrusion detection systems, and: log management software to filter through security device logs. All of these items are mandated by PCI-DSS and are required to be installed on systems that process cardholder data. In addition, the retailer also operated a security operations center in Minnesota. It had installed a $1. 5 million advanced malware detection system, FireEye, which did detect the malware that ultimately compromised its network. In short, Target could not possibly have been accused of skimping on security. What happened? Target’s failure came down to something fairly simple: the various silos of security did not talk to each other. Target’s advanced malware detection system saw the malware and created an alert. But the information was not acted on by Target’s staff. It was lost amidst the noise, or not presented in a relevant or timely way. Target did not arrive at the conclusions they needed to fast enough, which was not “you’ve got malware” but: “your point of sale systems are being taken over by a criminal enterprise. ” In short, Target’s tragedy was the failure to think of its data sources, individual security systems, directories, suppliers and point-of-sale terminals as a single, interconnected system, and to attach relevance and meaning to the patterns of behavior seen within it. A system, in the broad definition, is a set of connected technologies or processes that form a greater, more complex whole. Target thought it had a system in place, but it’s clear it only had silos: FireEye, the Bangalore team, the Security Operation Center in Minnesota, and many individual security technologies. When needed the most, they acted (or didn’t act) separately. When we rethink security, we must re-imagine security processes as an integrated whole. Systems thinking. To prevent and detect attacks, one must integrate all the elements — email, networks, physical, web, monitoring systems and many others. The components don’t all have to be from the same company, but they need to be integrated in such a way that the data flows seamlessly. Crucially, the information needs to be filtered and packaged so that it can be rapidly assessed, evaluated and acted on by human analysts. Getting the full picture of risk: Second, we need to think about the full picture of risk. Digital crime, particularly cyber crime, does not happen in a vacuum. Regardless of whether an attacker is trying to steal secrets, purloin personal information or launder lucre, nearly every type of digital crime can be reduced to a few common steps.  The attacker must plan his “campaign”: perform reconnaissance, communicate with confederates, collect insider information, create exploits, or infiltrate a network of people.  The attacker must commit his crime: break into a system, steal an identity, launch a denial of service attack, abuse administrator privileges, or use non-public information.  The attacker must harvest his gains: purchase or sell goods, make fraudulent claims, sell secrets, or launder money. Every method used in these steps generate some sort of tell-tale signal or artifact: a phone call, an entry in a log, a transaction, an intrusion alert, a payment or a sale. Appreciating the full picture of risk means having full knowledge, within the span of your control, of all of these artifacts. It means having the ability to sift through noise to find signal. It means acquiring, analyzing and acting on information at high speeds and at large scales. And it means having effective processes, technology and skills to spot anomalies, communicate them coherently, and act quickly. Scaling up: Third, we need to scale up. The problems of digital crime are complex, critical and costly. I will explain this by way of example. Much of the work that we are inspired to do by our customers are multi-billion-pound problems, for example:  First-party financial fraud costs institutions $18 billion a year globally Intellectual property stolen from U. S. firms costs $300 billion every year U. S. health care fraud costs insurers and the government nearly $75 billion annually, of which over $6 billion is cyber-related Tax fraud globally is estimated at 5% of the total global economy: over $300 billion in the US and over $100 billion here in the UKWhat unites these problems is that they are sufficiently large to escape the grasp of any one company, institution, or government. Effective approaches must necessarily be multi-company, industry-wide, and transnational in scope. For complex, critical and costly problems, only large-scale solutions will suffice. For example, here in the UK, we work with the Insurance Fraud Bureau. Software supplied by our Applied Intelligence unit analyzes every auto and property insurance claim submitted by every claimant in the country. This industry-wide capability has resulted in over 600 arrests and a large reduction in the amount of insurance fraud committed. This is not something that could work for a single insurer. This truly is a Big Data problem. Here in the United States, we are working with several state insurance agencies to reduce medical insurance fraud, again, as an industry-wide solution within each state. We provide essential network security services for nearly 15% of all American banking and credit union institutions. We monitor a quarter-million daily transactions processed by a New York-based clearing house, about $1. 2 billion worth of instruments every day. These are all examples of how having a multi-company, transnational vantage helps solve industry-wide problems. ConclusionsThe three strategies I’ve described — employing systems thinking, not silo thinking; getting the full picture of risk; and “scaling up” to span industries and international boundaries — are key to solving the complex, costly and critical problem of digital crime. But these items will not be sufficient in and of themselves. Because what we also need as businesses, as consumers and as society as a whole is a new mindset. The risk intelligence mindset: The mindset we need to adopt is a more informed, intelligent approach to thinking about and managing risk: “risk intelligence” if you like. Not every plan to protect the business will be perfect. It is impossible to imagine a world in which there is no fraud, no theft, and no successful cyber-attacks. BAE might well wish it could sell silver bullets in addition to the conventional kind, but silver bullets do not exist. What I mean by “risk intelligence” is that customers have enough information to act, even in conditions of uncertainty. I mean that when customers’ most well-considered security and risk plans fail, they can still act decisively, and can make decisions appropriate for their businesses. Customers need to be able to:  quickly acquire data about risks and threats at the highest level that could affect them and their customers; effectively analyze the data on hand to create information that can be put to use; and then: decisively act on that information to achieve better business outcomes: for example, reducing fraud, repelling cyber attacks, or rapidly responding to a break-in. Learning from John Boyd: There is a precedent for this type of thinking, and it comes courtesy of BAE’s main business, the military business. In the 1970s American military strategist Colonel John Boyd wrote about something called the “OODA loop,” which stands for Observe, Orient, Decide and Act. Boyd theorized that in combat conditions, one must:  Observe the enemy’s movements; Orient oneself by creating a mental picture of the situation; Decide on the courses of action available, and then: Act decisivelyBoyd believed that the combatant who can observe, orient, decide and act fastest would win the battle. This means achieving the clearest and most accurate conception of battlefield position, and then taking action, as fast as possible. Boyd also believed that a combatant who can observe, orient, decide and act faster can overwhelm his adversary’s decision-making capability, achieving victory in a fraction of the time required by conventional warfare. That was why Hitler’s blitzkrieg attacks were so effective. It is why the US-led Operation Desert Storm, for which Boyd was a key architect, was able to conquer Iraq — a country whose territory is nearly twice the size of the UK — in less than four days. It is also why digital crimes take days, months and years to detect. Adversaries are able to observe, orient, decide and act much more quickly than their victims. So, when we say that to properly combat digital crime, we need “risk intelligence,” we mean quickly acquiring data, effectively analyzing it, and decisively acting. In essence: speeding up customers’ own analytics and decision-making processes to match or exceed the speed of the adversary. Result: make customers’ jobs easier: Imagine a world where risk intelligence becomes the norm. Done right, our customers’ jobs become simpler. Today, the Chief Information Security Officer’s role in most organizations is to catalog all of the vulnerabilities in the environment; prioritize them; and then serially eliminate them one after the other. He or she buys many best-of-breed products to solve many narrow problems. Along the way, he or she writes policies that few people read, and some business unit owners actually regard as harmful. He or she spends valuable staff time answering hundreds of pesky audit questionnaires. That is the day job. The after-hours job is what happens when the company is actually compromised or subjected to fraud or attack. In these circumstances, the Security Officer scrambles, dodges, and weaves before making the best of a bad situation. Because policy is prioritized over speed of decision-making, the Security Officer is always caught by surprise. In future, the Chief Information Security Officer’s job will be measured not by the pound — that is, by the weight of policies produced and purchase orders placed. It will be measured instead by the tick — that is, by the number of ticks of the clock between when the adversary initially acts, and when he or she is able to acquire, analyze and act in response, or in advance of the adversary’s next move. Parting thought: I will close with a quote from Sir Winston Churchill:  “Want of foresight, unwillingness to act when action would be simple and effective, lack of clear thinking, confusion of counsel until the emergency comes, until self-preservation strikes its jarring gong - these are the features which constitute the endless repetition of history. ” Let us learn from history. Thank you for your time and attention today. "
    }, {
    "id": 20,
    "url": "http://localhost:4000/blog/2013/10/06/chef-3rd-course/",
    "title": "The DevOps Security Handbook&#58; Building Security In With Chef, Part III",
    "body": "2013/10/06 - IntroductionThis is the third in a series of occasional posts about security and DevOps. The ultimate goal of this series is to show how to build a reasonably secure Apache web server using the popular DevOps automation tool Chef. The server will be suitable for serving static content such as that generated by OctoPress. Each post explores a new aspect of Chef. If you read the first and second posts in this series, you learned how to set up the Chef workstation and server; created webserver and base roles; created a test environment and a virtual machine; and built a partially hardened server called tester. local. This server has a minimized Apache configuration, and a restricted OpenSSH configuration. In this post, I will demonstrate one of the most challenging aspects of any server automation project: copying sensitive keying materials, such as SSL private keys, to server nodes. Although SSL certificates themselves are not sensitive, certificate private keys are. In order to use Chef to truly “build security in,” these materials must be securely conveyed from the Chef server to the target server nodes. To do this, you will use Chef’s encrypted data bag feature and an add-on feature called chef-vault. You will create a custom cookbook recipe that performs all of the necessary decryption and file-creation actions on the target node. At the end of this post, you will possess a repeatable, reliable and secure method for conveying SSL keying materials or other secrets to target nodes. Generate self-signed SSL certificateTo use SSL with your webserver, you must have an SSL certificate. In production environments, you will likely use a certificate signed by a public certificate authority, such as VeriSign, Thawte, or GoDaddy. But you can also use a self-signed certificate, which you will do here. At the command line, change to your chef-repo/. chef directory. Type: openssl genrsa -aes128 -out tester. local. key-with-password 2048This creates a 2048-bit RSA key and wraps it with 128-bit key secured by a password. You should see output similar to the following (and will be prompted to enter a password): Generating RSA private key, 2048 bit long modulus. . . . . . . . +++. . +++e is 65537 (0x10001)Enter pass phrase for tester. local. key-with-password:Verifying - Enter pass phrase for tester. local. key-with-password:In most situations it isn’t desirable to have a password protecting the actual key file, because when you start Apache, it will block until the password is entered. If you have a large-scale infrastructure, you don’t want to type in passwords every time some random server starts up. (As a compensating control — later on in this post — you will make the key file accessible only to root. ) To ensure that Apache starts cleanly, let’s remove the password. Type: openssl rsa -in tester. local. key-with-password -out tester. local. keyEnter the password when prompted. A new key file tester. local. key will be created. Remove the old password-protected key file: rm tester. local. key-with-passwordNext, generate a certificate signing request (CSR). Type: openssl req -new -key tester. local. key -out tester. local. csrEnter data used in the certificate: country name, state, locality, organization name, OU name, common name, and email address, as shown in the sample output below: You are about to be asked to enter information that will be incorporatedinto your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '. ', the field will be left blank. -----Country Name (2 letter code) [AU]:USState or Province Name (full name) [Some-State]:MassachusettsLocality Name (eg, city) []:BostonOrganization Name (eg, company) [Internet Widgits Pty Ltd]:MarkerbenchOrganizational Unit Name (eg, section) []:SSL Test Certificate DirectorateCommon Name (e. g. server FQDN or YOUR name) []:tester. localEmail Address []:nobody@markerbench. comPlease enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:Most of the fields are mandatory, but only one really matters: the common name. This must match the name of the webserver host; in this case, tester. local. The CSR will be written to tester. local. csr. After it is created, create a self-signed certificate by typing: openssl x509 -req -days 365 -in tester. local. csr -signkey tester. local. key -out tester. local. pemYou will see this output: Signature oksubject=/C=US/ST=Massachusetts/L=Boston/O=Markerbench/OU=SSL Test Certificate Directorate/CN=tester. local/emailAddress=nobody@markerbench. comGetting Private keyThe certificate will be written to tester. local. pem. You can verify the certificate contents by using OpenSSL’s x509 command: openssl x509 -in tester. local. crt -noout -textYou should see output similar to this: Certificate:Data:  Version: 1 (0x0)  Serial Number:    90:aa:b2:e4:06:ca:50:32  Signature Algorithm: sha1WithRSAEncryption  Issuer: C=US, ST=Massachusetts, L=Boston, O=Markerbench, OU=SSL Test Certificate Directorate, CN=tester. local/emailAddress=nobody@markerbench. com  Validity    Not Before: Oct 5 21:05:32 2013 GMT    Not After : Oct 5 21:05:32 2014 GMT  Subject: C=US, ST=Massachusetts, L=Boston, O=Markerbench, OU=SSL Test Certificate Directorate, CN=tester. local/emailAddress=nobody@markerbench. com  Subject Public Key Info:    Public Key Algorithm: rsaEncryption    RSA Public Key: (2048 bit)      Modulus (2048 bit):        00:c4:3b:79:55:78:15:c2:82:a6:e3:e9:f0:64:c7:        …(content omitted)        56:e1:57:0d:b0:e0:37:31:19:ee:31:95:8f:2f:a6:        c3:3b      Exponent: 65537 (0x10001)Signature Algorithm: sha1WithRSAEncryption  1e:a6:1a:27:d3:5d:08:bc:ad:00:df:4e:6a:5b:4c:a4:be:80:  …(content omitted)  23:0e:02:be:3e:e8:89:75:58:03:7d:70:ac:13:a3:f4:d5:02:  2e:d8:58:7fCongratulations. You have created a self-signed certificate called tester. local. pem in the local directory, and a corresponding private key file tester. local. key. Installing Chef-vault for distributing secretsWith the certificate and private key created, the next challenge is to use Chef to copy these two files to the correct locations on the web server. SSL certificates are stored at /etc/ssl/certs; private keys are stored at /etc/ssl/private. There are many ways to convey the SSL certificate and key to the webserver. The easiest way would be to include these files in a cookbook as file resources, then use a recipe to copy them to the correct locations on the server. That is easy, but not very secure because the keying materials would be stored as part of the cookbook, and therefore in the clear. (The Git source tree contains your cookbooks and all of their supporting files. If the SSL private key were checked in as an unencrypted regular file, other people would be able to see it. ) It would be much nicer to store the SSL private key in an encrypted form — somehow — so that it can be copied to the server without worrying about who sees it (an attacker would just see ciphertext). After it is copied the server note, it can be decrypted in-place and moved to the correct destination in /etc/ssl/private. Chef has some tools that make conveying secret materials easier. It provides a construct called a data bag for storing custom configuration items and other materials that target nodes need. Data bags are essentially hash maps (otherwise known as associative arrays, or in Rubyspeak, “hashes”) that are stored on the Chef server and retrieved by target nodes when chef-client runs. Data bags items can be encrypted. To encrypt a data bag item, you pass an symmetric encryption key (or password) to the knife data bag create command. For example: knife data bag create certs tester_local_key --secret-file /tmp/my_secret_key…where my_secret_key is a secret key generated, for example, by OpenSSL. To decrypt the item on target nodes, the target nodes perform an equivalent decryption operation, passing in the same secret key used to originally encrypt the item. Go back and re-read that last paragraph. See anything problematic? Nodes that need to decrypt the data bag item need the the secret key used to encrypt it. That might sound obvious, but the it raises a question: how does the secret key actually get copied to the nodes? The Chef documentation is silent about how this is done; it leaves the problem of key management as an exercise for the reader. We are left to assume that the secret key is “somehow” copied to the target nodes. How should this be done? You could copy the secret key using a Chef recipe, but you’d have the same problem all over again — sensitive keying material would be exposed. You could, instead, copy the secret key manually to the node over SSH, but that defeats the whole point of Chef — automation of configuration tasks. It would be much nicer if there was a way to encrypt sensitive materials without requiring lots of complicated key management. Sadly, Chef does not provide such a method. However, a clever programmer named Kevin Moser, who works for Nordstrom, has created Chef plugin called chef-vault that solves the key management problem rather elegantly. Kevin’s chef-vault tool takes a clever approach. Chef-vault uses a type of key encapsulation to protect secret materials using the public keys of target nodes that need them. These public keys are the same ones the nodes use to authenticate to Chef. Because these keys must, by definition, already exist, using them for encryption creates no extra work for you. Essentially, Chef-vault’s encrypt operation does the following:  Creates a symmetric encryption key (“secret key”). This secret key encrypts the plaintext (the thing you want to encrypt), and creates a ciphertext.  Adds the ciphertext to the data bag.  For each target node, encrypts the secret key with the node’s public key, creating an encapsulated key blob. The same operation is repeated for authorized users using their public keys as well Adds each encapsulated key blob to the data bagThe result of Chef-vault’s encrypt operation is a data bag that contains an encrypted item for the secret being protected, and, for each authorized user and target node, an encrypted data blob that allows each user or node (and only that user or node) to recover the encryption key and, thus, decrypt the encrypted item. Chef-vault (essentially) extends Chef’s data bag structures to use its own public-key encryption system so that secret keys can be conveyed securely to target nodes. This rather neatly solves the secret-key distribution problem. Enough talk. Let’s install chef-vault. Normally, you would use the Ruby command gem install chef-vault to install it. However, as of this writing, only chef-vault version 2. 1 has the ability to encrypt entire files. That ability is missing in the version of chef-vault in the Gem repositories. So you must build it yourself using the latest version from Github. At the command line, change to a directory outside of your chef-repo directory. Type: git clone https://github. com/Nordstrom/chef-vault. gitYou will see output similar to this: Cloning into 'chef-vault'. . . remote: Counting objects: 667, done. remote: Compressing objects: 100% (343/343), done. remote: Total 667 (delta 324), reused 619 (delta 279)Receiving objects: 100% (667/667), 107. 30 KiB | 0 bytes/s, done. Resolving deltas: 100% (324/324), done. Change to the new chef-vault directory and build the Gem: cd chef-vaultgem build chef-vault. gemspecYou will see output similar to this: WARNING: no homepage specifiedWARNING: description and summary are identical Successfully built RubyGem Name: chef-vault Version: 2. 1. 0 File: chef-vault-2. 1. 0. gemInstall chef-vault by typing: gem install chef-vault-2. 1. 0. gemRuby will report successful installation as follows: Successfully installed chef-vault-2. 1. 01 gem installedInstalling ri documentation for chef-vault-2. 1. 0. . . Installing RDoc documentation for chef-vault-2. 1. 0. . . Creating an encrypted vault for the SSL certificate and keyWith chef-vault installed, you can now use it to encrypt sensitive materials and convey them securely to nodes. As an example, let’s encrypt the SSL certificate file. Change back to the chef-repo directory. Type the following, where username is your Chef username: knife encrypt create certs tester_local_pem --mode client --file . chef/tester. local. pem -A  username This Knife encrypt command tells chef-vault to encrypt the contents of file . chef/tester. local. pem (the SSL certificate) and to authorize the user username to decrypt or update its contents. You can use any valid Chef username, or multiple usernames separated by commas. (If you need to find out your Chef username is, type knife user list. ) The contents of the encrypt operation are added to a vault named certs. The vault is backed by a data bag with the same name. You can verify that the data bag certs exists by typing: knife data bag listYou will see the data bag certs in the list. You can see the items added to the data bag via the knife data bag show command. Type: knife data bag show certsYou will see the following items: tester_local_pemtester_local_pem_keysThe first item, tester_local_pem is a hash that contains the encrypted contents of the file. The second item, tester_local_pem_keys, is a hash containing the list of authorized nodes, users and their associated public-key-encrypted blobs. Take a look at the encrypted file contents. The command for viewing data bag items is knife data bag show name-of-data-bag name-of-item. Type: knife data bag show certs tester_local_pemYou should see output similar to the following: file-content: cipher:     aes-256-cbc encrypted_data: QpB63Qv2650jwmWfj3IX4iXAIoGz8WZkggoV+wbLyI0T4nUivD5QBovdjtJU YkhI9QOrbW55HVwew7tLW+ee0cetjZm+Amaa0Gyo8ehBsTbRAeY3jkdWv8Ia … (content omitted) … jGAUa+xcdDedmBSiRxoUwrjSq85hnAGwmKovXqKZeK4=  iv:       kOrZ5kIrTCmwRloUodCtgA==  version:    1file-name: cipher:     aes-256-cbc encrypted_data: kEL5rHzmx85diXKC1AL7EXdEID+SC1E58GuNFBeu9lK1k+Bv5GbcQXK/iDtS L8tQ  iv:       xEhV676bjE4SwVYZZwkFtw==  version:    1id:      tester_local_pemAs you can see, the file_content hash key contains the a child hash containing the cipher (AES-256-CBC), initialization vector, and the encrypted data blob. The file-name hash key contains similar data that corresponds to the original file name (which is also encrypted). Let’s look at the encryption keys. Type: knife data bag show certs tester_local_pem_keysYou should see output similar to the following: admins: arjarj:   SDqZuaFrpy28YOSDDhkyDmDBLPZHuRSDXjOgHklnaetDjl8QI7zuTvznmg1Q…(content omitted)…f+s7gdSVBZ0el7Uc9gDhOFZA0hz0ADqcIPd2hA90PQ==clients:id:   tester_local_pem_keysHere, the admins entry’s value is arj, indicating that the user arj is authorized to decrypt or update the contents. The arj entry contains the secret key, encrypted with arj’s public key. Of course, instead of seeing arj you will see your own username. Note that the clients entry is empty because no nodes are authorized to decrypt yet. You can decrypt the secret by using the knife decrypt command. Type: knife decrypt certs tester_local_pem file-content --mode clientThis command decrypts the payload stored in the key path tester_local_pem &gt; file-content in data bag certs. Because your Chef user is an authorized user, you should be able to see the decrypted content. It starts with the string -----BEGIN CERTIFICATE-----. Compare the output to the contents of the file . chef/tester. local. pem; the contents should be identical. At this point, only one party (your Chef user) is authorized to decrypt the certificate. Of course, the web server needs to be authorized also! To authorize more users or nodes after the original knife encrypt create operation, use the knife encrypt update command. In this case, you want to authorize the tester. local node that will actually use the SSL certificate. Type: knife encrypt update certs tester_local_pem --mode client --file . chef/tester. local. pem -S  name:tester. local You can examine the contents of the data bag by typing knife data bag show certs tester_local_pem again. If you do that, you will see that the contents of the data bag item are much the same as before, although the initialization vector file-content &gt; iv and encrypted data blocks encrypted_data are different. That is because the vault has re-encrypted the contents with different keys. Examine the tester_local_pem_keys entry. Type: knife data bag show certs tester_local_pem_keysYou will see that the contents of this entry are now a little different: admins:    arjarj:     SDqZuaFrpy28YOSDDhkyDmDBLPZHuRSDXjOgHklnaetDjl8QI7zuTvznmg1Q…(content omitted)f+s7gdSVBZ0el7Uc9gDhOFZA0hz0ADqcIPd2hA90PQ==clients:   tester. localid:      tester_local_pem_keystester. local: y7DM7oQZj9+Yd5oRLFA4eSVOZ/+g/NYUNjfMJvsxxd1Nv85yLigzjb1JlaYm…(content omitted)yYWtFXX47765NivPGNTszfJQ8igNNBy1+YvfQn/wNw==You can see that the clients entry contains the value tester. local, and that a corresponding encrypted data blob named tester. local has been added. Splendid! With the certificate correctly added to the vault, let’s add the private key. Instead of doing a two-step process of creating the encrypted data items and then authorizing the node, let’s do it in one step by supplying both the user and node to the create operation. Type: knife encrypt create certs tester_local_key --mode client --file . chef/tester. local. key -A  arj  -S  name:tester. local Substitute your own username instead of arj, of course. If you want to, you can verify that the SSL private key was added successfully by typing the now-familiar knife data bag show certs tester_local_key command. Creating a cookbook for configuring SSLAt this point you have added the SSL certificate and its corresponding private key to the encrypted data vault certs. Now you need to get the vault’s contents over to the target nodes so you can create the certificate and private key files. First, create a new cookbook called ssl-config: knife cookbook create ssl-configAdd the new cookbook to the webserver role so that it is executed whenever chef-client runs: knife role edit webserverAdd the recipe for ssl-config to the role by editing the run_list as follows:  run_list : [  recipe[apt] ,  recipe[apache2] ,  recipe[ssl-config] ],Next, edit the default recipe file ssl-config/recipes/default. rb as follows: chef_gem 'chef-vault'require 'chef-vault'directory '/etc/ssl/certs' do recursive true owner 'root' group 'root' mode '0755'enddirectory '/etc/ssl/private' do owner 'root' group 'root' mode '0700'end# Certificate entries equal to hostname but with _ replaced by . vault     = 'certs'hostname   = node['fqdn']cert_prefix  = hostname. sub('. ','_')cert_cert   =  #{cert_prefix}_pem cert_key   =  #{cert_prefix}_key cert_chain  =  #{cert_prefix}_chain puts  Creating certificates for #{hostname} using vault #{vault}.  # Decrypt certificateputs  Decrypting certificate from hash item #{cert_cert}.  begin item = ChefVault::Item. load(vault,cert_cert) file  /etc/ssl/certs/ssl-cert-snakeoil. pem  do  owner 'root'  group 'root'  mode '0444'  content item['file-content'] endrescue ChefVault::Exceptions::KeysNotFound raise ChefVault::Exceptions::ItemNotFound,   Certificate not found at #{vault}/#{cert_cert}! end# Decrypt certificate chainputs  Decrypting certificate chain.  begin item = ChefVault::Item. load(vault,cert_chain) file  /etc/ssl/certs/#{hostname}. chain  do  owner 'root'  group 'root'  mode '0444'  content item['file-content'] endrescue ChefVault::Exceptions::KeysNotFound Chef::Log. warn( No certificate chain in #{vault}/#{cert_chain}.  )end# Decrypt private keyputs  Decrypting key from hash item #{cert_key}.  begin item = ChefVault::Item. load(vault,cert_key) file  /etc/ssl/private/ssl-cert-snakeoil. key  do  owner 'root'  group 'root'  mode '0400'  content item['file-content'] endrescue ChefVault::Exceptions::KeysNotFound raise ChefVault::Exceptions::ItemNotFound,   Private key not found at #{vault}/#{cert_key}! end# Configure the SSL default site if enabledapache_site  default-ssl  do enable node['apache']['default_site_enabled']endThere might appear to be a lot going on in this recipe, but it is actually quite simple. First, the chef_gem and require lines tell the target node’s Chef client to download the chef-vault Gem. The directory '/etc/ssl/certs' do block creates the directory that should contain the SSL certificate /etc/ssl/certs if it does not already exist. Directory ownership is changed to root and it is made world-readable. The directory '/etc/ssl/private' do block creates the directory that should contain the SSL private key /etc/ssl/private if it does not already exist. Directory ownership is changed to root and it is made readable only by root. The next part of the recipe assigns variables used for looking up and decrypting the node’s certificate, private key and certificate chain. The key names for these items are equal to the fully-qualified domain name of the node with periods escaped as underscores, plus the _pem, _key, and _chain suffixes, respectively. For example, for your test VM tester. local these values are tester_local_pem, tester_local_key, and tester_local_chain. (In case you were wondering: that is why the knife encrypt create commands you typed earlier created items named tester_local_pem and tester_local_key. ) The next three code blocks (each beginning with the comment # Decrypt) actually decrypt the file contents and save them to files. Let’s look the first of these. In the first decryption block, the line ChefVault::Item. load(vault,cert_cert) decrypts the certificate object and assigns the result to the variable item. The value of item will be a hash. The next 6 lines that begin with file  /etc/ssl/certs/ssl-cert-snakeoil. pem  do create the certificate file, assign ownership to root, make it world-readable, and set the contents to item’s hash entry named file-content. Note that all of this code is enclosed in a begin/rescue/end block, so that the ChefVault::Exceptions::KeysNotFound exception can be trapped. ChefVault::Item. load throws this exception if the vault does not contain the expected entry, in this case one whose key is tester_local_pem. If the entry is not found (for example, because you forgot to add the certificate to the vault), the recipe will throw and exception and fail — as it should. The second decryption block decrypts and saves the certificate chain, if one was added to the vault. Because tester. local’s SSL certificate was self-signed, it does not need a certificate chain. However, in production situations you might have one, and if you do, you can ensure that it is copied to the server by adding it to vault using the usual knife encrypt create command and specifying an item named nodename_chain, where nodename is the escaped form of the fully-qualified domain name (periods replaced by underscores). Unlike the first decryption block, however, the recipe does not crash and burn if the certificate chain item is not found. Instead, the recipe simply warns that no chain was found. The third decryption block decrypts and saves the private key. As with the first decryption block, the recipe fails if the key’s expected entry is not found in the vault. The last block turns on the default-ssl site in Apache, which is preconfigured to use the various ssl_snakeoil certificate files and private keys. The ssl-config recipe is fairly bare-bones, but sufficiently flexible that it will work with any SSL-enabled web server node. As discussed above, all you must do is (1) ensure that the node’s SSL certificate and private key are added to the vault correctly, and (2) configure the node’s run-list so that it executes the ssl-config recipe. Copying SSL certificates to the serverWith all of the prep work out of the way, it is time to finally configure the server. Upload the ssl-config cookbook to the Chef server: knife cookbook upload ssl-configSSH into tester. local: vagrant sshOnce in, run chef-client: sudo suchef-clientMany console messages will scroll past you at a dizzying pace. Look for these lines: Recipe: ssl-config::default…Creating certificates for tester. local using vault certs. Decrypting certificate from hash item tester_local_pem. Decrypting certificate chain. [2013-10-06T23:29:50+00:00] WARN: No certificate chain in certs/tester_local_chain. Decrypting key from hash item tester_local_key. These indicate that the recipe worked as expected. If there is a problem finding or decrypting the certificate or private key, the output will show an exception. Assuming the recipe ran successfully, the output will also contain lines showing that the certificate and private key files were created also. Look for lines similar to these, which shows the certificate file was created: - create new file /etc/ssl/certs/ssl-cert-snakeoil. pem- update content in file /etc/ssl/certs/ssl-cert-snakeoil. pem from none to 53f4ae  --- /etc/ssl/certs/ssl-cert-snakeoil. pem	2013-10-06 23:29:51. 663590528 +0000  +++ /tmp/. ssl-cert-snakeoil. pem20131006-9127-hmw8o1	2013-10-06 23:29:51. 667592528 +0000  @@ -0,0 +1,23 @@  +-----BEGIN CERTIFICATE-----…and these, which shows the private key file was created: - create new file /etc/ssl/private/ssl-cert-snakeoil. key- update content in file /etc/ssl/private/ssl-cert-snakeoil. key from none to 60fcbe  --- /etc/ssl/private/ssl-cert-snakeoil. key	2013-10-06 23:29:51. 727622527 +0000  +++ /tmp/. ssl-cert-snakeoil. key20131006-9127-m9p4zk	2013-10-06 23:29:51. 731624527 +0000  @@ -0,0 +1,27 @@  +-----BEGIN RSA PRIVATE KEY-----After the recipe runs, you can verify the files were correctly created by cat-ing the files /etc/ssl/certs/ssl-cert-snakeoil. pem and /etc/ssl/private/ssl-cert-snakeoil. key. The files should be owned by root/root; permissions should be restricted to 444 and 400, respectively. Testing the webserverTo test that the webserver is working as it should, we need to do two more things: edit the webserver role to enable SSL and the default site. Then, we re-push the cookbook and restart the server. First, edit the role as follows using the usual command knife role edit webserver. As shown below, add SSL as an enabled module by adding  ssl , to the default_modules array, and turn set the default_site_enabled value to true:  override_attributes : {   apache : {    allow_override :  None ,    contact :  nobody@example. com ,    default_modules : [     alias ,     cgi ,     deflate ,     dir ,     log_config ,     logio ,     mime ,     rewrite ,     ssl ,     setenvif    ],    default_site_enabled : true,. . . . Also, enable port 443 in the listen_ports section:     listen_ports : [     80 ,     443    ],On tester. local, run chef-client as root again and watch the node converge using these new settings. Then, open your browser to tester. local using regular HTTP. You should see a page that screams It works!. Try using HTTPS; you should see the same message (and likely after getting an SSL warning about an untrusted certificate). Save your workYou are done. Back up your nodes, roles, data bags and environments from the Chef server to your local workstation. Type: knife backup exportYou will see the following output: Backing up nodesBacking up nodes tester. localBacking up rolesBacking up roles baseBacking up roles webserverBacking up data bagsBacking up data bag certs item tester_local_keyBacking up data bag certs item tester_local_key_keysBacking up data bag certs item tester_local_pemBacking up data bag certs item tester_local_pem_keysBacking up environmentsBacking up environments testingNext, edit . gitignore in your chef-repo directory so that your SSL certificate and private key are not stored in Git. Add this line somewhere near the top (for example, underneath the line . chef/*. pem): . chef/*. keyFinally, commit your work. You can see what files were modified with the usual command git status; if you do, you will see that some new files have been added: . chef/chef_server_backup/data_bags/certs/cookbooks/ssl-config/You will see that a few have also been modified. Commit everything: git commit -am  DevOps Secuity Handbook Part 3 Remember, the keying materials (the *. key and *. pem files in the . chef directory) are not versioned in your Git repository. This is both a feature and a bug. You can safely move the tester. local. pem and tester. local. key files to offline media now, if you wish; they are safely encrypted in the data bag certs and no longer need to be in the local filesystem. Next: Adding custom contentIf you have completed the instructions in this post, you learned how to do some very useful things. You created a self-signed SSL certificate and private key for tester. local. You installed the chef-vault plugin for storing the SSL certificate and private key as encrypted data bag items. You authorized the user arj and node tester. local to decrypt these items. And you created a cookbook that decrypts the certificate, private key and certificate chain and creates files in the correct locations on the server. In the next post, you will use Chef to configure Apache for serving custom content. You will create a non-privileged user whose home directory stores static HTML. This directory will be served up by Apache as the default website. In keeping with the SSH configuration introduced in this post, the user account will be configured to use SSH public keys for authentication rather than passwords. This post was updated July 22, 2015 to change the naming convention for SSL certificate files on the target box. It also added a short section that enables the default normal and SSL sites, as well as a short section for testing the actual SSL configuration. Image copyright 2016 by Kharnagy, licensed under the Creative Commons Attribution-Share Alike 4. 0 International license. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/blog/2013/10/03/chef-2nd-course/",
    "title": "The DevOps Security Handbook&#58; Building Security In With Chef, Part II",
    "body": "2013/10/03 - IntroductionThis is the second in a series of occasional posts about security and DevOps. The ultimate goal of this series is to show how to build a reasonably secure Apache web server using the popular DevOps automation tool Chef. The server I am describing how to build will be suitable for serving static content. Readers of this blog know that I am a fan of static blogging tools like Octopress, which I use to generate this website. If you read the first post in this series, you learned how to set up the Chef workstation and server account. You created an Apache server role and a test environment; set up a virtual machine; and built your first node. In this post, I will show you how to create a new role called base that includes security enhancements to OpenSSH. You will also fine-tune Apache to remove non-essential modules. Tightening the Apache configurationTo recap, in the last post I described how to create a sample virtual machine called tester. local, onto which Chef installed the Apache 2 web server. If you were (as they say in the game-show world) “playing along at home,” you created a sample role called webserver that caused the apache2 and apt packages to be installed on the node tester. local. You also bootstrapped the node so that it converged into the desired state. As a refresher, let’s review a few details from last time. In your chef-repo directory, at the command line type: knife role edit webserverYou should see something that looks like this: {  name :  webserver ,  description :  Web server for my. org ,  json_class :  Chef::Role ,  default_attributes : { },  override_attributes : {   apache : {    listen_ports : [  80  ]  } },  chef_type :  role ,  run_list : [   recipe[apt] ,   recipe[apache2]  ],  env_run_lists : { }}This configuration works just fine, of course. It sets up Apache with the usual defaults. Lots of modules are enabled, and a default website is configured automatically. For demonstrations, that might be dandy. But in production situations, you should tighten up the configuration so that it is more secure. Security professionals know, as a general rule, that when something has fewer configured options, it is usually more secure. In that spirit, let’s:  Minimize the attack surface by removing Apache modules we don’t need Decrease the amount of information “leaked” by the server by turning off server tokens and signatures Increase server performance by eliminating HTTP keep-alives Remove the default server websiteIf you have tried to do these things in the past, you probably wrote shell-code or some other kind of custom script. Or perhaps, like me, painstakingly hand-tuned the server and wrote down all of your specific hardening steps in a notebook in case you needed to do it again. The genius of Chef’s apache2 cookbook is that you no longer have to do those things. The apache2 cookbook recipes are cleverly written; they allow Apache to be heavily customized without requiring you to write code. Nearly everything that Apache does (or should not do) can be controlled through attributes. Attributes and their values can be defined in cookbooks via attribute files and within recipes. They can also be defined for individual roles or environments. When attributes are defined in more than one place, those defined for specific environments beat those defined for roles, which in turn beat those defined in cookbooks. Attribute values can also have multiple priorities. In reverse order of precedence, these are default, force default, normal, override, force override and automatic priority types. That is, the default attributes are used unless there are force-default, override, force-override or automatic values supplied somewhere; force-default attributes apply unless normal, override, force-override or automatic values are found, and so on. The precedence rules are fairly complex; OpsCode’s documentation discusses them at length. In this case, you will define a several override attributes that will take precedence over the default values defined in the apache2 recipes. When chef-client runs on the target node tester. local, these overridden values will be used in the various recipes to produce a more secure web server. At the console, type: knife role edit webserverIn the editor screen, modify the webserver role so that it looks like this: {  name :  webserver ,  description :  Web server for my. org ,  json_class :  Chef::Role ,  default_attributes : { },  override_attributes : {   apache : {    allow_override :  None ,    contact :  nobody@example. com ,    default_modules : [     alias ,     cgi ,     deflate ,     dir ,     log_config ,     logio ,     mime ,     rewrite ,     setenvif    ],    default_site_enabled : false,    directory_index :  disabled ,    directory_options :  None ,    ext_status : false,    keepalive :  Off ,    keepaliverequests :  100 ,    keepalivetimeout :  15 ,    listen_ports : [     80    ],    serversignature :  Off ,    servertokens :  Prod ,    timeout :  120 ,    traceenable :  Off   } },  chef_type :  role ,  run_list : [   recipe[apt] ,   recipe[apache2]  ],  env_run_lists : { }}The hash named apache (inside the override_attributes hash), contains the attributes that modify how the Apache is configured. If you are familiar with Apache configuration files, you can probably guess what many of the attributes do. In order, the override values tell Apache to:  allow_override: Prevents . htaccess files placed in content directories from overriding any directives already in place for the directory contact: Sets the contact email address printed on Apache error pages to a bogus address default_modules: Restricts Apache loadable modules to just the few needed to server static content; in this case, mod_alias, mod_cgi, mod_deflate, mod_dir, two logging modules, mod_mime (MIME support), mod_rewrite (for URL re-writing) and mod_setenvif (useful for sending different responses based on browser types) default_site_enabled: Disables the default website directory_index: Disables directory indexing directory_options: Disable all “extra features” in directories, such as fancy indexing, symlink-following, multi-views, server-side includes and so forth ext_status: Disables extended status messages keepalive, keepaliverequests and keepalivetimeout: Disables HTTP Keep-Alive messages, which can cause performance to suffer in many cases serversignature: Removes server signatures from error messages servertokens: Minimizes the response header field to include just the webserver software (“Apache”) but not the version, OS or compiled-in options timeout: Increases the time the server is allowed to respond to a request to 120 seconds traceenable: Removes support for the HTTP TRACE methodOf these attributes, the default_modules attribute is the most interesting because its value causes various Apache modules to be enabled or disabled. By default, the apache2 recipe loads a huge number of modules. By overriding the defaults you can restrict what is loaded to a small subset. Note that Apache always loads a few other modules regardless of the value of the default_modules attribute. These include authorization, content negotiation, timeout and status modules. But by keeping the list of modules small, you keep the server’s memory footprint smaller. You also get rid of features that aren’t needed in most websites and can be sources of risk, such as WebDAV support, LDAP authentication, proxying and so forth. I do not claim to be an Apache expert by any means, but default settings in the list above are reasonably tight. Certainly, they are good enough to demonstrate how you can use attributes to customize how the Apache cookbook runs. Now that you have created override attributes for the web server role, it is time to put them to use. Save and close the role editor; the contents will be saved to the Chef server. SSH into the test VM and execute the node’s run-list again so that the new attribute values are applied. From the post from last time, recall that the Chef role webserver had been assigned to tester. local. All that you need to do, therefore, is run the client again. SSH into the box and elevate to root: vagrant sshsudo suand then: chef-clientYou should see a dizzying rush of console messages, including many indicating that various Apache-related files are being modified. The run process should only take a few seconds. Assuming all recipes succeed, you will see a message at the bottom similar to the following: Recipe: apache2::default * service[apache2] action restart  - restart service service[apache2]Chef Client finished, 31 resources updatedCongratulations; your Apache server is now just a little bit faster, and a little bit tighter. You did it solely by twiddling a few attributes, without having to write any code. Nice, huh? Creating a new role for server hardeningLet’s do some more attribute-twiddling. This time, your objective is to tighten the configuration of several common server components that reside on most servers: the SSH configuration, and the Chef client itself. Download the cookbooks for SSH and the Chef client: knife cookbook site install opensshknife cookbook site install chef-clientUpload the cookbooks to the Chef server: knife cookbook upload --allCreate a second role. This role, called base, will be used by all servers and will include recipes that every server should use. Type: knife role create base…and supply the following contents into the editor: {  name :  base ,  description :  Essential recipes for securing every server ,  json_class :  Chef::Role ,  default_attributes : { },  override_attributes : {   openssh : {    server : {     allow_agent_forwarding :  no ,     allow_tcp_forwarding :  no ,     client_alive_count_max :  0 ,     client_alive_interval :  600 ,     ignore_user_known_hosts :  yes ,     login_grace_time :  30s ,     password_authentication :  no ,     permit_root_login :  no ,     rsa_authentication :  no    }  } },  chef_type :  role ,  run_list : [   recipe[openssh] ,   recipe[chef-client::delete_validation]  ],  env_run_lists : { }}The openssh recipe configures SSH on the machine. The override attributes above it configure the OpenSSH server daemon so that it uses sensible settings. Root logins are disabled, password authentication is disallowed; only public-key authentication is allowed. Session-forwarding is disabled, making the server unsuitable for use a “jump box. ” (For more information on hardening SSHD, see the many fine articles on the subject. ) In addition to the SSH settings, notice the addition of the chef-client::delete_validation recipe. This recipe does something rather important from a security prospective. As discussed previously, Chef server communicates with its nodes and clients using public/private key pairs. When a new node is added, a shared “validation key” is copied to the new node. This is a standard 2048-bit RSA private key with a name similar to organization-validator. pem; it is stored in your Chef repository’s . chef directory. It is not versioned by Git because . chef/*. pem is added to . gitignore, and it is obviously very sensitive. Anyone who obtained the validation key could conceivably join your Chef node set and gain access to the configuration data, recipes and more. Despite the sensitivity of this key, however, after the bootstrap operation completes, Chef inexplicably leaves it on the new node! It would be much nicer to remove it after the bootstrap. For security reasons, you should remove the validation key after the initial bootstrap because it is not needed any more. The chef-client::delete_validation recipe does that. That is why it is in the run-list for the base role. Adding the base role to the serverAfter you define the base role, you need to apply it to the test VM tester. local by adding it to the node’s run list. At present, tester. local is only running recipes that are part of the webserver role. As you might expect, you can add to a node’s run-list by using knife. Type: knife node run_list add tester. local  role[base] You will see output similar to the following that confirms that the base role has been added to tester. local’s run list. tester. local: run_list:  role[webserver]  role[base]SSH back into the test box (type vagrant ssh followed by sudo su). Run chef-client again. You will see many messages scroll by indicating that the /etc/ssh/sshd_config and /etc/ssh/ssh_config files have been updated. By default, the Chef openssh cookbook configures these files with the default settings that ship with OpenSSH. Console output should look similar to the following: Recipe: openssh::default * package[openssh-client] action install (up to date) * package[openssh-server] action install (up to date) * service[ssh] action enable  - enable service service[ssh] * service[ssh] action start (up to date) * template[/etc/ssh/ssh_config] action create  - update content in file /etc/ssh/ssh_config from 265a26 to 74365c    --- /etc/ssh/ssh_config	2012-04-02 11:49:30. 000000000 +0000    +++ /tmp/chef-rendered-template20131003-3037-n6ytk	2013-10-03 02:14:48. 674543237 +0000    @@ -1,53 +1,3 @@. . .  * template[/etc/ssh/sshd_config] action create  - update content in file /etc/ssh/sshd_config from 33469d to 1ba1c4    --- /etc/ssh/sshd_config	2013-05-11 06:10:17. 805866080 +0000    +++ /tmp/chef-rendered-template20131003-3037-6xl885	2013-10-03 02:14:49. 114323240 +0000    @@ -1,88 +1,14 @@    -# Package generated configuration file    -# See the sshd_config(5) manpage for details    +# Generated by Chef for tester. localRecipe: openssh::default * service[ssh] action restart  - restart service service[ssh]You can verify that SSH has been reconfigured correctly by trying to SSH into tester. local using the default Vagrant account credentials (vagrant/vagrant). They should no longer work. However, typing the vagrant ssh command should still get you in. That is because the vagrant ssh authenticates using an embedded private key that is hardcoded into Vagrant. The public half of this key is an authorized key in the vagrant account’s list of public keys. (You can verify this yourself by examining the file /home/vagrant/. ssh/authorized_keys on tester. local. It shows one entry whose description reads “vagrant insecure public key. ” How did it get there? Well, that is part of the ”contract” of building a Vagrant-compatible base box. )  Note: running the openssh recipe with the attributes as shown above can have adverse consequences on production nodes if you aren’t prepared. The recipe with the attributes as shown removes SSH root access. Unless you have another way of becoming root on the box, you might find yourself locked out! If your machine is a Vagrant machine, you can use the vagrant ssh command to become root. For non-Vagrant machines, you will need a non-root account that allows public-key logins and can su to root. You have been warned. Next: Managing SSL certificates and keysThis post introduced the concept of using Chef to partially harden a web server. You reduced the number of loadable Apache modules to a minimum set, disabled unnecessary services and reduced the amount of useful information an attacker could obtain. You created a second role called base and assigned two recipes, openssh and chef-client::delete_validation. These recipes configure OpenSSH in a more restrictive manner by disabling password authentication, disabling root logins and preventing session forwarding. The delete_validation recipe removes the Chef validation key from the node after it is created, which removes a potential security risk. In the next post, you will switch back to Apache. You will use Chef to perform one of the most challenging aspects of any server configuration: copying SSL keying materials to server nodes. Image copyright 2016 by Kharnagy, licensed under the Creative Commons Attribution-Share Alike 4. 0 International license. "
    }, {
    "id": 22,
    "url": "http://localhost:4000/blog/2013/10/01/chef-starter/",
    "title": "The DevOps Security Handbook&#58; Building Security In With Chef, Part I",
    "body": "2013/10/01 - IntroductionThis is the first in a series of posts about Chef, an infrastructure automation platform. The goal of this series is to describe how to build a reasonably secure Apache web server. By using Chef, we can quickly and efficiently build identical web servers with assurance that they will work the same way, every time, and have the security properties we want. You will build this server in stages. The server will ultimately contain the following elements:  Apache 2 HTTP web server, with minimal modules and a virtual host defined for serving website content A limited user account whose home directory contains the website content. The account only accepts SSH remote logins that use public-key authentication. The Apache virtual host’s document root will point to a subdirectory of the account’s home A user group whose name matches the user account name, and which contains the user as its only member Hardened configuration with minimized services, synchronized time, intrusion prevention, and other security characteristicsFor purposes of testing, the server will be spun up as a virtual machine on your local workstation. You will use VirtualBox VMs for this purpose. This first post will describe how to set up a basic test infrastructure that uses Chef. You will set up the Chef workstation and server account, create an Apache server role and a test environment, set up a virtual machine, and build your first node. The web server will not do much, and it will not be especially secure — at least not initially. Subsequent posts will gradually add more security components. By adding security features gradually, you will learn how to use Chef. As a side effect, you will learn how Chef’s philosophy of “convergence” makes it easy to gradually massage your nodes into the states you want. This is important when adding Chef to servers that already exist. Getting startedIn order to demonstrate how Chef works, you will need a virtual machine to play with. To create one, you will use Vagrant to instantiate a new VirtualBox VM. Our goal is to create a VM that you can boot and access on your laptop for testing purposes. After you do that, you will bootstrap it with Chef so that you can configure and manage it. Some prerequisites. You will need to download and install:  VirtualBox from Oracle, which creates and manages guest virtual machines.  Vagrant, which creates, manages, and destroys VirtualBox VM images from the command line.  Git, the ubiquitous version-control system that will allow you to “check in” your Chef repository and manage its versions as you create the server.  Chef 11. x workstation software, which is where all of the magic happens.  Ruby 1. 9. 3 or higherChef works best on Unix- and Linux-based systems. I used a Mac to prepare this guide. But my instructions are largely platform independent; as long as you have a Linux- or BSD-based workstation, or a Mac, you should be in good shape. OpsCode’s QuickStart guide does a fine job explaining how to do the initial preparatory steps in their Workstation setup page. OpsCode recommends that you install a Ruby version manager. I use RVM myself, although the documentation (in the Advanced tab) recommends RBENV. Open up OpsCode’s QuickStart guide and do everything on Page 1. It should take you about 5 minutes. Next, you need to create an Enterprise Chef account, and download the starter package using the Enterprise Chef web interface.  Page 2 of the documentation page explains how to do this. The free version of Enterprise Chef supports up to five nodes, which is perfect for our purposes. After you sign up and create an account, create a new Organization and download the “Starter Kit” as described on QuickStart Page 2. Follow the instructions on this page all the way up to the “Create a Simple Cookbook” section. Once you have done that, you have configured your Chef workstation properly. A word about the “Starter Kit. ” The Starter Kit is a zipped bundle that contains a sample Chef repository directory structure, and crucially, a private key for the your workstation, which Chef calls a “client. ” When you expand the Starter Kit, it will unpack into a directory called chef-repo. This is your Chef repository, and you should move it somewhere useful. I put mine in ~/workspace, which is where I keep all of my dev stuff, but you can put it anywhere you like. Using the Chef workstation tools, you create and edit Chef roles, environments, cookbooks and other locally on your workstation. When you want to push new versions out to your nodes, you use Knife to upload them to the Enterprise Chef server. When you upload, Knife uses the client’s private key to authenticate with the Enterprise Chef server. With the initial setup stuff out of the way, let’s start getting into the fun stuff. Creating sample server run-lists, roles and environmentsI have found the OpsCode QuickStart documentation to be quite well-written. But it only gets you so far, and it leaves out some important steps for using Chef in a more serious way. Let’s take this opportunity to stray from the OpsCode documentation a bit and lay down some additional foundation-work for building the web server. In particular, let’s set up some initial run-lists, roles and environments for your test VM. Some background. Chef “converges” nodes into their desired states by applying a ”run-list” of recipes to each node. The run-list of recipes (Apache2, NTPD, user creation, etc) that apply can be specified in several ways. The quickest and most direct way is to specify the node’s run-list of recipes when the node is initially bootstrapped with Chef; that is, when the Chef agent (chef-client) is initially installed on the node. Bootstrapping the node configuration is done using Knife, and the syntax looks like this: knife bootstrap tester. local --run-list  recipe[apt],recipe[apache2]  -E testingI have omitted some of the syntax the sake of simplicity; don’t try running this. There are important concepts to understand here. The bootstrap command causes the chef-client application to be installed on the node. The chef-client is essentially an agent. It configures and installs software based on instructions (”recipes”) it receives from the Chef server. Notice the run-list parameter: it indicates that the APT and Apache2 recipes will be applied to node tester. local. What this means is that when chef-client is bootstrapped onto the node, the APT and Apache packages will be downloaded, installed and configured as well. Notice also the -E parameter. This means that tester. local should be assigned to an environment called testing, which you will define in a minute. By ”environment,” Chef means a group of nodes that typically correspond to a stage of development, for example “testing,” “staging,” or “production. ” Let’s create the testing environment now. Type: knife environment create testing…and type or paste the following JSON contents into the file: {  name :  testing ,  description :  Test environment ,  cookbook_versions : { },  json_class :  Chef::Environment ,  chef_type :  environment ,  default_attributes : { },  override_attributes : { }}Nothing tricky here — just a simple JSON file with a few attributes in it. The default_attributes and override_attributes items can be used to supply variables to the recipes that are unique to the testing environment, for example, debug settings or dummy passwords. You will leave these blank for now because they don’t apply in this case. As I mentioned, there are several ways to assign run-list items to nodes. Direct assignment of recipes during bootstrapping, shown in the edited knife bootstrap command above, is the easiest way. But that won’t scale if you have multiple nodes that must be configured identically. It makes more sense, instead, to create a role, which allows common run-lists to be defined for groups of machines that do the same thing. Instead of bootstrapping with a specific run-list of recipes, you can bootstrap with roles. When you use a role, Chef looks up (dereferences, if you will) the run-list for the role and applies all of the recipes it contains, along with any custom attributes. You can think of roles as a type of pointer. Let’s create a new role called webserver. In it you will add the components needed to run your website. Type: knife role create webserver…and supply these contents: {  name :  webserver ,  description :  Web server for my. org ,  json_class :  Chef::Role ,  default_attributes : { },  override_attributes : {   apache : {    listen_ports : [  80  ]  } },  chef_type :  role ,  run_list : [   recipe[apt] ,   recipe[apache2] , ],  env_run_lists : { }}Notice that the run-list attribute contains the apache2 cookbook, similar to what you used in the initial bootstrap command. The listen-ports override attribute tells the apache2 cookbook to configure Apache to listen just on port 80. You will learn more about override attributes in a future post. But if you are curious about how the cookbook works, and about the various attributes you can use to customize Apache’s configuration, see OpsCode’s online decimation. Notice also the apt recipe; this is required because Debian’s APT package updater is how Apache is actually installed onto the node. To bootstrap using roles instead of directly specifying recipes, you would use the following syntax (some details omitted): knife bootstrap tester. local --run-list  role[webserver]  -E testingAgain, don’t type this in, because it won’t work without some additional syntax; you will get to it soon enough. Let’s complete the initial Chef setup. So far, you have created a sample test environment called testing, and a sample server role called webserver. To complete the initial setup, you need to do two more things: download the actual cookbooks that Chef will apply to the node; and upload the cookbooks to the Chef server so that any nodes that are assigned it can get it. The cookbooks we need are apt (required to install Apache), and apache2 (Apache itself). To install the apache2 cookbook, type: knife cookbook site install apache2This command looks up the apache2 cookbook on the Opscode community cookbook site and causes it to be downloaded to your workstation. You will see a series of output messages showing the progress of the download, followed by a completion message when it succeeds. While you are at it, go ahead and install the apt cookbook too. After downloading both, commit your current Chef repo to Git: git add . git commit -m  Added Apache and APT cookbooks.  Then upload your cookbooks to the Chef server: knife cookbook upload --allIt might seem a little strange to have to upload the cookbooks to the Chef server. After all, they are managed centrally from the community cookbook site. Why can’t roles simply reference the cookbooks stored there, instead of needing to make copies? Frankly, I am not too sure why this is the case. I suspect Chef works this way so that cookbooks and recipes can be hacked up when needed. Regardless, you must upload cookbooks to Chef server after you update them. If you don’t, the Chef client on any nodes you create will continue to use outdated recipes. Backing up Chef server dataBecause you are using Enterprise Chef, your nodes, roles, environments and data bags are stored on the server — not locally. While I trust OpsCode to keep their servers up and available, I like to keep copies of important data on my client so that I have a record of them, and can version them with Git. You should, too. To do that, you will need to install the backup-export Knife plugin, part of the Knife Hacks package. Then, you should copy a specific plugin file from GitHub into our local Chef knife plugin cache in ~/. chef/plugins/knife, creating the directory if necessary. A few quick commands should do the trick: mkdir -p ~/. chef/plugins/knifecurl https://raw. github. com/stevendanna/knife-hacks/master/plugins/backup_export. rb &gt; ~/. chef/plugins/knife/backup_export. rbChange back to your chef-repo directory and issue the following command: knife backup exportYou’ll see output similar to this: Backing up nodesBacking up nodes tester. localBacking up rolesBacking up roles webserverBacking up data bagsBacking up environmentsBacking up environments testingBy default, backups are stored in . chef/chef_server_backup. You can change this by modifying the chef_server_backup_dir entry in . chef/knife. rb, but there’s no obvious benefit to doing that here. It is sufficient simply to have them present in the Chef repo directory, because they can be checked into Git using the usual familiar git add . and git commit steps. Go ahead and do that now. If you have gotten this far, your initial Chef setup is complete. Now, let’s create a test machine. Creating a virtual machine for testingChange to your Chef repo directory. Create a new file Vagrantfile with these contents, or edit the existing one so that it matches this: # -*- mode: ruby -*-# vi: set ft=ruby :Vagrant. configure( 2 ) do |config| config. vm. box =  opscode-ubuntu-12. 04-i386  config. vm. box_url =  https://opscode-vm. s3. amazonaws. com/vagrant/opscode_ubuntu-12. 04-i386_provisionerless. box  config. vm. hostname =  tester. local  config. vm. define :tester do |t| end config. vm. network  private_network , ip:  192. 168. 56. 2  config. vm. provider :virtualbox do |vb|  vb. gui = false  vb. name =  tester. local  endendVagrantfile’s job is to tell Vagrant how to set up the test VM. If you have used Vagrant before, you will notice that this Vagrantfile is shorter than the default file Vagrant supplies. Here’s what it does:  Downloads an Ubuntu 12. 04 base box (essentially, a virtual machine image) from OpsCode’s repository on Amazon Creates a VirtualBox VM based on the machine image Gives the VM the network name tester. local. This is the name that the Unix command hostname will return when you log into it Names the VirtualBox machine tester. This is the name used to start, stop and delete the VM when using the VirtualBox command-line tools or the VirtualBox GUI. Names the VirtualBox image directory tester. local. By default, VirtualBox names the image based on the directory that contains Vagrantfile, plus a timestamp suffix. The vb. name property inside the config. vm. provider block overrides the default so that it matches the host name.  Configures the VM’s networking interface to use a private network address 192. 168. 56. 2. This will allow us to start the VM and see it on our workstation, but the VM won’t be accessible from the outside.  Specifies that when you boot the VM, it will be booted in headless mode; the VirtualBox GUI won’t be displayed. That is all you need to instantiate a new VM on our workstation. Next, edit your workstation’s /etc/hosts file and add a line that points to the VM using the private IP address and name tester. local: 192. 168. 56. 2  tester. localGreat. Now, let’s go ahead and actually create the VM. From the command line in the same directory as Vagrantfile, type: vagrant upVagrant will look by default in the same directory for Vagrantfile, and having found it, will create the VM according to the contents of the file. You will see output similar to the following: Bringing machine 'tester' up with 'virtualbox' provider. . . [tester] Importing base box 'opscode-ubuntu-12. 04-i386'. . . [tester] Matching MAC address for NAT networking. . . [tester] Setting the name of the VM. . . [tester] Clearing any previously set forwarded ports. . . [tester] Creating shared folders metadata. . . [tester] Clearing any previously set network interfaces. . . [tester] Preparing network interfaces based on configuration. . . [tester] Forwarding ports. . . [tester] -- 22 =&gt; 2222 (adapter 1)[tester] Booting VM. . . [tester] Waiting for VM to boot. This can take a few minutes. [tester] VM booted and ready for use![tester] Setting hostname. . . [tester] Configuring and enabling network interfaces. . . [tester] Mounting shared folders. . . [tester] -- /vagrantThe entire process should take between 30 seconds to a minute if the base box is already cached on your workstation. If not, the first time you do vagrant up Vagrant will need to download the machine image from Amazon. You can verify that the new test VM is up by pinging tester and verifying that it responds: Tweety:chef-repo arj$ ping tester. localPING tester (192. 168. 56. 2): 56 data bytes64 bytes from 192. 168. 56. 2: icmp_seq=0 ttl=64 time=0. 582 ms64 bytes from 192. 168. 56. 2: icmp_seq=1 ttl=64 time=0. 638 ms…Typing vagrant status will also indicate that the VM is up and running: Tweety:chef-repo arj$ vagrant statusCurrent machine states:tester          running (virtualbox)The VM is running. To stop this VM, you can run `vagrant halt` toshut it down forcefully, or you can run `vagrant suspend` to simplysuspend the virtual machine. In either case, to restart it again,simply run `vagrant up`. You can repeat this process as often as you like by destroying and recreating the VM: vagrant halt testervagrant destroy testerIf you would like to verify that the VM is really up, you can SSH into the box using the username vagrant and password vagrant. You can also use the command vagrant ssh which does the same thing.  Note: by default, base boxes used with Vagrant ship with a pre-installed SSH public/private key pair that is used for SSHing into VMs it creates. These base boxes also ship with default vagrant/vagrant credentials. This configuration is not secure. For testing purposes on your local workstation this should not be a problem, because we have configured the VM to use host-based networking. It cannot be accessed outside of the workstation. But production servers should not use Vagrant with its default configuration. Bootstrapping the virtual machine with ChefSo far, so good. You have successfully created a test virtual machine, but it isn’t much good to us yet because it doesn’t have Chef on it. Until it does, you cannot manage it. It is (finally!) time to “bootstrap” the VM using Knife. This installs the chef-client agent on the node, and registers the new node with the Chef server. Type in the following: knife bootstrap tester. local --ssh-user vagrant --ssh-password vagrant --run-list  role[webserver]  -E testing --sudoViola! Assuming you did everything as described, Chef will SSH into the box, download and install Chef client onto it, and begin converging the node into its desired state; in this case, installing and configuring Apache. Immediately after hitting Enter, a long list of output lines should appear. These should resemble the following: Bootstrapping Chef on tester. localtester. local --2013-09-29 03:20:41-- https://www. opscode. com/chef/install. shtester. local tester. local Resolving www. opscode. com (www. opscode. com). . . tester. local 184. 106. 28. 82tester. local tester. local Connecting to www. opscode. com (www. opscode. com)|184. 106. 28. 82|:443. . . tester. local connected. tester. local tester. local HTTP request sent, awaiting response. . . tester. local 200 OKfollowed by tester. local Starting Chef Client, version 11. 6. 0tester. local tester. local resolving cookbooks for run list: [ apt ,  apache2 ]tester. local tester. local Synchronizing Cookbooks:tester. local tester. local  - apttester. local tester. local  - apache2tester. local tester. local Compiling Cookbooks. . . and then a series of lines that indicate that APT and Apache have been installed. The last lines indicate that Apache has been installed and restarted, and that the resources on the box have been updated: tester. local Recipe: apache2::defaulttester. local tester. local  * service[apache2] action restarttester. local tester. local tester. local   - restart service service[apache2]tester. local tester. local tester. local tester. local Chef Client finished, 28 resources updatedtester. local If you see output similar to this, and no errors, it means that you have successfully converged your first node. Congratulations! Excellent work. You verify that the web server is up by firing up your browser to the address http://tester. local. It should return a “Forbidden” message because we have not actually provided any HTML pages for Apache to serve up. But that is evidence enough that Apache is actually working. Next: Adding security to the boxThis post covered the basics of how to get going with Chef. You have installed the Chef workstation software and supporting components Git, Ruby and VirtualBox and Vagrant. You have created a sample role called webserver and assigned two sample recipes, apache2 and apt, to it. You created a virtual machine called tester with the domain name tester. local and bootstrapped Chef onto it, placing it under Chef control. In the next post, you will begin doing more useful work. I’ll describe how to fine-tune the Apache installation. We will also begin increasing the security of the machine. This post was updated October 1, 2013 to change the hostname used in the examples from tester to tester. local. It was updated on October 2, 2013 to remove references to the half-configured SSL support for Apache; I’ll cover this more fully in a future post. Image copyright 2016 by Kharnagy, licensed under the Creative Commons Attribution-Share Alike 4. 0 International license. "
    }, {
    "id": 23,
    "url": "http://localhost:4000/blog/2013/09/23/Building-Security-in-Using-Chef/",
    "title": "Building Security In Using Chef",
    "body": "2013/09/23 - Lately I have been spending a lot of time with a new best friend. This new friend is reliable; he does everything according to plan and always exactly the same way. The results are exactly the same every time, too. And he speaks to me in a language that I understand — the language of food. I am not talking about a new buddy gourmand, about a pal I go out to restaurants with, or about a super-reliable project manager. My new best friend is a technology called Chef, made by OpsCode. Chef, along with Puppet and CFEngine, is a flexible toolset for building infrastructure. The Chef mantra is “infrastructure as code,” which means simply that you can build infrastructure — servers and workstations the same way every time. Chef has important implications for security because, by using it, you can ensure that your nodes have exactly the security properties you want by “baking it in” to what Chef calls “cookbooks,” the core component. I’ll come back to the security implications in a few minutes, but in the meantime I should explain what a cookbook is. Cookbooks are packages that define how packages, applications or system functions should be built and configured. Cookbooks exist for Apache, NTP, user and group account creation, and just about every common application you can imagine. At a file level, cookbooks are basically composed of property files, templates and clever glue-code. The cookbook’s job is to declare required packages and dependencies; provide templates for configuration files that need to be modified, and provide Ruby code that sets up the packages, configures things or does whatever is needed to achieve the desired result. The process for building nodes is similar to how developers build code. Typical Chef workflowHere’s what a typical project workflow looks like. With Chef, you:  Create a new developer project using Git Download one or more cookbooks for the applications or services you want to manage; or, you create new ones from scratch Modify the properties associated with each cookbook as needed Upload the modified cookbooks and/or properties to the master Chef server “Bootstrap” new nodes from standard machine images, for example a generic CentOS VM. The bootstrap process injects Chef agents onto the new nodes and then… “Converge” each new nodes into the desired state by downloading the required cookbooks and properties (the “run list”) that apply, and then running all of the cookbook recipes in the run-listsI’ve glossed over quite a few things here, but the overall strategy is that the Chef agent transforms the node into the state you want. Sometimes this takes multiple passes through the run-list, although the Chef agent is generally smart enough to figure out how to manage dependencies without intervention. That is why Chef uses the term “converge” to describe how the node morphs into the desired state. Nodes need not be clones of each other, and indeed Chef can be injected into existing systems long after they are created. One might say that the Chef philosophy is exactly the opposite of the traditional “golden image” concept where every system is an exact copy of every other. It is more correct to say that with Chef, every package and application within scope — those you have created cookbooks for — is configured in exactly the way you expect. Chef stresses idempotency — a fancy way of saying that when you execute the run-list on multiple nodes, you get the same result every time. For the curious, Sean O’Meara provides an excellent overview of Chef on his blog. Chef tools for cooking in the kitchenChef includes several components that work together to produce consistent results every time:  Knife, a command-line workhorse that you use to create, download, edit and upload cookbooks, clients, nodes, roles and environments. Clients are the workstations that edit Chef configurations. Nodes are the machines that Chef produces. Roles are run-lists of cookbooks and configs for a common purpose, for example, a role called “webserver” with cookbooks and properties for Apache, PHP, CGI, and your company’s standard HTML chrome. Environments are variations on either global configurations or roles for specific situations, for example “development,” “production,” etc.  Chef-server, which serves as a master repository for your cookbooks, property files, and lists of clients, nodes and environments. You can set up your own server by downloading and running the community open source version. OpsCode also provides a hosted option called Enterprise Chef, which is free when used with five nodes or less.  Public/private keys, which allow clients, nodes and servers to authenticate each other without needing passwords. When your initial account is created on Chef server via the web GUI, the server creates a key-pair. The private half is added to a zipped download bundle that is expanded on the client into a directory. The client directory is then checked into Git (keying materials are not checked in). Whenever Knife is executed, it uses the private key to authenticate with the server first. The client bundle also includes a “validation key,” which is copied to new nodes at the time of creation. This validation key is used to initiate a key-exchange process with the server to create a node-specific key, after which point the validation key can be removed from the node.  Resource providers, which perform tasks listed in cookbook scripts. These providers allow Chef cookbook commands to remain relatively abstracted from the underlying OS commands. For example, the include resource provider invokes the package managers on various systems (yum for Red Hat or CentOS, apt for Debian-style Linuxes etc). Creative combinations enable interesting results: for example, you can populate directories on target nodes with Git checkout contents. If you had previously versioned website page contents, contents, creating an up-to-date static webserver can be done automatically by causing it to pull the latest content from the master repo — a neat trick.  Community site, which hosts cookbooks from OpsCode and third parties, saving you the trouble of writing your own cookbooks. The Apache cookbook, for example, is extremely complete and allows for flexible customization. I have not finished fooling around with it yet in my own experiments, but the properties files allow for quite a bit of hardening; you can specify which Apache modules to include and exclude, create website aliases, map directories and do many of the things that old Apache-tuners like me have been doing by hand for years. As you might expect, the degree of configurability for any particular cookbook varies greatly depending on the skill of the author and amount of iterative refinement the cookbook recipes have received over time. In addition to its own components, Chef also makes good use of a few other key tools that you might be familiar with, chiefly:  Git, the distributed version control system at the epicenter of the DevOps movement. When you create a new Chef project, the first thing you usually do is commit the new project into a local Git repository. At that point, you can easily create and link to a remote repository so that changes to the project are appropriately versioned centrally. As noted above, client-side keying materials are not automatically versioned; they are part of the default . gitignore file initially downloaded from the server.  Vagrant, a command-line utility for managing virtual machines. Vagrant allows you to download and cache a pristine community machine image, which can be quickly spun up, bootstrapped with Chef, and destroyed. The default VM image type is Oracle’s VirtualBox, but Vagrant can also manage VMWare, Amazon and Rackspace images. With VirtualBox images, Vagrant can also manage networking settings so that it is easy to create test machines on your laptop. Using Chef and Vagrant together, for example, I was able to create a new virtual machine, bootstrap it with Chef, and converge it to a desired Apache state in about 30 seconds. Implications for securitySo, why is a security guy like me fooling around with Chef, and what are the implications for security? Here’s what I like about it:  Infrastructure as code. I really like how you can create and manage machines essentially as code. I do a fair amount of programming as an after-hours “professional hobby,” so it is great to be able to use some of the same tools and languages (notably Git and Ruby) here also.  Clever crypto. The mutual authentication system using naked public/private keys is clever. I’ve always felt that for the sorts of things Chef does, certificates would be too heavyweight and too much bother. While it is true that the client-side private key is not, by default, protected with a password, one can easily be added. The no-password default, however, does strike a nice balance of making it easy to communicate with Chef server without needing to worry too much. As long as the client node is protected, subversion isn’t a huge worry.  Stepwise assimilation. I like how Chef can be added to an existing machine so that it can be massaged into the desired state. When I have a little more time, I plan to perfect my Apache cookbook adaptations and converge my existing securitymetrics. org server into it. That would allow me to quickly recreate the web-server parts of the site if it got 0wned. I keep a rather long list of anal-retentive instructions for hand-tuning the Apache, Mailman, Logwatch, SSHD, etc. I intend to gradually move each of these items under Chef control. Gradual assimilation is nice, because it easier for most organizations to implement rather than focusing on big-gulp “golden image” projects.  Baked-in security possibilities. As you might imagine, the ability to converge nodes into predictable and known states is Chef’s strong point. If you are a security professional who believes in Building Security In (“Mr McGraw, white courtesy phone…”), Chef gives you powerful tools in service of that goal. Through Chef, cookbooks, services and applications can be minimized. Key exposures can be limited via existing cookbooks or through custom ones that you may create. Key caveats when working with ChefSo, that’s what I like about Chef. However, Chef has some important limitations that security professionals must keep in mind:  Chef’s frame of reference is that of an Agile developer, not that of a system administrator or a security pro. Cookbooks and recipes, and infrastructure-as-code are powerful metaphors, but they are different than those used by traditional configuration management tools. There is no concept of a CMDB other than in a very loose sense — the Chef server data and any projects managed by Git. Using Chef effectively requires you tho think like a developer. In companies where Agile or Lean has taken root — where development and operations are tightly coupled in a common workflow — this is a plus. But shops that aren’t fully wedded to the DevOps philosophy are likely to find Chef’s mindset a little alien.  Chef’s learning curve is steep and can lock you in. Chef property files and cookbook scripts are nothing more than stock Ruby files arranged in specific directory layouts and used in specific ways. Consequently, mastering Chef requires one to learn a bit of Ruby. Personally, I’ve found Ruby easier to learn than Perl or Bash (neither one of which I like very much). It allows me to express intent more simply and in a more compact fashion. What it means is that if you are a security or infrastructure professional who wants to build security in, you will have to roll up your sleeves a bit and learn a new language. Your investment in learning Chef and Ruby will lead to increased lock-in, which is usually a good thing. Certainly, it is better than the alternatives — rat’s nests of Perl, Bash, wikis and READMEs.  Chef’s documentation is average for open projects, with the pluses and minuses this implies. OpsCode offers a licensing and support model similar to other hybrid companies: the source code is freely available for most components; licensing is generous and corporate-friendly (Apache 2. 0 license); and a vibrant community helps newbies ascend the initial learning curves. If you want support you have to pay. For those who want to self-support, documentation is on par but not dramatically better than many open source projects: it covers basic use cases well, but minor deviations from potted plots cause hiccups. In my own experiments, for example, a server node wasn’t converging as it should have because chef-client wasn’t running as root. Error messages were cryptic and shed no light on the cause. Attempting to reinitialize the master workstation client made matters worse because I erased my private keys. I eventually figured out what was going on, but only through logical deduction rather than consulting the documentation.  Chef is server-centric, and won’t help you converge state on other types of devices, such as routers, load-balancers or databases. For those whose ambitions extend to automating the configuration of entire virtual or physical environments, you will need to bolster Chef with other tools. That isn’t necessarily a minus, but it does mean that Chef is only good at the things it is meant to be good at. It won’t be the only tool in your bag. Alternatives to ChefAs Sean’s blog points out, Chef is not the only game in town. Puppet serves a similar role for many companies, and its design philosophy is close to that of Chef. Both were inspired by CFEngine. I chose to experiment with Chef because I felt it had more polish and refinement than Puppet. I have no idea whether this is actually true or not. At a certain point, it does not matter. Whether you like Chef, Puppet or CFEngine, the point is to try them out and see where it takes you. I am quite pleased so far with Chef and look forward to using it more with my own projects. I will post more details in future blog posts. If you are a security or infrastructure who is working with Chef or similar tools, I would love to hear about you experiences. Add a comment or send me an email! Image copyright 2016 by Kharnagy, licensed under the Creative Commons Attribution-Share Alike 4. 0 International license. "
    }, {
    "id": 24,
    "url": "http://localhost:4000/blog/2013/08/26/new-web-adventures/",
    "title": "New Web Adventures with Heroku",
    "body": "2013/08/26 - Many ardent followers of this blog know that among other things, one of my professional hobbies is application development. I am a “weekend programmer. ” I always have a side project or two going, but do not professionally program (much) as part of my day job. That’s not necessarily for lack of talent (cough), but for lack of desire to make my living from it. That said, as the CTO of a cloud security software company, it’s rather good to know how software is built these days. As a bonus, by staying close to dev via a hobby or two, I can relate better to my colleagues who actually do make their living from programming. I have been programming most of my life. I learned to program around age 11 on time-sharing systems, and then later, on the Apple II+ and the PDP 11-44. My high-school computer science team was nationally ranked, usually #1 or #2 in the country, and I scored well enough on my high school Advanced Placement (AP) test — 5 on a 5 scale — to waive all of my college science requirements in college. I could have majored in computer science but chose economics and political science instead, with huge dollops of Japanese and architecture on the side. I took just one computer science class in college — for fun — as a senior. It was CS 201, the hard-core freshman course for future majors. The course focused on LISP. I hated it, frankly, because Lisp is a weird language, and because all of my 201 classmates were jumping with both feet into their future majors and left me breathing their dust. As a result, I found myself — for the first time in my life — on the ass end of the grading curve. That aside, I got my first consulting gigs after college as a programmer, and have kept coding, on-and-off, ever since. A few of my “weekend projects” have been more than that. For example, in the early 2000s I became enamored of Java 2 Enterprise Edition (J2EE) in general, and with a Java-based wiki software package (JSPWiki) in particular. I didn’t like JSPWiki’s security model and volunteered to re-write it. That was fun. Five years later, I had contributed about 100,000 lines of code, added LDAP and database authentication, re-wrote the authorization system, given it a new front end based on Stripes, and helped incubate it into a top-level Apache Software Foundation project. By that time, I had essentially become the co-lead on the project with my colleague Janne Jalkanen. But I found that I was no longer using the software day-to-day, and other life priorities intervened (marriage and a job change). So I retired from JSPWiki. More recently, I have indulged interests in two areas: mobile development — iOS in particular — and Dev Ops. In the mobile realm, I have been working on-and-off on an ambitious (too ambitious?) productivity app that will address a need that everyone has. On Dev Ops, I have lately been fooling around with some of the build-automation and hosting frameworks. Heroku is my current preoccupation. It combines server-side build automation with hosting. What that means is that developers can write code in their language of choice using a Heroku-mandated directory structure and packaging specification. Developers check in their code to Heroku’s servers using Git. When the code is checked in, Heroku packages the app based on the packaging document — called a Procfile — and deploys it in one or more web servers, depending on how much scalability the customer pays for. Heroku also offers a pre-configured private SQL database (Postgres), which developers can use for application data storage. As a bonus, Heroku offers a downloadable set of command-line utilities called Toolbelt that allow the server-side environment to be simulated and tested on the client. Best of all, Heroku offers compute time on one server — which they call a dyno — more-or-less for free, assuming the cycles don’t exceed a relatively generous threshold. From the developer’s perspective, Heroku is pretty great. Code is effortlessly deployed when it’s checked in. One simply pushes the latest code to Heroku using the usual method — git push heroku master. Heroku’s server-side hook detects the check-in, builds the deployment bundle (what Heroku calls a _slug) and deploys it on the dyno(s) in a few seconds. All of the most popular server-side development stacks are supported: JEE, Ruby + Rails, Ruby + Sinatra, Java + Play, Scala, Node. js, Clojure, Python + Django and many more. There’s a Postgres database in the cloud that is pre-provisioned for each application and is just “there” waiting to be used. A vibrant ecosystem allows third parties to offer NoSQL, monitoring and other services. Server scaling is merely a question of pulling out a credit card and buying some more incremental compute cycles. The documentation is simple and clear. And Heroku’s command-line Toolbelt tools make everything very, very easy and quick. What it all means is that developers can create, deploy, test and use low-volume web applications without spending a dime. Other than typing a few initial Toolbelt commands, everything else is done using their everyday workhorse, Git. The infrastructure is completely abstracted so that pushing an app out to the Internet is as simple as typing the words git push. All of which ought to be terrifying for security managers. From the big picture perspective, Heroku represents a complete rethink, and outsourcing of, the entire application development stack. That it can all be done for free — at least, for the first hit — means that Heroku and providers that offer similar stacks (CloudBees, Joyent, Engine Yard) create a natural alternative to traditional IT for prototyping, experimentation, and possibly, deployment. We can take this one step further. What Heroku and services like it means is that in the future, IT will remain relevant only if it can continue to engender respect with developers. If IT insists on being a roadblock — for example, if it can’t or won’t buy prototyping servers fast enough, imposes uninformed mandates about “company standard” frameworks, continues to require CVS (shudder) or SVN (wince) rather than Git, or breaks out in hives at the mention of this newfangled Hadoop thingy — it will create economic incentives for developers to look elsewhere. By “IT” I mean it as an aggregate entity — the architect rule-setters, security-gate-keepers and purse-string-holders that collectively and emergent-ly determine how applications are made and where they run. In my next post, I’ll offer some perspective on some of my experiments with the Play Framework, a radical re-think of Java that offers a compelling alternative to traditional JEE applications. My occasional correspondent and Twitter friend Rob Williams turned me on to Play. It can be deployed quickly and simply on Heroku. I’ll have some observations shortly. Image copyright 2016 by Kharnagy, licensed under the Creative Commons Attribution-Share Alike 4. 0 International license. "
    }, {
    "id": 25,
    "url": "http://localhost:4000/blog/2013/08/13/my-kind-of-cranky/",
    "title": "Review of Stephen Few&rsquo;s &ldquo;Information Dashboard Design, Second Edition&rdquo;",
    "body": "2013/08/13 - Twenty years ago, a polymath prophet named Edward Tufte self-published an incendiary book, The Visual Display of Quantitative Information. It forever changed how a certain species of white-collar professional viewed the world. As a DNA-tested, confirmed member of the species homo visualis, I can tell you that his book, and successors such as Envisioning Information, taught me how to create strong, effective statistical graphics. Tufte introduced the concepts of chart junk, the data-to-ink ratio, small multiples and sparklines. He argued forcefully and persuasively that designers of statistical graphics need not condescend to their audiences. And perhaps most important, he inspired a generation of authors, professionals and scientists — call them “Tuftees” — to strive for simplicity, clarity and honesty in their representations of data. Indeed, in my book Security Metrics: Replacing Fear, Uncertainty, and Doubt, I wrote an entire 40-page chapter on how to graphically present security data. That chapter owes everything to Tufte. I mention my own book not out of a desire to gratuitously promote it (not that there’s anything wrong with that), but because in the 2nd edition of Stephen Few’s Information Dashboard Design: Displaying Data for At-A-Glance Monitoring I can sense exactly why and how Mr Few was driven to write his own book about visualization. In my case, I felt compelled to summarize quickly everything I had learned about effective graphical techniques, because I wanted to help security professionals create exhibits that weren’t awful. After I put fingers to keyboard to write the chapter, though, I found it hard to stop writing. No treatment of security metrics would be complete without an honest discussion of visualization techniques, and that took space and length to do well. Cranky about the state of graphical practice in my own industry, and lacking decent models to point others at, I decided to build some of my own, often imperfect, models. (Really cranky, too: after re-reading chapter 6, it’s a wonder Addison-Wesley let me publish the book at all!) In short, pissyness led to something productive. You can smell the same faint alternating whiffs of frustration and hope in Mr Few’s book, too. He’s my kind of cranky. He’s a Tuftee. The first half of the book, about 110 sparse pages, focuses on what not to do when designing dashboards. Dozens of examples of bad dashboards fill the first hundred pages. I can only imagine the nightmare of getting screenshot copyright clearances from the vendors whose products he made examples of. But despite the sport he has with the screenshots, Information Dashboard Design also grounds practitioners in the basics. Few defines a dashboard as:  …a visual display of the most important information needed to achieve one or more objectives, consolidated and arranged on a single screen so that the information can be monitored at a glance. That is nicely said. Building on this fundamental definition, the first half of the book covers these additional topics:  Clarifying the Vision: What is a dashboard? Why do we use them? Thirteen common mistakes in dashboard design: exactly what you’d imagine; this is a regular rogues gallery Assessing what’s needed: what people need when they see a dashboard Fundamental considerations: how frequency of use, screen sizes, and data types influence dashboard design Tapping into the power of visual perception: how we can use what we know about cognition to improve perception of dashboards Achieving eloquence through simplicity: A Tufte-inspired discussion of maximizing the data/ink ratio, and of getting rid of filler and unnecessary ornamentation Advantages of graphs: why pictures are worth a thousand wordsThe remainder of the book covers putting theory into practice. I have not read these chapters yet, but am looking forward to them. If you are a Tuftee, you won’t find much in the first half of Mr Few’s book that breaks new ground. At least, not as of 2013. But then again, in 2006, this book was a big deal. It was well-received, sold well enough to merit a second edition, and has been widely cited since. I admire Mr Few very much for writing this book. I don’t get the impression that he was a graphic designer by training. Nor does he appear to have an economics or statistics degree — indeed, I can’t find a résumé or LinkedIn profile anywhere. And he’s not a programmer. Not that any of that matters. Few is clearly a fanatic; he won’t change his mind, and won’t change the subject. The principles in this book don’t apply just to dashboards, however. Every business professional who creates any kind of chart or exhibit can benefit from this book. I can say that with a high level of confidence — and I haven’t even gotten to the really good bits yet. Stay tuned for my review of the second half of the book. "
    }, {
    "id": 26,
    "url": "http://localhost:4000/blog/2013/06/04/tia-m2m-talk/",
    "title": "Cybersecurity for Machine-to-Machine (M2M) Networks",
    "body": "2013/06/04 - This is the nominal text of panel remarks I delivered at the Telecommunications Industry Association’s M2M &amp; Cybersecurity Workshop on June 4th, 2013. The objective of the panel was to discuss the following topic:  Define a cohesive vision for a secure, reliable and economically viable machine network. What are the key objectives and what level of risk can be tolerated? Good afternoon. I am Andrew Jaquith, the CTO of SilverSky, a leading cloud security provider. It’s great to talk to you today. You may not know SilverSky, so first, a little about us and our qualifications:  SilverSky protects our customers’ most important information. We manage customers’ email and collaboration, secure their data with our security software, and monitor their infrastructure for compromises, all from our cloud.  We have 6000 customers, mostly in the private sector, including 1800 in the most risk averse and security sensitive industry there is: financial servicesWe filter 50 million emails a day, and analyze 425 million security events We protect half a trillion in banking and financial assets We have a growing presence in telecommunications and communications service providers. We partner with Cbeyond, Telepacific, NTT, Windstream and — thanks to an an acquisition we are announcing tomorrow — with XO and Peak10While we aren’t a device maker or carrier ourselves, we see a large volume of network traffic and security events every day. We see a lot of activity and have an perspective of what’s going on in the private sector. Let’s talk about machine-to-machine (M2M). M2M means any digital, network-protected device that is part of a larger system. The “M” in M2M means something with an IP address. Everything from ATM machines to smartphones to copiers to energy grid sensors to that networked refrigerator we’ve all been predicting — at least, ever since MIT networked its soda machine in the early 1990s. I remember as a young pup around 1993 when Novell predicted that one day, there would be 1 billion network-connected devices. That prediction seemed audacious then; it is merely quaint now. The “2M” part of M2M means that connected thing is a node in a larger network, and that the communications are only partially directed by a human. A consumer, for example, might own a mobile phone. They will surf the web, buy their kids gifts on Amazon, and play Words with Friends with, well, their friends. There’s nothing about these activities that is different or more interesting than what we have seen on the PC. However, all of the supporting services underpinning the mobile experience — cellular data communications, background telemetry, push notifications, carrier updates — that is all M2M traffic. So are the networked soda machine replenishment signals, SCADA traffic, the cellular tower updates, etc. These are not initiated by humans; these are all machines talking to machines. The reason we are all here: talking about security in M2M. We are here, I think, because so much of what we experience and take for granted every day relies on networking; that is, the “2M” part. Increasingly, all of that networking is under the covers, and not directly perceived or controlled by the consumer or end customer. It is of paramount importance, therefore, that we can trust the networks, devices, clouds and data that underpin the M2M economy. We need to trust the things that filter the water we drink, transmit the power we consume, and connect us to other people. What is does success at securing M2M look like? With something as diverse as M2M, one cannot easily articulate a “vision” for security. There is no single “system” one can articulate a vision for. It’s a “system” in the same way that health care is a system: fragmented, partly analog, few standards, and filled with many parties with competing interests. But the need is clear. Risks abound across the system. A popular grey-hat security research project, for example, has released automated exploits for SCADA systems from Rockwell, GE, Schneider, Siemens and many others — making it relatively easy for attackers to weaponize and use on a large scale. Scary. And these are people who are supposed to be our friends. Then there are those who are not our friends: nation-states such as China, Russia and Iran, which have funded large offensive cyber-warfare teams. It is certain that M2M systems are on the target list.  Rounding out the list of threat actors includes the usual criminal gangs, unsavory hackers, miscreants, attention-seekers, pirates and — arguably the worst of the bunch — Mr Murphy (as in Murphy’s Law). So, defining a vision for M2M is arguably a fool’s errand. That said, if I could suggest one big hairy audacious goal for M2M security, it would be this:  The absence of surprise “Surprise” in the context of M2M means disrupted business, theft of service, successful attacks on critical infrastructure, civil unrest, loss of life or livelihood, theft of secrets or corrupted data. Drilling down a bit more, “absence of surprise” implies four other goals. It implies:  Designing for failure: having compensating processes for dealing with compromises Designing for resilience: making it possible to diagnose, upgrade in the field, and have robust functions in less-than-optimal environments Eternal vigilance: having a strategy for continuous monitoring; for incident handling, and for response activities (often neglected) Risk management: eyes-open knowledge of what adverse events are acceptable, and how frequently they can be toleratedLet me illustrate by example. Ten years ago I helped design of a security subsystem for some hardware devices due to be deployed by one of the most zealous and security conscious organizations around. This organization would do just about anything to ensures that their mission was achieved, that their devices were not compromised, and that they were as protected as possible from the threat posed by attackers. No, I’m not talking about the military, the CIA, or the NSA. I’m talking about cable TV. The job was to design Comcast’s next-generation conditional access system (called DCAS aka True Thru-Way). What was the goal? To design a bulletproof CAS that would securely deliver any programming of the customer’s choice, so they could get anything they wanted and paid for. But — and this is important — not what they didn’t pay for. Also: nobody else could get the programming without paying either. The system we designed had a three key features designed to advance this goal:  Device integrity: keep the device in a known state. This implied that we needed not just a way of keeping a set-top box (STB) from being tampered with, but a way of knowing when it was being tampered with.  Content protection: require encryption between the cable network head end and the STBs. A strategy for hardening the box. Creating a cryptographic “key ladder” with long-lived session keys and ephemeral ones, so that compromising a more frequently used key meant a finite window of time for the compromise. We also needed “secure elements” on the box that would be “personalized” for each unit.  Device updates: develop a way of revving the local STB firmware and updates. That implied having a “root of trust” derived from keys that were managed centrally. We know from watching the experiences of DirecTV (and today, Apple) with “hackers” that adversarial warfare with determined opponents makes defenders stronger. What this meant in our case: lots of crypto. Serious review and iteration. Willingness to learn through evolution. Knowing that you have to walk a fine line between between security robustness, flexibility, usability and ability to manage at scale in the field. Perhaps most important: all of the design decisions were informed by an acceptance by Comcast of exactly how hard it ought to be for a pirate to pop a box and get free TV. How hard should it really be, and what would the company tolerate? Also, Comcast defined which “tail risks” they wanted to avoid. That is, what does catastrophic failure look like? In this case, just for example, Comcast wanted to make sure that other than stealing the topmost root key — which was made very, very difficult — no mass compromise was possible; an attacker would have to go box-by-box. This should give you an idea of what is required to build devices with high levels of security, where that security supports the business goal. For a more modern example, look at Apple’s iPhone. That is a great example of fairly robust security and usability. Fifteen years ago, if I told you that you would see the rise of a consumer computing platform with over 500 million units deployed, where the entire platform includes trusted boot, mandatory access control, full device encryption, mandatory application screening, mandatory application signing from a central authority, a vibrant developer scene, and very little (essentially zero) malware, and one that doesn’t drive customers batty — indeed is one heck of a pleasure to use — you’d say I was nuts. But yet that’s what we have. I don’t advocate Apple’s model per se. But it illustrates one way to try to accomplish many goals, and do them all well enough that the net risk to consumers is very low. On the other side, look at what happened with Stuxnet. The attack was essentially via USB stick plus a stealthy worm that attacked Siemens SCADA systems used to control and monitor centrifuges for enriching uranium. This system runs a variant of Windows. Very few of the ideal security characteristics one would like to see in a robust, secure embedded operations system were in place in this case. (Arguably in the case of Stuxnet this was a feature rather than a bug. ) My wish for the industries that are involved in M2M, looping back to my original comment, is that we design collectively and individually for the absence of surprise. Any surprises you get should be those you expect… And then, of course, they aren’t surprises. They fall into the category of what Donald Rumsfeld memorably called “known unknowns. ” Our eyes are wide open, based on enlightened economic self interest. In addition, I would hope that have enough eyes wide open that many of the “unknown unknowns” are imagined as well. That won’t be good enough in all cases, though. In closing, we will need to consider incentives to swing the calculus to align economic self interest with good security outcomes. Speaking as a trained economist who works in the security field (and who programs to relax), almost all security failures are rooted in perverse economic incentives. Our goal ought to be to align incentives so we get better outcomes. In my view, everything should be on the table: software security liability for manufacturers, legal shielding for sharing of security data and incidents, promotion of industry standards and inclusion of these standards in purchasing guidelines, and, in cases where the risks demand it, regulation or legislation. If we do all of these things, we will have successfully used our collective imaginations to identify, reduce, or willingly accept the M2M risks we face, both today and in the future. Thanks very much for listening. "
    }, {
    "id": 27,
    "url": "http://localhost:4000/blog/2013/02/21/Mulally-leadership/",
    "title": "&ldquo;Everything was green. Mulally thought that was odd for a company losing billions.&rdquo;",
    "body": "2013/02/21 - I have been a fan of the Ford Motor Company ever since I was a boy. There’s no rational reason for it, but then again, experts tell us that brand preferences are formed at very early ages. Somewhere around the age of 10 or so I decided I liked Ford cars. My first car after college was a 1993 Ford Taurus, which I later gave to my sister when I moved overseas. My second car was a 1998 Ford Contour. I changed to a nice little Honda Civic five years ago; at the time, the domestics weren’t looking so great. But I still have a certain patriotic wistfulness about Fords, and probably always will have. For this reason, I’ve been watching Ford’s recovery with interest. Most people know that Ford didn’t take a dime in government money during the Great Recession. It was the only one of the Big Three automakers that did not. Much of the credit for this belongs to Alan Mulally, the CEO of Ford. He made the gutsy and prescient decision to take out a $23 billion loan two years before the recession hit. He used absolutely everything as collateral to get it, including the iconic blue oval logo. Since that time, Ford sold off its troubled and de-focusing Jaguar, Volvo and Aston-Martin luxury brands, built a terrific new line of fuel-efficient cars, whittled the number of cars “platforms” it used globally down to just a few, and steadily increased its car quality. Ford’s near-death experience – and subsequent rejuvenation – have been the subject of many case studies. A good short one, and the impetus for this post, is “An Insider’s View of the Ford Story” from the Ross School of Business at the University of Michigan. In it, Ford COO Mark Fields tells a wonderful anecdote that most of us can relate to:  At a weekly business status meeting early in Mulally’s tenure, charts from top executives didn’t indicate the company was in any trouble. Ford uses a color code for topics — green for good, yellow for a potential issue, red for a problem — and everything was green. Mulally thought that odd for a company losing billions.  Meanwhile Fields, then president of the Americas, had an issue with a product launch that year. The new Edge had a liftgate problem that threatened to delay its critical debut.  “I said, ‘Code it red,’ and they said, ‘Are you sure you want to do that?’,” Fields said. “I said, ‘This is what Alan wants. Let’s go for it. ’”  Finally it was Fields’ turn — Edge launch: bright red. “I could feel the chairs move away from the table,” said Fields. “I said we have a problem, and I’d love to have help from manufacturing and quality to help resolve it. Alan turns to me and starts clapping. The next week, everybody’s chart was like a rainbow. By all accounts, Alan Mulally is a no-BS guy who does not fear hearing bad news. Indeed, everything I’ve read suggests that he encourages his staff to bring problems to the surface so that they can be discussed dispassionately and dealt with. Crucially, he encourages his team to do this without finger-pointing. At Ford, this has helped break through the factionalism that had traditionally plagued the company. As Fields puts it, “Working together has been so crucial for us to get through a very difficult time and work through our issues on our own. ” As described in an older CNN Money story, establishing trust and a culture of openness was a big change. But there’s no doubt that the ultimate referee is Mulally:  There are no pre-meetings or briefing books. “They don’t bring their big books anymore because I’m not going to grind them with as many questions as I can to humiliate them,” Mulally says. “We’ll see them next week. We don’t take action - I’m going to see you next week. ” No BlackBerrys are allowed, and no side conversations either - Mulally is insistent about that. “If somebody starts to talk or they don’t respect each other, the meeting just stops. They know I’ve removed vice presidents because they couldn’t stop talking because they thought they were so damn important. ” Ford’s success isn’t solely due to the leadership qualities of the CEO, of course. Building better quality cars, after all, is the point of the whole exercise. That, the company has done well. But I love this story because it shows how setting the “tone at the top” matters, and that having a positive culture of problem-solving can (literally) pay dividends all around. Public domain photo by Eric Draper. "
    }, {
    "id": 28,
    "url": "http://localhost:4000/blog/2013/02/15/Bully-for-BlackBerry/",
    "title": "Bully for BlackBerry. But Is It Too Late?",
    "body": "2013/02/15 - Last week Research In Motion announced three things:  It had renamed itself to BlackBerry It would soon ship two new BlackBerry 10-compatible devices, the Q10 (with keyboard) and Z10 (touchscreen only) It had shipped the new BlackBerry Enterprise Service, version 10These three announcements, taken together, signaled the end of a long period of frustration for customers, employees and shareholders. After a wait of nearly three years, BlackBerry, indeed, delivered the goods. Reviewers of the Q10 and Z10 have generally been impressed; these are solid handsets. Ditto for the BlackBerry 10 operating system. BlackBerry Enterprise Service 10 includes updated software updated for managing BlackBerry 10 devices and PlayBooks (the BlackBerry Device Service); a new bundled, reskinned, version of the Ubitexx MDM software it acquired in 2011 for managing iOS and Android devices (now called the Universal Device Service); and, an updated version of the server software for routing data to older BlackBerry devices using the “classic” BlackBerry network infrastructure (BlackBerry Enterprise Server). BES 10 also includes an updated version of BlackBerry Enterprise Server Express (aka “the cheap one”) for customers who don’t need all of the power and complexity that the non-Express version offers. At least one reviewer intoned that with its new offerings, enterprises now had a true BlackBerry BYOD (bring-your-own-device) solution. It seems that the wait may have been worth it. Left unanswered, though, is the existential question of whether it matters. Recitation of the facts make for lamentable reading. The company’s share of new handset sales in North America was just under 3% for the most recent quarter, down from 35% just five years ago. Apple came out of nowhere to become the US’ biggest handset maker, something BlackBerry had no answer for. The Android juggernaut, led by Samsung, seems poised to take most of the rest of the market, leaving BlackBerry and Microsoft-aligned handset makers such as Nokia to fight for the scraps. As reported by Asymco’s inimitable Horace Dediu, Apple and Samsung together accounted for 103% of the smartphone industry’s profits – a number greater than 100% because competitors (RIM and others) lost money. The question isn’t so much whether BlackBerry will suddenly exit the market – partial annuity business are nearly impossible to kill. The question is, can it continue to make products that it can sell at a profit? The path to profitability starts with great products, which in ideal circumstances lead to a virtuous cycle of desire, demand, scale, continued cost decreases and increased pricing power. That assumes that the company executes well. It hasn’t in the past. The PlayBook, for example, contained the germ of a good idea but didn’t take root with buyers for lots of reasons. Among them, an asinine and insulting advertising campaign that failed to ignite interest. (Alternatively: “We’re the only tablet that delivers the whole Internet because we have Adobe Flash!” and, “Amateur Hour is Over,”). The PlayBook also initially omitted key features, such as the ability to do on its own the thing that BlackBerry customers prized most: get email. BlackBerry also stubbornly clung to the idea that it needed to be a network provider, years after TCP/IP over cellular had become commonplace. What was once a compelling competitive advantage has turned into disadvantage: an extra cost for customers to bear, and a single point of failure. For context, see something I wrote on the Forrester analyst blog three years ago:  The BlackBerry was introduced in 1999 as a two-way pager on steroids. Back then, TCP/IP over GSM (and other wireless networks) was just a pipe dream. RIM implemented a system by which all traffic is collected from the mobile networks of the sender, funneled through RIM servers and then routed back onto the recipient’s mobile networks and pushed to the handset. In essence, RIM – rather than the Interwebs – provided the routing capabilities needed to ensure that mail and messages are delivered. That was necessary, and worked well, when Internet data plans were not universally available. It gave BlackBerry instant push e-mail and guaranteed delivery. And critically, it was a competitive advantage that no other wireless vendor had. And then, last year, in the wake of the pervasive RIM network outages that swept the globe, I noted the following on the SilverSky (nee Perimeter E-Security) blog:  Data plans that provide TCP/IP over wireless carrier networks are now ubiquitous, nullifying a key RIM advantage. Moreover, push email protocols such as ActiveSync are licensed by the two Post-PC device leaders, Apple and Google. ActiveSync isn’t as good as BlackBerry push email, but it is good enough for most businesses. But in spite of the ubiquity of TCP/IP-over-wireless, RIM continues to do its own thing. Essentially, when you choose BlackBerry, you are making a bet that RIM’s reliability will be better than that of your wireless carrier’s data service. That might have been a safe assumption five years ago, but [with the recent outages] it isn’t any longer. Fast-forward to last week’s announcements. BlackBerry has directly addressed the network issue with the new handsets in two ways. First, to BlackBerry’s great credit, they are allowing the new Q10 and Z10 devices to get email, calendars and contacts over ActiveSync, using regular carrier data networks. Even better, the BES management server appears to be able to communicate with BB 10-compatible handsets over the carrier data networks, too. In short: BlackBerry has signaled its intent to make the classic RIM data service – yesterday’s network – optional. The company has not played this up, for obvious reasons. But it’s a big win for customers. I’ve already gotten inquiries from several SilverSky customers who want to scrap their expensive data plans and use the new devices in ActiveSync-only configurations. This sounds like good news, but is it too good to be true? Some early reviews suggest that BlackBerry’s ActiveSync feature support isn’t complete, with one reviewer finding himself unable to inviting people to meetings, for example. On the CrackBerry boards, many confused addicts can’t figure out whether a separate data plan really is needed for devices that are managed by BES. BlackBerry’s website and documents are maddeningly vague. Worse, BlackBerry’s three administration components – BlackBerry Device Service, Universal Device Service, and BlackBerry Enterprise Server – are not well integrated. Well, they are “integrated” in the sense that the are all part of the “BlackBerry Enterprise Service” 10 family, and some user interface elements are shared. But administrators must read three different manuals. There’s also components called BlackBerry Management Studio, BlackBerry Mobile Fusion Studio and BlackBerry Enterprise Server Express, which have their own interfaces as well. Confused? Old-hand BES-heads probably aren’t. But suppose you are an enterprise CIO who considering, or has already purchased, a product from an MDM arriviste such as MobileIron, AirWatch of FiberLink. Would you “leap backwards” and consolidate everything with BlackBerry’s new services? You might, or you might decide that the company’s management tools still aren’t ready. Compared to the seductive “one-console, one-policy, all-device” visions that the arrivistes are painting, BlackBerry’s looks complex and parochial by comparison. Why suffer through Jackson Pollock when you can run away with a Renoir? BlackBerry, then, has five challenges as I see it. It must:  Stanch the bleeding in its traditional customer base. The new Q10 and Z10 handsets and BB 10 operating system should ensure that wavering customers give the new products a long look. That’s step one. Continuing to keep up a rapid cadence for operating system upgrades, third party app availability and new handset models: that is step two. Step three will be convert existing customers as quickly as possible to the new operating system and server software, locking them in for another few product cycles.  Eliminate any perceived or actual dependencies on the classic RIM network. BlackBerry needs to show customers that the traditional RIM routing and transport service isn’t needed any more, and that it has fully embraced ActiveSync as an alternative mail protocol. It must do this to eliminate the notion that its products are expensive to operate in addition to stodgy (although the “stodgy” perception will be lessened by the new handsets). This will necessarily reduce network and BES revenues but will increase sales of handsets. It is, in effect, a form of cannibalization. BlackBerry executives need to adopt the same attitude the late Steve Jobs did when explaining to shareholders why Apple didn’t fear the prospect of iPads cannibalizing Mac revenues: “if you don’t cannibalize yourself, someone else will. ” Offer a compelling, unified alternative to MDM products. MDM vendors offer the promise of being able to define a single IT policy once, and apply it across all devices regardless of make or model. The reality is different, though; industry’s dirty secret is that most of the MDM products are focused almost exclusively on ActiveSync platforms, and iOS in particular. BlackBerry management features are usually paper-thin. That is partly because BlackBerry doesn’t offer good APIs, and partly because customers need help most with ActiveSync; the BlackBerrys they have are company-owned and well-managed. With focus, BlackBerry could offer a first-class management experience across all device types. But – and this is important – ActiveSync support can’t be half-assed.  Offer “something more” to CIOs. The preceding three recommendations will at best stabilize BlackBerry’s declining share of the enterprise mobility market. To grow it robustly, the company needs to offer something other handset makers and MDM vendors can’t. Some suggestions: (1) extend the “Balance” data labeling technology to iOS and Android; (2) introduce a bulletproof proxy platform that does for ActiveSync what BES did for RIM devices and what Blue Coat did for web security (think: attachment management and stripping; bandwidth management; content inspection; time and location controls; APIs third parties can use, etc); (3) unveil a cross-device, encrypted mobile cloud backup and sharing network (think: a more secure iCloud + DropBox on steroids). These might not be the right sorts of “something more”; but regardless, the focus should be on differentiating versus handset makers and MDM vendors.  Attract consumers again. Although BlackBerry likes to talk about its success in developing markets (Exhibit A: Nigeria), substantial revenue growth won’t come unless it can succeed in developed markets. Consumers, which substantially outnumber business customers, are the key. BlackBerry could do many things to increase its consumer share, ranging from incremental to radical. The most radical step would be ride the coattails of a consumer-friendly OS by shifting platforms yet again, to Android – or better – to Windows Phone. Less radical steps include building products that target demanding “prosumer” segments such as photography, design or programming; bribing popular app makers to develop to BlackBerry first; or… well, I’m at a bit of a loss here. BlackBerry will never be cool. But being seen as “reliable,” “fast” and “trustworthy” might be enough. BlackBerry must do most – say, four out of five – of these things well in order to grow again. If the company does not, it will continue to shrink, slowly ceding shelf space to Apple and Samsung. It must make BlackBerry executives crazy to think that enterprises are willing to grapple with consumer-grade devices instead of soldiering on with their trusty, secure BlackBerrys. It must make them crazier still to know that they must put to pasture one their finest inventions, the RIM network. And yet, that is the world we live in. One where “good-enough” security and management has beaten great, where TCP/IP-over-cellular has supplanted proprietary networks, and where the black-and-white of company-owned has been replaced by the gray of BYOD. It is late in the day for BlackBerry. It’s not too late, however, for the company to pull more rabbits out of its toque. I hope it will. "
    }, {
    "id": 29,
    "url": "http://localhost:4000/blog/2013/02/14/SOTU/",
    "title": "Four Things To Like About Obama&rsquo;s Executive Order on Cyber-Security... and Four to Dislike",
    "body": "2013/02/14 - During his State of the Union Address on Tuesday night, President Obama announced an Executive Order on Cyber-Security. The full text is available in many places, including Wired. I’d urge you to read it in full; it is short and well-written, as you might expect anything coming from this president (or his staff) to be. The Order directs DHS to notify private companies in “critical infrastructure” sectors of any impending attacks by extending the Enhanced CyberSecurity Services program. To promote greater information-sharing, the Order provides a “safe harbor” to companies that share information with DHS. It directs the National Institute for Standards and Technology (NIST) to create a new “Cyber-Security Framework” to reduce risk in critical industries. And to evaluate the success of the program, the Order includes a series of regularly recurring opportunities to review and recommend new actions to take. Understand that the President signed the Order because of lack of a Congressional alternative. Last year’s two dueling cyber-security bills died in session due to partisan wrangling. Republican senator John McCain objected to the initial bipartisan proposal, the CyberSecurity Act, because of the idea that government has a role to play in setting standards, which it clearly does. McCains’s alternative bill, the SECURE IT Act, preserved the CyberSecurity Act’s focus on information-sharing but watered down any additional regulatory oversight. The Order more closely resembles the McCain bill, if only by the necessity that the Order cannot ask agencies to do anything beyond what existing laws allow. I reviewed the Executive Order and found a lot to like in it. But it’s lacking in important ways, too. Here’s what I liked:  The scope of the proposed Cyber-Security Framework is comprehensive. The Framework will ostensibly “help owners and operators of critical infrastructure to identify, assess and manage cyber risk. ” It will identify areas of improvement that can be addressed by the private sector, identify methods for reducing risk, and will recommend ways that companies can measure their success at implementing their programs. This is good. Critical infrastructure companies, particularly those in comparative security backwaters like utilities, need all of the help they can get.  Materials shared by private sector are shielded from discovery. Section 5c of the order states that “Information submitted voluntarily in accordance with6 U. S. C. 133 by private entities under this order shall be protected from disclosure to the fullest extent permitted by law. ” What that means is that any information shared with the government can’t be obtained under a FOIA request, for example. The information could still be discovered in a private suit.  NIST’s Cyber-Security Framework will incorporate industry standards. I have a lot of respect for the work NIST does. I know and have worked with many people in the agency. NIST also regularly collaborates with outside organizations such as the Center for Internet Security (CIS) and SANS. These groups are doing good work as clearing-houses for effective practices. It’s good to see the President explicitly ask NIST to “incorporate voluntary consensus standards and industry best practices to the fullest extent possible. ” The Order offers wiggle-room to define what industries are “critical. ” The specific sectors covered by the Order are not mentioned in the text, but the scope defines critical infrastructure as “systems and assets, whether physical or virtual, so vital to the United States that the incapacity or destruction of such systems and assets would have a debilitating impact on security, national economic security, [or] national public health or safety. ” One can easily imagine that the critical sectors are likely to include energy, utilities, and financial services. But transportation, pharma, and the defense-industrial base would qualify, too, depending on the how the President and his advisors see things. We’ll see how this evolves, but having flexibility here is important. The President’s Cyber-Security Order is important because it puts an important stake in the ground in the absence of legislation. It recommends many important and fine things that we need more of, notably information sharing. But the Order also disappoints because it misses opportunities to do more. Some shortcomings are due to natural limits imposed on the Executive branch. The President cannot propose new regulation, for example. Others are failures of vision. Here are the four problem areas I see:  Participation by private companies is voluntary. The Order directs DHS to initiate an information-sharing program with industry to give them advance warning of attacks, and to obtain relevant information from target companies. The Order also asks DHS to create “incentives designed to promote participation” in the program and to analyze whether those incentives have been effective. That sounds like a tacit admission that they won’t be effective. To be fair to the President, he has no power under existing laws to compel participation; by definition, he must rely on incentives, persuasion, and motherhood-and-apple-pie instincts. Come to think of it, maybe he should send private sector CEOs… apple pies. Until legislation is passed that mandates participation, apple pies might be the best he can do.  Private companies that might have security insights aren’t included. Many large security companies have a significant amount of operational visibility into the day-to-day risks and attacks in critical infrastructure sectors. These include managed security services provides such as Symantec, IBM/ISS, Verizon, Dell SecureWorks and my company. They also include software and security companies that underpin large parts of the “trust infrastructure” that we all rely on, companies like RSA Security, Symantec (née VeriSign), Microsoft and Apple. Although one could file this in the “be careful what you wish for” category, it would seem odd that companies that control the keys to the many critical infrastructure kingdoms, or have visibility about what goes in or out of them, would not be in scope.  Wrong-headed emphasis on technology neutrality. The Order takes pains to emphasize that any guidance issued by NIST should be technology-neutral so that companies can “benefit from a competitive market for products and services. ” Never mind that this sentence makes no sense. The whole sentiment seems wrong to me, because cyber-security is one area where government should make specific recommendations about technologies. It is a fact that some technologies are better and safer choices than others. Divorcing the guidance from the technology turns NIST’s efforts into a big “process” exercise. Process is good, but fixing things is better. All the guidance in the world isn’t going to stop your wide-open Windows NT 3. 5 SCADA systems from being owned if they haven’t been patched since 1995. I don’t want NIST to “name and shame” or “pick winners and losers,” but it should be prescriptive where possible. That’s not a technology-neutral activity.  The framework will take too long to develop. NIST won’t finish its draft Cyber-Security Framework until mid-October. The final framework won’t come until February 2014. NIST offers plenty of vendor-neutral, technology-neutral guidance already, covering everything from risk assessment to metrics. It seems to me that existing materials could be easily re-packaged for critical industries without much effort. Let’s hope the dates are sandbagged, and that we will see drafts sooner than October. Overall, though, President Executive Order for Improving Critical Infrastructure Cyber-Security is an important step forward. Let’s hope it prods Congress into passing something more permanent, prescriptive, and durable, with the regulatory powers DHS needs to get the job done. Note: this article also appears on my company blog at silversky. com. "
    }, {
    "id": 30,
    "url": "http://localhost:4000/blog/2013/02/04/Moving-Securitymetrics/",
    "title": "Moving securitymetrics.org to Octopress",
    "body": "2013/02/04 - Soon, I will be moving the securitymetrics. org website to a simpler, secure and more usable system – the same platform that powers Markerbench. It should be done in time for Mini-Metricon (March 1st, 2013). Some background. When I started securitymetrics. org in 2004 with Dan Geer and Kevin Soo Hoo, I had visions of making a cool, collaborative website for it. I liked the “wiki” philosophy (simple markup, text-based) and thought it would work well as a lightweight collaboration platform. I was, at the time, the co-author/co-architect of JSPWiki, an open source JEE-based wiki package. Because it contained a lot of my code, went the reasoning, I’d know just whose throat to choke when things went wrong. I could customize the server in all sorts of ways when I wanted to. And as a side benefit, I’d get to demonstrate my mad enterprise skillz by using JSPWiki’s four-tier architecture (client, web server, app server, database). That was the theory. But a few things happened between 2004 and now:  Competing content-management systems – like Wordpress and Drupal – got better and better. With bigger dev teams came more features. The wiki software hasn’t kept pace – no social integration, for example.  Spammers overran my wiki self-registation system. As a result, I was forced to disable additional registrations, rendering securitymetrics. org not very collaborative at all.  I discovered I hated maintaining and upgrading that four-tier architecture I had been so proud of.  The wiki server’s memory leaks meant it needed monthly reboots – if I remembered. If I didn’t, it meant lots of downtime.  The threat environment got a lot nastier, leaving me leery of running ANY kind of content management server, never mind a wiki server.  I got married. No explanation needed here, I suspect. It has been clear for a while that securitymetrics. org needed something better. Something simpler, more secure and (preferably) social. After a long period of on-and-off searching, I found something pretty close to ideal: Octopress. As I’ve written before, Octopress is a static website generator; it is derived from Jekyll. You can think of it as a website “compiler. ” You feed it text files, and it emits lovely HTML, CSS and JavaScript with all of the modern goodies built-in: discussion threads and comments via Discus, social integration via Twitter, Google and Facebook “like” buttons, search via Google Simple Site Search, and site traffic analysis via Google Analytics. The best part is that all of the components are 100% static. There are no application servers, no databases, no users; and, no user input accepted or stored. All of the dynamic behaviors result from loading up various bits of client-side JavaScript to invoke other people’s stuff. That makes the attack surface pretty close to zero. And, because everything is static, all that static stuff can be WAN-accelerated until it screams, and stored just about anywhere (Amazon S3, GitHub Pages, Heroku) without worry. What does this mean for securitymetrics. org?  We will switch over to the new site at or around Mini-Metricon on March 1st.  The new website will be better looking and much better organized. I’ve edited and re-organized ALL of the Metricon content, for example, while I was at it.  I will be the sole “publisher” of the website, at least for the time being, but welcome all collaborators.  We will have a new workflow for people who want to write on the securitymetrics blog. It will involve Markdown and DropBox.  After Mini-Metricon I will probably move the mailing list to a new, faster, provider (first things first: website, then listserv)For those of you who will be attending Mini-Metricon, I am looking forward to showing you the new site. "
    }, {
    "id": 31,
    "url": "http://localhost:4000/blog/2013/01/29/All-Posts-On-Markerbench/",
    "title": "All Andy&rsquo;s Posts Now on Markerbench",
    "body": "2013/01/29 - As part of a continuing experiment with static blogging, I have moved all of my historical blog posts from securitymetrics. org to Markerbench. com. Everything is now here, including the somewhat notorious essay Escaping the Hamster Wheel of Pain, which introduced a certain rodent-related metaphor to the security trade and served as the introduction to my book, “Security Metrics: Replacing Fear, Uncertainty and Doubt”. For the curious, here’s some background on why I moved everything here: The securitymetrics. org site has for many years been running on JSPWiki, a Java Enterprise Edition (JEE) application that uses “wiki text” as a markup language. It has served me well, and I am proud to have been one of the platform’s primary authors. However, my (older) deployed version of JSPWiki has suffered from a slow memory leak that has required me to restart the web app container about once per month. I have also had to disable site registration and commenting features, due to the lack of a reasonably-bulletproof spam filtering system. That meant that securitymetrics. org had become essentially a static website. So, why go to the trouble and expense of hosting it on a complex web app server? Now that I’ve gotten the hang of Octopress, a Jekyll-based static web publishing system, the time was right to make the move to a much simpler alternative. And because I had used securitymetrics. org as a personal blog, it seemed like a good idea to move all of the bloggish-type posts here. Moving everything to Octopress means that I now write in Markdown, John Gruber’s elegant markup language. Among other things, that gives me the ability to use my preferred writing application, iAWriter – a beautiful writer’s tool that synchronizes with iCloud and hence, with all my devices. Markdown is a simpler markup language than wiki text; similar in many respects but with some key differences:  Headings: in Markdown, you create a new heading this way: # (Heading 1), ## (Heading 2), etc whereas in wiki text you use are !!!, !!, and ! respectively. I find the Markdown syntax a little more economical.  Emphasis: in Markdown, you emphasis text by surrounding it with _ (for italics) and __ (for bold). In wiki text you use '' for italics and _ for bold. Not a big difference, but logically, it makes sense that more emphasis means more characters to type (2 for bold versus 1 for italics).  Code blocks and inline code: In Markdown you indent text by four or more spaces to indicate a code block, and you indicate inline code by enclosing the text in back ticks (`). In wiki text you enclose the text with triple and double curly braces.  Hyperlinks: You create links in Markdown with the hyperlinked text enclosed in square brackets, and the link itself in parentheses, for example this snippet [link](http://www. markerbench. com) links to my blog. In wiki text, you use square brackets separated by a pipe eg, [link | http://www. markerbench. com]. Both are lightweight enough for my needs; the Markdown syntax is slightly easier. I’ll miss JSPWiki’s neat table syntax, which allowed you to create tables simply and cleanly (|| head 1 || head 2 || head 3 for header rows and | cell 1 | cell 2 | cell 3 for regular rows), but you can create tables in Markdown simply by passing through HTML. That’s ok with me; I haven’t seen a clean way to do tables on any platform. Supposedly-“WYSIWYG” editors – such as the one in Wordpress – mess tables up regularly. Writing them out manually is a little more work, but not too much. Apart from the syntax itself, one difference between Octopress and JSPWiki is the way post metadata is stored. In JSPWiki, metadata such as the author name is stored in . properties files; last-modified times are whenever the file was last touched. In Octopress, one stores this and other information in the Markdown document itself, right at the top, in YAML syntax. For example, here’s the YAML for this post: ---title: All Andy's Posts Now on Markerbenchcreated_at: 2013-01-29 07:00:00 -0500layout: postcategories:- blog- applicationscomments: true---Pretty simple stuff; you need to remember a few basic rules. Beyond the syntax, though, I like the simplicity of the system as a whole. By turning the blog into something that I “publish” with simple command-line invocations, I get rid of a lot of headaches. Instead of worrying about a web application that I need to maintain, upgrade, and secure, I only need to worry about my writing. (And the occasional GitHub update. ) Being the industrious-lazy sort, to move the blog posts, I created a little Ruby script that munged the wiki markup and produced decent Markdown. All of the posts needed a little work done to them, mostly to fix a few bullet issues my script didn’t account for, and to assign categories to each post. At some point I will post the script to GitHub, as soon as I get the hang of that. I’ll also, for the sake of completeness, I will likely cross-post a few notable essays from my work blog. After I have tinkered a bit more with the Markerbench site, I’ll go ahead and move http://securitymetrics. org to Octopress as well, right in time for Metricon 8! I may, or may not, put the site contents on GitHub pages, as I have with this blog. Regardless, making it a totally static website makes it simple to host and scale. In addition, moving the rest of the securitymetrics. org site will allow me to use a much less expensive hosting plan. The only thing I’ll need a hosting subscription for, at that point, is to host the securitymetrics. org mailing list. "
    }, {
    "id": 32,
    "url": "http://localhost:4000/blog/2013/01/21/paving-over-the-proprietary-web/",
    "title": "Paving Over the Proprietary Web: The Java Security Bigger Picture",
    "body": "2013/01/21 - Perhaps you’ve heard about the recently disclosed Java 7 zero-day exploit. The flaw allows a remote attacker to take complete control of a computer. It has been incorporated into many exploit kits. The Department of Homeland security regards the Java exploit as sufficiently serious to recommend “disabling Java in web browsers until adequate updates are available. ” Oracle’s fixes — aren’t. Many of my colleagues at other security firms have spilled a lot of ink describing why this particular Java exploit is bad. It is indeed that bad; Apple, for example, has forced down an update that blocks the Java 7 plugin from executing in the browser at all, at least until Oracle is able to distribute an update. If you are in the habit of keeping Java switched on in your browser, you should turn it off — of course. But that isn’t always possible. Client-side Java, for example, powers GoToMeeting. Many other companies — including my own — rely on client-side Java for critical functions. So one cannot simply rip it out, or mandate that it be banned. Reality has a habit of messing up the best-intended recommendations. But make no mistake, at some point very soon Java on the client needs to go. CIOs, please take note. Client-side Java is part of the web’s proprietary past, and its time is ending. That proprietary past also includes ActiveX and Flash, two other technologies that saw widespread adoption in the early 2000s. That all three of these technologies came of age at roughly the same time isn’t a coincidence; they all filled gaps in the web experience. ActiveX was Microsoft’s way of adding native client functionality to a then-crude web experience; client-side Java (Swing, Java Web Start etc) did the same. Flash and its cousin ShockWave provided smooth video and animations. Since 2005, though, the native web has changed dramatically, and for the better. HTML 5, CSS and JavaScript toolkits have been the major catalysts of a revolution in web design. The canvas element added to HTML 5, for example, allowed standards-compliant browsers to draw shapes, create and fill paths, and animate objects. This, plus the video element, freed designers from needing Flash. Cascading Style Sheets (CSS) Levels 2 and 3 gave designers increasingly pixel-perfect control over the placement and appearance of content — a task made even easier with CSS pre-processors such as LESS and Sass, and with kitted CSS assemblies such as Twitter Bootstrap. On the JavaScript front, third-generation toolkits such as jQuery made it simple to make websites dynamic and responsive. You can do all of these things for free, without needing to buy any of the various Studios from Adobe or Microsoft. The slow-motion revolution in how the Web is made means that the raîson-d’être for proprietary web technologies is going away. Like a lumbering concrete mixer, HTML5 and JavaScript are slowly paving over the parts of the web that had previously been occupied by Flash, ActiveX and Java. Ironically, the vendors of these proprietary technologies have, in their own ways, added limestone, clay and water to the paving machine. Microsoft, for example, turned an entire generation of web developers against it with its long, and ultimately fruitless, resistance against robust CSS support in Internet Explorer. Although modern versions of IE are highly standards-compliant, Internet Explorer did not pass the CSS Acid3 test until September 2011. Any web developer who has been working with CSS for more than 5 years, for example, can probably regale you with stories of massive hacks needed to allow older Microsoft browsers to work with standards-based websites. The roots of Adobe Flash’s decline are a little different. Nothing was “broken” with Flash, functionally speaking1. Two related events resulted in a decline in Flash usage: Steve Jobs’ public refusal to add Flash support to the iPhone and successor iOS devices; and Google’s decision to convert its vast library of YouTube clips to HTML 5-compatible WebM and H. 264 formats. These actions, plus the increasing viability and efficiency of WebM and H. 264, meant that you didn’t need Flash video any longer. This has clear implications for customers. For customer-facing websites, you can (and should strongly consider) retiring Flash video in favor of H. 264. This is a quick win; the re-encoding process is relatively quick and painless. That said, the need is not as urgent compared to Java. Adobe’s security team (under the leadership my former @stake colleague Brad Arkin) has upped the tempo of bug fixes, adopted auto-update, and is taking security seriously enough that Flash has become less risky than it had been. Still, if you could remove a dependency on a third-party component that needs to be maintained and updated in addition to the base operating system, why wouldn’t you? Java, on the other hand, is simply a mess. From a pure features perspective, Java’s caretaker parent, Oracle, no longer employs the kind and number of Java engineers that will keep it up-to-date — never mind put it back on the cutting edge. Most of the Java engineers and visionaries such as James Gosling, Josh Bloch, Tim Bray, Amy Fowler, and Adam Bosworth — the people I learned from and looked up to while I was learning Java J2EE — left long ago to greener pastures. Although server-side Java is still widely used, nobody I know would consider it for greenfield development for use with a browser. 2 From a security standpoint, it is hard to see why Oracle would be Johnny-on-the-spot with security fixes. As my other (!) former @stake colleague David Litchfield has pointed out, the company doesn’t have the best track record on security. We can reasonably assume that fixing client-side Java security holes isn’t anywhere near the top of Oracle’s priority list. And even if it becomes so because screaming customers demand it, legacy products get legacy engineers. That’s just the way it is. The same goes for Microsoft’s ActiveX. Developers don’t use it for new web-based projects, and the company has for several years recommended that developers use other technologies3 to make dynamic websites. The risks associated with ActiveX continue to be high, no doubt because ActiveX controls are basically chunks of native code written by various vendors of varying skill, remotely triggered by websites that may or may not be under the user’s control. (What could go wrong with that?) To be sure, Microsoft has done as much as any vendor in the industry to set the standard for responsible and secure development practices. Over the years, they have responded relatively quickly to the various ActiveX security issues that have popped up over the years. But as with client-side Java, it’s legacy technology maintained by legacy engineers. It is much, much easier to talk about how the slow-moving concrete machine that is the modern web — HTML 5, CSS, and JavaScript — will slowly pave over the proprietary web. It is harder to state with confidence what it will mean for security. However, one may hazard a few guesses. The decline of these three technologies should increase the overall level of security over time. Logic dictates that a browser festooned with fewer proprietary plugins is a more secure browser. Put differently: migrating older websites to use CSS, HTML 5 and JavaScript support will have the effect of concentrating the attack surface by reducing the number of parties who must defend that surface. Over time, the broad public ought to be better served by having Apple or Google or Microsoft be responsible for the entire web browsing experience — including security. But in the short term, it won’t be so clean. Based on vulnerability counts — an imprecise metric at best — the “younger guys” don’t score well. For example, the US National Vulnerability Database shows that the WebKit browsing engine had over 198 disclosed vulnerabilities last year. Internet Explorer? Just 61. Meanwhile, ActiveX, Java and Flash had 73, 169, and 67, respectively. I draw no other conclusions from these data, other than the simplest one — increased use of native browser capabilities is likely to increase risks in the short term, even as the decreased use of proprietary technologies decreases it over the longer term. At some point the two lines will cross and we will all be better off. In the meantime, the cement truck keeps rumbling. 1 Functionality aside, Flash’s security track record has been poor for a while. 2 Java development is alive and well on the Android platform, of course. 3 It’s fair to say that Microsoft has been all over the place on this subject over the last 10 years: DHTML, XAML/SilverLight, and now Windows 8-style apps. "
    }, {
    "id": 33,
    "url": "http://localhost:4000/blog/2013/01/17/phoenix/",
    "title": "Review of Gene Kim&rsquo;s novel, &ldquo;The Phoenix Project&rdquo;",
    "body": "2013/01/17 - Over the Christmas holidays, I read an advance copy of Gene Kim’s first novel, “The Phoenix Project. ” Gene’s co-authors were Kevin Behr and George Spafford. It was a better read than I was expecting. It is about 350 pages. Here’s my review. The book aims to describe how to bring TQM and “lean” (as in, “manufacturing”) disciplines to IT. Although TQM is especially important in the context of operations, the book shows how “systems thinking” that spans the development and IT operations organizations, and reaches upstream into finance, sales and marketing is critically important for technology-reliant companies. Because all but the most hidebound companies rely on IT to run (and transform) their businesses, the lessons in this book are generalizable to every company. The book is less of a dramatic novel than a disguised set of parables about IT and DevOps, using a fictional company, crisis and characters to illustrate them. Most lessons are imparted by a mysterious “board member” named Erik, who serves as a combination Greek chorus, Zen master, drill sergeant and mentor to the main protagonist, a generically named midrange IT operations manager named Bill. Bill receives a battlefield promotion into a role he is unprepared for while, all around him, systems are crashing and strategically important projects are running horrifically off track. Most of us who have been in IT roles for a while can relate. The authors are at their best when they focus on the technology and related (often broken) processes that permeate most companies’ IT processes. The protagonist Bill’s evolution as a manager from fire-fighting and free-fall, to productivity and proactivity, is well-done. As he embraces his new job, he slowly begins to understand the four types of work: business projects, IT projects, changes, and unplanned work. He also learns the tools he needs to increase his ability to do all four types of work using techniques such as simple change management meetings, kanban boards, monitoring processes and targeted, cross-functional “tiger teams” that automate the ability to deliver finished work more quickly. It is very clear that the authors have a soft spot for the Agile and Lean styles of application development, the hallmarks of which feature short two-week development “sprints,” tight feedback loops between operations and development, and continuous integration workflows that build, test and deploy finished systems in minutes or hours rather than in weeks. The type of skills and tools needed to successfully implement these types of processes are loosely called “DevOps,” and they are what have enabled companies like Flickr and Twitter to innovate quickly. By the end of the book, Bill’s company has embraced this thinking, too. Although there is a certain amount if hand-wavey “and then a miracle occurred” glossing over of how Bill and his formerly bedraggled band of IT misfits manage to pull it all off, it’s fun to watch. In contrast to the care and attention spent on describing IT processes, the characters through which the various lessons are imparted are paper-thin; we don’t really get to know them all that well, and it is not obvious why we should care about their fates. Like in a typical Tarantino film, the dialogue is mostly one voice spoken by multiple characters. I don’t think that is a major flaw, though; most readers are not going to read Gene’s book expecting another Navokov, or even the Second Coming of Crichton. Overall, the book succeeds in its goal of communicating complex processes by way of extended example. If you want to understand the most important ways in which modern, technology-reliant companies need to transform their IT organizations, this book offers a valuable introduction. If I had one criticism, it is that I found myself wishing that the “lessons” from each chapter were summarized at the end of each major section, textbook-style. And although I had some trouble getting through the first 20 pages or so — which bogs down with exposition and character development — the rest of the book flew by. I read “The Phoenix Project” in essentially one sitting with a break for lunch. I liked it much more than I thought I would, and recommend it. "
    }, {
    "id": 34,
    "url": "http://localhost:4000/blog/2013/01/08/static-blogging/",
    "title": "Outsource your web risks with a static website",
    "body": "2013/01/08 - A few weeks ago I put together my annual Predictions blog post for the coming year. In that post and accompanying webinar, I suggested five emerging risk areas that CISOs need to pay attention to in the coming year. These are:  CISOs will wrestle with the risks of “as-a-Service” platforms Android’s security issues will force CISOs to take action Cloud application vendors will compete on metrics California will become the de facto privacy regulator Your password policy will undergo a major overhaulOf these, prediction numbers 1 and 3 both related to cloud services, and to the security thereof. The first one, about Platform-as-a-Service (PaaS) was by far the one that I spent the most time thinking about. That is because PaaS is the one that CISOs have the least amount of control over. It is the sneakiest. From CISO’s standpoint, knowing that large parts of your developer toolchain (source code repository, test VMs) and runtime environment (web servers, databases) is sitting out there in “the cloud” is scary, not least because these parts didn’t exactly go through the traditional procurement channel. Even worse, your typical IT security auditor isn’t really going to know what to do with PaaS, other than slap hands on face, MacCauley Culkin-style, and make a beeline for the exit. {% asset paas-2013. png alt=”Platform-as-a-Service Ecosystem” %} However, in this post I will describe one way in which the use of pervasive — and free — platform cloud services can actually reduce risk. That may sound ridiculous on its face, but I offer one worked example that proves the point: static websites. Static websites are exactly what you might think: websites whose content is entirely static. The server never executes any business logic: it simply retrieves whatever is asked for and serves it up to the client. By “whatever is asked for” we mean plain-old HTML, images, cascading style sheets, JavaScript or anything else that makes up the website. Static websites depend on four principles:  Websites are “compiled” offline on a workstation, and “published” by uploading to the server Servers execute no code, and only serve static resources All dynamic features execute on the browser via JavaScript Third party services provide “outsourced” commenting, social and analytics features via JavaScript, which are removed from the server’s areas of concernBecause the web server is not doing anything other than serving up static resources, it can be dumb as a bag of hammers, and locked down within an inch of its life. Even better, the simplicity of the server results in a radically reduced attack surface; there are no “user accounts,” no databases, and no middleware. Because the server does not need to, and indeed cannot, accept any user input, no application code needs to be audited. A new wrinkle on an old idea: Static websites are not new. They have been around for a long, long time — as long as the World Wide Web. In fact, prior to the invention of CGI and early server-side scripting languages like Cold Fusion, all websites were static. But in the mid-1990s, developers began adding server-side languages and scripting frameworks to make websites more dynamic. These include PHP, ASP, JSP, and more recently, server-side JavaScript implementations like Node. JS. Developers have also increased the number of components that collaborate server-side, too. In the early days, simple static websites required only a web server. But modern dynamic web applications are composed from many architectural components in addition to the web servers themselves. These additions include the various server-side scripting languages, plus application servers, application code, databases, and directories. And that is just for the simple applications. Even the humblest business website that serves up nothing more than corporate information from a content management system (CMS) needs most of these components. A site like that needs a web server (for example, Apache), scripting language (PHP), content management system (Drupal), database (MySQL) and a directory for authentication (Active Directory). That’s five components, and collectively they aren’t doing all that much. In contrast to the complexity of modern web applications, static websites turn back the clock on the web. The static website philosophy mixes old-school web publishing and new-school DevOps. If you want an example of old school, for example, look at my friend Dan Geer’s website or a representative posting on it. Dan’s site is just text; no flashiness, and no graphics. On its face, Dan’s site and mine are similar in one major respect. Both offer the same thing: static resources served up by a dumb server. Why not compile your website instead?: Modern static websites differ from their old-school cousins in two ways. First, the highly automated, explicitly developer-centric processes used to produce them feature many of the same tools used to produce code. Authors write posts using plain text editors rather than a WYSIWYG editor or CMS. They “check in” their posts into code versioning repositories the same way they check in their code. After the post or page is ready to publish, a designated DevOps person — perhaps the author — types a few commands to “compile” the site and upload it. Some static website aficionados have automated the process completely: one simply saves a new version of a post to a designated directory, and the website compiler automatically checks the page into GitHub, regenerates the site from scratch, and publishes it to the web server. The second difference between modern static websites and their old-school cousins is inclusion of dynamic features by deliberately “outsourcing” them to other, usually free, service providers. Instructions on the static web page cause JavaScript code to be loaded and executed, which communicates with the provider’s service and provides the illusion of dynamic behavior. This allows site owners to include modern features would ordinarily require server-side code. Years ago, if you needed analyze website visitor traffic, you would install WebTrends on the server. Today, you just pop in a couple of lines of JavaScript for Google Analytics (free). If you wanted commenting features with protections against spam, you needed an application that had a back-end database and a decent anti-spam filter, like WordPress. Now, you can simply embed Disqus (which is also free). Or suppose you wanted to allow visitors to recommend and share items on your website. Traditionally, you’d need to create a web form, hook it up to an email server, and create scripts to send recommendations via email. Now, all you need are a few JavaScript statements to load up Facebook’s “Like” button, Twitter tweet and follow buttons, or Google’s +1. Dynamic features aren’t just the only parts of the website that can be outsourced. The underlying web servers can be, too. For example, GitHub provides a free service called GitHub Pages that allows developers to upload HTML and other static resources. These are served up just like a website. Amazon S3 provides a similar service. For low-volume websites like this one (ha!), Amazon S3 is completely free. Outsourcing risk: Static websites are simple, and require just one architectural component: a web server. By contrast, the typical corporate website that does nothing more than serve up company information, and forward leads to Salesforce, nonetheless requires five. The simpler website is better because it is less complex, and less complex is good. But that is not the only advantage static websites have. Modern web applications aren’t just complex, but risky as well. They typically need to reach beyond the DMZ’s back-side firewall to resources inside the company; for example, to a database or three, or to an Active Directory forest. These additional network connections confer a corresponding amount of risk. Then there’s the setup, operations and maintenance tasks. Each architectural component needs to be configured, hardened, horizontally scaled, patched and monitored — forever. But when you create a static website, most of the complexity goes away, along with the cost and risk associated with each. If you choose to outsource the remaining architectural component — the web server — to a third party, that goes away too. Why not let the fine folks at Amazon, Heroku or GitHub configure, harden, scale, patch and monitor the web server? They are likely to be better at it than you are. Simplifying your architecture by eliminating complexity — and outsourcing the web server — eliminates a huge amount of security risk by cutting the attack surface nearly to zero. But the outsourcing of dynamic features such as user tracking, commenting, social sharing, analytics has another side-effect. Because the web server processes no user-generated content, a whole class of application and data-related security risks goes away. Cross-site scripting, SQL injection, parameter tampering, and the rest of the OWASP Top Ten are no longer worries. You don’t have any potential data breach obligations because you don’t keep any data. There is nothing to steal. Of course, just because you no longer need to worry about application and data-related security risks, your outsourced comment-management service (Disqus) still does. They, and Facebook, Twitter and other providers your client-side JavaScript links to still need to police their members for spam, fraud, impersonation, and identity theft. They need to secure their JavaScript APIs and web applications. But you don’t need to do any of these things any more; with a static website, you have essentially outsourced your risks to them. Indeed, it is more correct to say that you have transferred them. A few security risks remain. Access to servers that host static content must be controlled. If you manage those servers, you need to manage the SSH keys or passwords used for uploading content. And you should probably restrict the number of people allowed to operate the website compiler machinery to a few. And of course, you also need to worry about, um… a bunch of other, er… important stuff, like for example… let’s see… Honestly, I can’t think of anything else. Lock down the web servers and make sure only the right people can compile and post. That shouldn’t be too hard, right? Static websites aren’t for everybody. They still require a certain amount of developer savoir-faire, and they won’t reduce the need to build genuine web applications for business units. You can’t build a static e-commerce site or anything stateful, for example. But if you are a security-conscious company that just needs an online presence, static websites might be just the thing. If you disagree, feel let me know in the Comments section below. I’m using Disqus — of course! Coda: the making of this web page: I became interested in static websites several months ago when I read a few stray articles about the concept. But it wafted past me like so much second-hand smoke; I didn’t really inhale. However, after I did my Predictions webinar in December, I began spending more time digging into the capabilities of “new school” free-ish web service providers such as Heroku and GitHub. At a holiday party, my friend, neighbor and Drupal guru Stephan pointed out that these days it is pretty easy for a motivated developer to assemble a complete app infrastructure more-or-less for free. A few weeks later, to support one of my professional hobbies, I opened a repository on GitHub. Shortly thereafter, I read a few more articles about static blogging and started connecting the dots. I decided it would be fun to create my own static website to prove the concept. But to make it interesting, I wanted to create something representative of what most people would want. That meant that it needed to have the typical kinds of things you would expect, such as commenting features and social integration. I downloaded and started experimenting with two static blogging packages with a lot of buzz, Nanoc and Jekyll. Both implement the “website compiler” strategy: you customize some templates, write a few posts in Markdown and then type a few commands to generate the site and upload the contents to a web server. After starting with Nanoc and finding myself a little frustrated, I moved on to Jekyll. I was halfway through my proof-of-concept with Jekyll when I discovered Jekyll Bootstrap, a more kitted and polished version of Jekyll that didn’t have the some-assembly-required feeling. But finally, I discovered Octopress. It too is based on Jekyll, but includes pre-configured support for Google Analytics, Discus, Facebook, Twitter and Google+. In short, exactly what I wanted. So I got to work getting a feel for the software, started drafting this post, and after about a day or two of after-hours work, things looked good. I needed to find a place to host the blog and decided on GitHub Pages, which is part of my GitHub account. While I was at it, I created Google Analytics and Disqus accounts. All pretty easy to do. Octopress worked pretty nicely once I got over a few self-imposed obstacles. What you see here, on this page, is a totally out-of-the box standard version of OctoPress, with nothing more than a few titles and text properties changed. [Author’s note: as of February 1, 2013, Markerbench is no longer out-of-the box; I now build it using a brilliant Twitter Bootstrap-derived theme created by Adrian Artiles. ] With a little more effort, maybe someday I’ll be able to make something as nice-looking as the Trail of Bits website. One can but dream, no? [Author’s note: it turned out to be a fairly straightforward weekend project. ] As for this blog post: I initially set out to write something very silly about how cool it was to try my hand at this. But the post kept getting longer. Whoops. "
    }, {
    "id": 35,
    "url": "http://localhost:4000/blog/2008/02/19/Every-time-you-perform-arithmetic-operations-on-ordinal-numbers-God-kills-a-kitten/",
    "title": "“Every time you perform arithmetic operations on ordinal numbers, God kills a kitten”",
    "body": "2008/02/19 - I was reading Rich Beijtlich’s blog today, and came across that quote from a commenter known only as JimmyTheGeek. Wonderfully funny, and spot on. "
    }, {
    "id": 36,
    "url": "http://localhost:4000/blog/2008/02/05/Passwords-O-Plenty/",
    "title": "Passwords-O-Plenty",
    "body": "2008/02/05 - Before the holidays I ran a quick, three-question, survey of the securitymetrics. org mailing list membership about the number of passwords people use. Here are the results, drawn from 51 responses (not bad, considering the list membership is about 400 people). I’d promised the respondents that I’d share the results… so here they are. Securitymetrics. org Quickie Survey: Online Credentials: 1. How many online accounts do you manage, in total? How many “sensitive” accounts do you maintain? By “account” I mean a public or private website, server or network that you log in to, for which you maintain a password or other credential. For example, a password or application entry in an OS X Keychain could be considered an account. For purposes of this question, “sensitive accounts” means ones that you would consider problematic if they were compromised. Typically, these could be accounts that keep credit card information, manage your 401k details, or contain employment details. Results (n=51): MetricAll accountsSensitive accountsMean60. 7 accounts20. 6 accountsStandard deviation55. 029. 7Min30First quartile23. 56Median4015Second quartile72. 525Max207207Mode4020Comments: I draw 3 conclusions from these figures.  First, people have lots of accounts to keep track of – on average.  That said, the quartiles and median show that respondents skew towards the “conservative case” – that is, they most don’t tend to maintain too many accounts. A few crazy outliers (like me) are pushing the average number up.  Third, the ratio of sensitive-to-non-sensitive accounts stays fairly constant across quartiles, ranging from 26-38%. In other words: of all of the account passwords people maintain, it’s a fair bet that about a third of them will be “sensitive. ”I’d also note that the survey base is self-selected – in the sense that it’s the members of this list. Most of us are professional paranoids, right? Not sure if that means that the average user is worse off than the respondent base (more passwords to keep track of) or better off. Regardless, I’d say it does confirm what I already knew: we’re drowning in passwords. Further insights or armchair-psychology comments welcome. 2. What is your primary coping strategy for managing your online accounts?  I keep all of my passwords the same: 10% I write everything down on paper: 12% I use a form-filler product, like Apple’s Keychain, and use random passwords 12% No particular strategy: 20% Other: 47%Comments: I can’t draw too many conclusions from the responses to this question, because I asked it badly. Considering that my day job is as an analyst, you’d think I would’ve asked this question in a way that got better answers. :) 3. Do you like the idea of surveying securitymetrics. org members about security practices?  Yes: This is a good idea: 92% No: I’ve got enough spam as it is: 8%Comments: Everyone seems to like the idea of surveying the membership more often. Cool! I’ve asked mailing list members to suggest ideas for future surveys. Note: I’ve proposed that we spend some time on the subject of community-building at this year’s Mini-Metricon at RSA. More on this later… Betsy Nichols is going to put up a blog entry about Mini-Metricon on the website later today. "
    }, {
    "id": 37,
    "url": "http://localhost:4000/blog/2008/01/31/Retired-Comedians-and-Missed-Opportunities/",
    "title": "Retired Comedians and Missed Opportunities",
    "body": "2008/01/31 - There’s this old joke about a comedians’ retirement home that goes something like this: An aging comedian decides to retire to a community that has just other comedians living in it. On his first day there, he does down to lunch, and there’s a bunch of retired fellow comics sitting around the table. The conversation they’re having puzzles the man a bit. One of comics at the table yells out, “12!” and everybody just dies laughing. Then another one says, “44!” and a three of them laugh so hard they roll straight out of their chairs and onto the floor. When a lull in the conversation comes, the new guy introduces himself, and asks, “Hey, what’s going on? What’s so funny about yelling out numbers?” One of the comics says, “Oh, you’re the new kid on the block, eh? Here’s what’s going on. We’ve all been retired for many years. We’ve been telling and re-telling the same old jokes for so long, we’ve assigned them all numbers. To save time, instead of telling the joke again, we just say the number!” “Wow,” says the new guy. “I’ve never seen that before. That’s pretty cool. Mind if I join you?” “Sure,” the other comic says, and beckons him to sit down. The new guy is eager to fit in. So five minutes later, he yells out, “28!” NOBODY laughs – you could’ve heard a pin drop. His voice qwavering, the new guy asks, “What’s wrong? Isn’t number 28 a good joke too?” “Sure it is,” pipes in the other comic. “But it’s all about the delivery!” I mention this because I can’t stand Jeff Jones’ quarterly festivals of FUD. Rather than complain yet again, and in detail, about how dumb vulnerability-counting is, why the methodology is flawed, why it has limited bearing on security, how the system is easily gamed, why it’s colored by Jeff’s obvious agenda, and why it’s a tragedy that Microsoft does not do what it should, namely mine the world’s most complete bug databases and code repositories for truly compelling information about code quality and application security metrics. But I won’t do that again. I’m just going to, like these comics, just yell out the shorthand. “Jeff Jones. ” Note that I’m not laughing. "
    }, {
    "id": 38,
    "url": "http://localhost:4000/blog/2007/12/31/One-Prediction-for-2008-Site-Specific-Browsers/",
    "title": "One Prediction for 2008&#58; Site-Specific Browsers",
    "body": "2007/12/31 - I’ve noticed that sometimes it takes two or three “pings” for an idea to seep into my consciousness.  I just got my second “ping” on a potentially Big Idea: site-specific browsers (SSBs). Some background. Recently my professional research has taken me far down the anti-malware rabbit-hole. My latest report, “Herd Intelligence Will Reshape the Anti-Malware Landscape” has generated a fair amount of positive buzz. The report details how, among other things, online identity credentials have become the common currency of a new professional criminal class. But now that the report is in the can, I almost wish I’d waited a few months before writing it. That’s because perhaps one of the most elegant solutions for banks, stock trading accounts and credit unions seeking to combat identity theft might be one of the simplest. In essence, instead of having banks worry about whether the user’s general-purpose browser is secure, why not require the user to run a dedicated browser that won’t allow access to websites other than those its creator intended? Indeed, if I had to make a prediction, I’d say that the future of Internet banking might look a lot like Todd Ditchendorf’s Fluid site-specific browser. For a well-written overview of what’s going on with SSBs, see Chris Messina’s article, Fluid, Prism, Mozpad and site-specific browsers. I’ve got some e-mails in to the Mozilla and Webkit teams, to prepare for the research note. More details as they emerge. But in the meantime, I expect that we’ll all start hearing “site-specific browsers” and “security” in the same sentence, a lot, in 2008. Remember, you read it here first. "
    }, {
    "id": 39,
    "url": "http://localhost:4000/blog/2007/12/04/Meta-Conclusions-from-the-Chinese-Honeynet-Project/",
    "title": "Meta-Conclusions from the Chinese Honeynet Project",
    "body": "2007/12/04 - If you are involved in your firm’s desktop security strategies (Windows in particular), you should read this: Characterizing the IRC-based Botnet Phenomenon This is a fact-filled but eminently readable paper about 3,290 IRC-based botnet command and control networks in China from June 2006 to June 2007. In addition to doing the normal things you’d expect to see in a botnet analysis, the researchers analyzed the extent of malware samples circulated within the botnets. They also attempted to determine the effectiveness of nine anti-virus engines in detecting the samples in circulation. If you don’t want to read the whole thing, I’ve put together the Cliff’s Notes, at least from the perspective of a data junkie like me. Here are some of the more interesting metrics from the report. Some of these are from the report itself, and I’ve derived others. Editorial comments are in italics.  Number of infected bot nodes: 1. 5 million over the reporting period, for those 1904 botnets they could analyze in detail. Average botnet size over the period was about 800 nodes. Biggest was 50,000 nodes Botnet nodes are strongly diurnal, caused by infected machines being powered off at night Average lifetime of a botnet C&amp;C server was 54 days Only 8. 8% of the IP addresses of bots corresponded to blacklisted IPs on Spamhaus. Not sure whether this means Spamhaus was ineffective, or just that bots have been getting sneakier. It does suggest that “reputation services” won’t save us… Of the activities seen on the botnet channels, “spreading” commands (finding new victims) were 28% of the commands executed. DDOS attacks were 25%; information theft, 9. 8%; self-update activities, 14%. This last figure is interesting; it tells me that bot executables on infected nodes are continually updating themselves to evade detection Botnet spreading methods used exploits for the ASN1, DCOM, LSASS vulns in 50% of the spread commands executed. “Weak password” spreading methods were used, by contrast, only about 6% of the time.  Very few bot commands (low hundreds) were executed for visiting websites (which you’d expect to see for creating fraudulent search result clicks). That shoots my pet theory that bots are ideal “for distributed click fraud”… damn.  Within the botnets, 2,000-4,000 samples of malware were collected every day, with peaks at 7,000 per day 90,000 unique samples collected overall (average 250 new/day) For unique samples seen for the first time (within 1 hour of collection) by an AV engine, the malware detection rate was 70% or higher for only 4 of the 9 AV engines used. The four were Kaspersky (92%), BitDefender (86%), Rising (79%) and Trend Micro (78%). The report did not disclose what the other five engines were, but they all came in at between 50. 2% and 70% detection for new malware.  Even when samples were 30 days old, none of the top four AV engines topped 94% detection. It’s unclear how many engines would need to have been used together in order to catch “everything. ”The report offers plenty of conclusions. My own meta-conclusions are these:  AV is missing a lot of malware Unpatched systems seem to be a key spreading vector… still Bot infections are becoming self-modifying to ensure that they evade detection Reputation services don’t seem to help much Distributed click fraud is not yet a popular money-making techniqueMy last conclusion is perhaps the least intuitive: automated honeypot systems are cool. So cool, in fact, that I’m surprised that the big AV companies aren’t selling them yet as a standard detection tool. But, of course, that would tend to undermine the public position that anti-virus products offer “total protection” (to quote a McAfee product name) or “mega detection” (to quote Panda). On a side note, for my employer Yankee Group I’m currently finishing up a report on the future of the anti-virus industry. Reports like these reinforce my view that security vendors will be forced to strengthen the detection and recovery parts of their product portfolios — and cool the silver-bullet rhetoric about perfect protection. But, that’s thinking like a CISSP (the prevent-detect-recover triad), rather than like a desktop software vendor (software is installed, and problems are solved in one step). "
    }, {
    "id": 40,
    "url": "http://localhost:4000/blog/2007/11/13/Run-Do-Not-Walk-To-Your-Browser-and-Read-Dan-Geers-Analysis/",
    "title": "Run, Do Not Walk, To Your Browser and Read Dan Geer&rsquo;s Analysis",
    "body": "2007/11/13 - Dan’s a friend of mine, and we are both data junkies. Right about the same time I put the capper on a research report on malware trends (coming soon to Yankee Group subscribers), Dan releases this tour de force, a masterful synthesis of a lot of other people’s data. The upshot: things are getting worse. Needless to say, it beats the pulp out of any of the other Internet security trend reports I’ve seen all year. Stupendous. I doff my feathered foofy cap in your general direction, sir Dan. "
    }, {
    "id": 41,
    "url": "http://localhost:4000/blog/2007/10/17/Web-2.0-Means-Security-the-Max-Power-Way/",
    "title": "Web 2.0 Means &ldquo;Security the Max Power Way&rdquo;",
    "body": "2007/10/17 - Last week my Yankee Group research report “The Web 2. 0 Security Train Wreck” went live on the Yankee website, and is available to our customers. Douglas Crockford, a very smart and informed web application expert at Yahoo, who I interviewed for the report, gave it a generally positive review. I sent him a courtesy copy, as is our practice. However, he also states that I got some things wrong. If you read his critique, he faults me for not pointing out that there’s not much more broken in Web 2. 0 that wasn’t already broken. He is right in the sense that the problems are rooted in well-known anti-patterns — notably, ignorance of good security design. That’s true of “1. 0” apps too (and, I point this out). What is different is that the Web 2. 0 architectural style makes it easier and faster to hose yourself than ever before due to the fact that JavaScript is pretty much essential for any significant application. I am reminded of the Simpsons episode where Homer decides to legally change his name to accelerate his career prospects. He settles on the name “Max Power” because it was on his hairdryer. At the dinner table that night, he lectures Bart: “Boy, if there’s one thing you should know, it’s this. There’s the right way, the wrong way, and the Max Power way. ” “Uh Dad, isn’t that the wrong way?” “Yeah son, but FASTER. ” From a security design standpoint, “Web 2. 0” is the wrong way, but faster. "
    }, {
    "id": 42,
    "url": "http://localhost:4000/blog/2007/07/25/Excuses-Not-To-Use-CVSS/",
    "title": "Excuses Not To Use CVSS",
    "body": "2007/07/25 - I have always been a fan of the good work done by the CVSS folks. I have an obvious reason to like CVSS, of course: namely, to cheer on a former co-worker, Mike “Shifty” Schiffman, who was of the first version’s authors. But more than that, I like CVSS because it is a bold attempt to create a scoring system for vulnerabilities that is objective and independent of any single vendor’s spin. As an industry, we need this. I reference, and commend, CVSS in my book Security Metrics. Today, Computerworld reports that CVSS version2 is now out. That’s great news; congratulations to Gavin and the rest of the team. I hope Microsoft and other vendors actually start using it. One thing about that Computerworld story that annoyed me, however, was Robert Beggs’ comment that enterprises shouldn’t use CVSS to “manage by the numbers. ” Specific critiques of CVSS aside, why shouldn’t we do that? Isn’t that the point of measuring things? I guess we should manage by voodoo instead. Honestly, I find comments like this exasperating. On the other hand, you never know what a reporter is going to pick up on and write in a column. I’ve said some damned silly things, as throwaways, that were printed. (My comment to InformationWeek’s Marty Garvey, calling Mozilla’s tabbed browsing feature “the best thing since sliced bread,” is one such stinker that got printed. ) "
    }, {
    "id": 43,
    "url": "http://localhost:4000/blog/2007/07/19/The-Futility-of-Geographic-Security-Metrics/",
    "title": "The Futility of Geographic Security Metrics",
    "body": "2007/07/19 - While I would not call this a trend, I have noticed that lots of security companies like to put together impressive-looking charts, graphs and reports that purport to compare various metrics by country. Here are two recent examples:  Sophos released its list of Dirty Dozen spam-relaying countries. The avoweded goal of the report is to “name and shame” the countries whose servers are apparently the biggest spammers, and by implication, the most sloppily managed and secured.  Symantec’s semi-annual Internet Security Threat Report, an otherwise fairly interesting read most of the time, always devotes about three pages to documenting the “top attacking countries,” a subset of whose citizens have been determined to be involved in a variety of detectable online hijinks. Now, I don’t want to get off on a rant here, but I have three problems with these sorts of country metrics: accuracy, lack of throats to choke, and general pretentiousness of the whole exercise. Accuracy: Nearly every report that calculates country metrics relies on the same technique for determining geography: doing a WHOIS netblock search for the IP address in question, and using the record’s registration address as the source for the country. This is essentially how gambling sites, for example, figure out that Johnny is really from Queens and not from Bermuda, and therefore not able to gamble in virtual offshore casinos. Needless to say, this particular method of inferring the country of origin is not exactly reliable. What if the user or domain is using a foreign ISP? (Example: securitymetrics. org was originally hosted in Ireland. It is now hosted in Atlanta, even though I live in Boston. ) What about multinational corporations who centralize operations in a particular locality, but have distributed operations? And most important, what about persons who use relay machines (such as bots) as launching points for spam or attacks? Lack of throats to choke: In my book, Security Metrics: Replacing Fear, Uncertainty, and Doubt, I note that good security metrics need to have five properties:  Consistently measured,without subjective criteria Cheap to gather, preferably in an automated way Expressed as a cardinal number or percentage,not with qualitative labels like “high,” “medium,”and “low” Expressed using at least one unit of measure, such as “defects,”“hours,”or “dollars” Contextually specific: relevant enough to decision-makers so that they can take action Without question, metrics on which countries are attacking with the most gusto, or spamming the most frequently, aren’t totally horrible because they satisfy the first four properties. But they fail the last test miserably. Exactly who is going to benefit from the knowledge that, say, “the US” (note the scare-quotes) is the most aggressive spammer? Who will take action? Will it be…  The president, George W. Bush? Will he direct the SEC, GAO, FCC and the Treasury to declare a Global War on Spam Relays? Certainly not.  The captains of industry, such as the member companies that comprise the Dow Jones industrial average? Do you think this information would cause the respective CEOs to call their CISOs on the carpet and get them inspect and correct all of their security systems so that the US, as a whole, would rank better in next month’s report? Nope.  Foreign multinationals? Will they suddenly start curtailing their e-mail and web traffic to US companies, for fear of catching cooties? Meh.  Consumers? Do you think Johnny is going to pack up his tent and move to Lower Slobovia because the US is now far too dangerous a place to own a computer in, according to something he reads in The Register? Probably not, unless he wants to evade Bermudan gambling controls. These are admittedly silly examples, but the point I am making is more serious. Namely, it is that no single decision-maker gains anything useful from country-by-country metrics. There is nothing here that a CISO, security director or individual consumer could use to make smarter decisions, allocate their dollars more wisely, or change behaviors for the better. Which brings me to objection three, which is… The pretentiousness of the whole exercise: Although I have been in the security business for a few years now, it seems like I missed a memo somewhere. Exactly where does it say that in order to be taken seriously as a Security Authority, one must produce country-by-country graphs? Did marketeers watch War Games too many times? Do they have unrequited desires to work at NORAD? And where does the fixation from blaming countries come from? Does Carole Theriault wish to petition the UN so that she can be appointed High Commissioner for Internet Security? (She’d probably be good at it, but that’s a different story. ) The more I think about it, the more irritated I get. Creating geographic charts with impressive numbers on them, knowing full well that nobody can use the information on them to make better decisions, is a really nice, neat way to have one’s cake and eat it too. Symantec, Sophos and the like can marshal impressive statistics about particular countries, but they can’t be used by anybody for any purpose. Because nobody can gain any benefit from them, nobody can possibly be offended, either. Thus: country-by-country metrics are a safe way to display apparent expertise without rocking the boat. These reports might make for good PR. But where’s the courage in them? J’accuse!: Here’s a better alternative: actually naming names. Rather than “shaming countries,” why not use cross-sectional analysis to shame corporations, ISPs, and government agencies? These organizations have actual budgets, information security staffs, and public relations problems to worry about. When named publicly as nasty spammers, data leakers or clueless configurers, they will generally take action to fix the problems. In other words, security metrics produced by parties who are willing to stand up and say, “J’accuse!” would be useful to those responsible parties who can actually do something with the information. Here are two example of real courage:  Spamhaus. They have the de rigeur country-by-country report, of course, but they also report by ISP. Now that’s more like it — somebodies we can finger! Support Intelligence. Their Month of 0wned Corporations blog initiative was a brilliant public relations move, and it got them written up in the New York Times. How much would you like to bet that most of these companies have found and eliminated most of the botnets that Support Intelligence documented?I know that this post won’t affect the prevailing sentiments or practices of the most aggressive marketeers in the security industry. We will keep seeing more useless country metrics. But I thought I’d mention it… "
    }, {
    "id": 44,
    "url": "http://localhost:4000/blog/2007/05/22/What-do-Security-Conscious-People-Choose/",
    "title": "What do Security-Conscious People Choose?",
    "body": "2007/05/22 - At security conferences and events, I have noticed that the distribution of operating systems seems to differ somewhat from what I read in the papers. As my last post showed, the Internet Identity Workshop skewed decidedly in the Mac direction. I thought it would be fun to put together a quick poll asking the members of the securitymetrics. org mailing what operating systems they used. I sent out a note asking the membership to respond to two simple questions:  What is the operating system and e-mail client you use at work? What is the operating system and e-mail client you use at home (or for personal activities)?I’ve compiled some preliminary statistics for your reading pleasure. Thanks to the 27 people who responded out of a total membership of about 300. That’s nearly a 10% response rate in less than a day — not bad at all! Objectives and Methodology: The goal of this little survey was to try and figure out if self-selected, security conscious people had a preference for operating systems or e-mail clients that differed markedly from the mainstream. I’ve compiled operating system and e-mail statistics from three related sources:  Responses to my previous e-mail (27 replies) — what is your operating system and e-mail client at work and at home? Analysis of e-mail “X-Mailer” and related headers from the securitymetrics. org mailing list (20 June 2006 to present) Analysis of same from metricon@securitymetrics. org traffic (i. e. , paper submissions) (31 March 2006 to present)In total, I identified 170 people who have contributed to this mailing list or sent submissions to Metricon 1. 0 and 2. 0. Of those, 27 provided OS/email information to me directly; I relied on header analysis for the remaining 143. In total, I was able to identify a “preferred” operating system (either the one specified as the ‘home’ OS in a direct e-mail to me, or the one identified in the header) for 93 people. I identified e-mail programs for 131 people. Operating Systems: For respondents who contacted me directly, and specified their work OS (n=27), Windows was the majority OS. Name#%Windows1556%Linux519%OS X726%For home (n=28), the results are quite different: Name#%Windows829%Linux725%OS X1346%Of the 27 respondents, 14 (55%) reported using a different OS at home compared to work. After taking into account X-Mailer headers, I’ve concluded that for members of this list (“security conscious people”), we can conclude that when they have a choice, our members slightly prefer Macs. Results (n=92): Name#%Windows3841%Linux1516%OS X3942%Amazingly enough, this suggests that Windows is a minority operating system, at least on this list. E-Mail Clients: For respondents who specified their work e-mail client (n=27), Microsoft Outlook was the majority client. Name#%Outlook1452%Thunderbird415%Apple Mail311%Mutt27%Other415%For home (n=28), the results are, once again, quite different — and quite diverse: Name#%Thunderbird725%Apple Mail621%Outlook311%Google Mail311%Pine27%Mutt27%Other518%Of the 28 respondents, nearly 2/3 (17 or 63%) specified a different home e-mail client compared to the one they used at work. After analysis of X-Mailer headers is taken into account (n=131), I conclude that our members prefer webmail overall, and prefer free (and non-Microsoft) native clients. Name#%Google Mail2318%Thunderbird2217%Apple Mail2015%Outlook2015%Lotus Notes108%Other3627%Interesting, no? Statistically relevant — maybe not! Let the debates begin in earnest! "
    }, {
    "id": 45,
    "url": "http://localhost:4000/blog/2007/05/15/Metrics-from-Internet-Identity-Workshop/",
    "title": "Metrics from Internet Identity Workshop",
    "body": "2007/05/15 - This week, I am attending two security shows: the Internet Identity Workshop (IIW) in Mountain View, and the CardTech show in San Francisco. Both of these venues offer contrasting views of the portable identity market, an area I cover professionally for Yankee Group. As many people who know me can personally testify, I like to count things. Here a few statistics that will probably interest only me:  # attendees at Internet Identity Workshop: 150 # attendees from US Department of Defense: 1 # conference sessions on identity: about 40 # conference sessions explicitly devoted to identity theft and fraud: 1 # personal computers observed at general session, 10:10 AM today: 46 % of general session computers that were Macs: 55% (25/46) % of Macs that were MacBook Pros (that is, less than a year old): 90% # OpenID replying parties in November 2006: 550 # OpenID RPs today: 2500 # personal computers observed at Microsoft-sponsored working session on CardSpace: 14 % of Macs at Microsoft session: 42% (6/14)"
    }, {
    "id": 46,
    "url": "http://localhost:4000/blog/2007/05/02/Microsoft-Security-Intelligence-Report-2H06/",
    "title": "Microsoft Security Intelligence Report 2H06",
    "body": "2007/05/02 - This is essentially a forward reference to a comment I made to another blog, but as it is related to the nature of reporting for vulnerabilities and quantitative progress against them, perhaps it is relevant here. The topic is the “Microsoft Security Intelligence Report 2H06” and the comments follow the initial discussion. "
    }, {
    "id": 47,
    "url": "http://localhost:4000/blog/2007/04/15/More-Praise-for-Security-Metrics/",
    "title": "More Praise for &ldquo;Security Metrics&rdquo;",
    "body": "2007/04/15 - The bloggiste at Layer 8 just declared Security Metrics to be “That Good”. I have no idea who shrdlu actually is. But whomever she is, she deserves a hearty thank-you and an offer of a beer should we ever meet in person. Here is a snippet of what she said:  I have found the Metrics Prophet for our times, and his name is Andrew Jaquith.  I stumbled home yesterday from work, sleep-deprived, jittery, and feverish from an oncoming cold.  I tucked myself into bed, hoping to sleep—but I could not sleep until I had read Security Metrics cover to cover. It was That Good.  Now, either that makes me the biggest saddo anorak west of the Pond, or it means Jaquith is an extraordinary writer about what would otherwise be an extremely dull subject. I would of course prefer to think it’s the latter, and I’m sure he would too.  First off, his writing is chock full of playfulness and amusing literacy, from the literary nods (“Call me Analyst. ”) to the rimshots (“… the top and bottom 50% are divided by—wait for it—the median!”).  Secondly, his metrics are for the most part accessible, meaning that as soon as I see them, I think, “Yeah, I could get those!” And a whole lot of them are ones I’d already thought of, but there are a few gems in there that were like little Altoids in my mouth, that made me sit up and go, “Whoa. ” You can see the rest of her review on her website. If you are thinking of buying the book, her comments should give you an idea of what is inside. She has some excellent and perceptive constructive criticisms also, which are all on target. Ms. Shrdlu, thanks very much for the kind words. I especially appreciate that she caught my nod to Herman Melville in the first line of the book (“Call me Analyst. ”). "
    }, {
    "id": 48,
    "url": "http://localhost:4000/blog/2007/04/03/Alex-Hutton-Likes-Security-Metrics/",
    "title": "Alex Hutton Likes &ldquo;Security Metrics&rdquo;",
    "body": "2007/04/03 - Alex Hutton was one of the editorial reviewers for several chapters of Security Metrics, and offered some excellent feedback during the writing stages. Now that the book has shipped, as a way of saying “thank-you” my publisher Addison-Wesley hooked him up with a copy. Alex seems to like the rest of the book, too:  A Book You Should Buy  Finally, I’d like to point you to this. It’s a book every analyst should own, written by a very smart person (Andrew Jaquith), and filled with - mostly - very good material… Metrics are great, but there are so many, many ways to get them wrong… Do go out and obtain a copy for yourself and/or your analysts. You won’t be disappointed. It’s one of those books you’ll actually use. Thanks so much, Alex! When I see you at MetriCon 2. 0 later this year, you’ve got a beer coming to you. "
    }, {
    "id": 49,
    "url": "http://localhost:4000/blog/2007/04/01/Introducing-Security-Metrics-the-Cartoon/",
    "title": "Introducing Security Metrics, the Cartoon",
    "body": "2007/04/01 - Mark Curphey’s cynical vehicle for ripping the security industry gains another blunt instrument: the Hamster Wheel of Pain, featured in Chapter One in Security Metrics: Replacing Fear, Uncertainty and Doubt. Mark was kind enough commission a cartoon based a quick e-mail from me. I think the cartoon shows that I am at least as cynical as he. "
    }, {
    "id": 50,
    "url": "http://localhost:4000/blog/2007/03/30/Security-Metrics-Has-Shipped/",
    "title": "Security Metrics Has Shipped",
    "body": "2007/03/30 - Greetings everybody! I am pleased to announce that my book, Security Metrics: Replacing Fear, Uncertainty and Doubt has shipped from the printers and is on its way to better bookstores near you. You can also pre-order from Amazon. I have a printed copy in my hot little hands, and it looks great! I’d like to thank my agent, my producer and my good friend Walter Matthau. In all seriousness, as a first-time author I am especially gratified to finally see and touch the tangible result. Many people, including at least one ex-girlfriend, thought it would be a cold day in hell before this thing ever saw the light of day. As a serial perfectionist and sometime procrastinator, that was always a real risk. But my intrinsic stubborness — a trait I got from my brassy, scone-making Irish grandmother — guaranteed that sooner or later I’d get it done. (Friendly pressure from my editor helped too. ) In the end, I did what Microsoft did with Vista — I cut features until it shipped. The original table of contents called for about fifteen chapters; the finished book has eight. But the chapters I shipped are deeper, and better in many ways, than what I initially envisioned. I didn’t get a chance to really read the book all the way through until the galley proofs arrived — before that, I was a little “too close” to it. Nothing is ever perfect for me, and after a while I become fatigued, numb and just plain cranky about what I felt were all of its flaws. But most of these are things readers will never see. Despite all that, I am still very happy with the result, and delighted with how well it flows and tells a story. Overall, this is an exciting day. Feel free to buy lots of copies of Security Metrics for yourself, your family and friends. It makes a great gift! "
    }, {
    "id": 51,
    "url": "http://localhost:4000/blog/2007/02/14/Ryan-Joe-Joanna-and-the-Serious-Hole-in-Vistas-UAC/",
    "title": "Ryan, Joe, Joanna, and the &ldquo;Serious Hole&rdquo; in Vista&rsquo;s UAC",
    "body": "2007/02/14 - ZDNet’s Ryan Naraine blogs about Joanna Rutkowska’s blog post on Vista security. Joanna pointed out that Vista’s Mandatory Integrity Control feature has a few implementation flaws and seems to default to prompting for admin credentials whenever setup apps run. EWeek’s Joe Wilcox asked me to comment on the imbroglio which I was happy to do. I also posted a lengthy comment on Joe’s story, which for posterity I reprint here. Reprinted from eWeek posting:  One point of clarification about Joanna’s comment on setup programs needing admin permission. The issue is that Vista doesn’t necessarily know what permissions the files in an application might need. That’s because legacy setup programs are just big executables (EXEs). InstallShield, for example, will take a developer’s application and jam it into a big program. To Vista, the EXE is opaque, a blob. It can’t know that the files the setup program wants to install need to go into Windows System32, for example — which would need elevated privileges to install. Or, the files could be 100% local, and not need extra privileges to install. So, to be safe, Vista takes the position that it will need admin permissions to run.  This behavior is basically Microsoft needing to deal with how older setup applications have always worked since the early days of Windows. (Vista does have a newer format that allows permissions to be explicitly defined ahead of time, but few applications use this… today. ) Other operating systems do things differently, which was the point of my comments to Joe.  Example: OS X has two installation methods: drag-install or via a setup package. The drag-install method is what you see in 75% of the apps out there: you mount the disk image and simply drag the application icon to where you want it. Because the icon is actually a directory, all of its contents come with it. Assuming you don’t drag the application to a sensitive directory, you won’t get prompted. Personally, I love this feature and think it’s incredibly intuitive and natural — why “run a setup program” when you can simply move the app to where you want it?  The second OS X method involves running an actual setup program. In this case, the installer inspects what is called a Bill of Materials (BOM) that specifies exactly what files should be installed, and what privileges they require. The installer uses the BOM to determine whether it needs elevated privilges to install the app. Apple’s BOM method isn’t perfect, but it works quite well for the most part.  In UNIX, the prevailing installer methods are either simply file copies (like when you compile an application and type “make install”) or a package format like Debian’s APT or Red Hat’s RPM, which have “manifests” in them enumerating what files need to be installed. In these cases, the installers either will make a determination that you need (or don’t need) elevated privileges, or will simply fail to install unless you elevate. My point with this lengthy post isn’t to suggest that Linux or Mac are better, although I do believe in this case they’ve had the benefit of learning from the legacy Windows installer experiences. Vista’s next-generation technologies for this are promising, but for now we’ve got a whole boatload of legacy stuff to deal with. Hence Joanna’s objection. "
    }, {
    "id": 52,
    "url": "http://localhost:4000/blog/2007/01/10/And-So-It-Begins-With-Small-Saturated-Spots/",
    "title": "And So It Begins, With Small Saturated Spots",
    "body": "2007/01/10 - My publisher, Addison-Wesley, has recently updated the information on my book, Security Metrics: Replacing Fear, Uncertainty and Doubt on Amazon. Although I am particularly fond of the inside contents, I am also very pleased with the way the cover came out. AW’s very talented Alan Clement put this great picture on the cover: Isn’t it nice? I like it not just because it’s a great picture, but because it ties into the book’s overall theme. Allow me to explain. I am highly visual person. I have always believed that when you want to make a point, particularly to executives, you need to deliver it in a way that drives it home forcefully and clearly. Accompanying numbers with clear, crisp, honest graphics is an important way of doing this. The great Edward Tufte, arguably the most influential information designer of the last 25 years, contends that the strategic application of small, saturated points of color in exhibits can draw attention to key points. His tastes run exactly opposite to the mainstream, as evidenced by many exhibits I see from reputable vendors. Most people’s idea of a “slick exhibit” means three-dimensional bar charts clad in radioactive colors. But instead of drenching entire exhibits with leftover Sherwin Williams inventory, Tufte argues that in many cases exhibits should be relatively austere, save for those few key spots where you want to make your point. That is where you bust out the saturated color. In short, Tufte argues that restraint should rule. As a raging contrarian and visually driven person, I agree wholeheartedly. Which brings me back to my book’s cover. In the book itself, I have exercised fairly tight control over the presentation format of the exhibits, of which there are about 75. But the cover is something else again. My publisher wanted, for obvious reasons, to apply a common style to the cover that aligns it in with its other professional books. And of course I was happy to work with them on this. So when it came time to consider designs, AW described the overall aesthetic, and asked me for some general guidelines about cover art. AW’s professional books generally have a wildlife motif. I suggested that we look for an animal who camouflages itself or blends in with the natural surroundings, but also has some kind of small, dense, saturated marking or plumage. Alan sent me a dozen samples, which included tropical forests with wildly colored birds, an Arctic wolf in a snowstorm with shining yellow eyes, a tree owl with red eyes, a snake with unique markings, and several others. But the frog that you see above was far and away the most appealing. It was Alan’s favorite, too. Colors, security, metrics, outlier analysis, and book design — who knew they were all so related? "
    }, {
    "id": 53,
    "url": "http://localhost:4000/blog/2007/01/03/SSL-is-a-Concrete-Sewer-Pipe/",
    "title": "SSL is a Concrete Sewer Pipe",
    "body": "2007/01/03 - My buddy Gunnar Peterson has recently been raging about the inadequacies of REST security, pointing out that RESTful folks who equate transport-level confidentiality (such as SSL provides) with “security” are only partly right. Gunnar makes some fairly involved references (Neal Stephenson) to make the point. Of course, Gunnar is right. When I speak with people about application security, I try to use a few snappy analogies to drive the point home. And with respect to the difference between transport-level security and message-level security, the analogy I use is to compare SSL to a concrete sewer pipe. You may not be able to break into it, but you sure as hell have no idea what’s flowing through it. "
    }, {
    "id": 54,
    "url": "http://localhost:4000/blog/2007/01/01/Coding-in-Anger/",
    "title": "Coding in Anger",
    "body": "2007/01/01 - Last week’s shutoff of this website’s self-registration system was something I did with deep misgivings. I’ve always been a fan of keeping the Web as open as possible. I cannot stand soul-sucking, personally invasive registration processes like the New York Times website. However, my experience with a particularly persistent Italian vandal was instructive, and it got me thinking about the relationship between accountability and identity. Some background. When you self-register on securitymetrics. org you supply a desired “wiki name”, a full name, a desired login id, and optionally a e-mail address for password resets. We require the identifying information to associate specific activities (page edits, attachment uploads, login/log out events) with particular members. We do not verify the information, and we trust that the user is telling truth. Our Italian vandal decided to abuse our trust in him by attaching pr0n links to the front page. Cute. The software we use here on securitymetrics. org has decent audit logs. It was a simple matter of identifying the offending user account. I know which user put the porn spam on the website, and I know when he did it. I also know when he logged in, what IP address he came from, and what he claimed his “real” name was. But although I’ve got a decent amount of forensic information available to me, what I don’t have any idea of whether the person who did it supplied real information when he registered. And therein lies the paradox. I don’t want to keep personal information about members — but at the same time, I want to have some degree of assurance that people who choose to become members are real people who have serious intent. But there’s no way to get any level of assurance about intent. After alll, as the New Yorker cartoon aptly put it, on the Internet no-one knows if you’re a dog. Or just a jackass. During the holiday break, I did a bit of thinking and exploration about how to address (note I do not say “solve”) issues of identity and accountability, in the context of website registration. Because I am a co-author of the software we use to run this website (JSPWiki), I have a fair amount of freedom in coming up with possible enhancements. One obvious way to address self-registration identity issue is to introduce a vetting system into the registration process. That is, when someone registers, it triggers a little workflow that requires me to do some investigation on the person. I already do this for the mailing list, so it would be a logical extension to do it for the wiki, too. This would solve the identity issue — successfully vetting someone would enable the administrator to have much higher confidence in their identity claims, albeit with some sacrifice There’s just one problem with this — I hate vetting people. It takes time to do, and I am always far, far behind. A second approach is to not do anything special for registration, but moderate page changes. This, too, requires workflow. On the JSPWiki developer mailing lists, we’ve been discussing this option quite a bit, in combination with blacklists and anti-spam heuristics. This would help solve the accountability problem. A third approach would be to accept third-party identities that you have a reasonable level of assurance in. Classic PKI (digital certificates) are a good example of third-party identities that you can inspect and choose to trust or not. But client-side digital certificates have deployment shortcomings. Very few people use them. A promising alternative to client-side certificates is the new breed of digital identity architectures, many of which do not require a huge, monolithic corporate infrastructure to issue. I’m thinking mostly of OpenID and Microsoft’s CardSpace specs. I really like what Kim Cameron has done with CardSpace; it takes a lot of the things that I like about Apple’s Keychain (self-management, portability, simple user metaphors, ease-of-use) and applies it specifically to the issue of identity. CardSpace’s InfoCards (I have always felt they should be called IdentityCards) are kind of like credit cards in your wallet. When you want to express a claim about your identity, you pick a card (any card!) and present it to the person who’s asking. What’s nice about InfoCards is that, in theory, these are things you can create for yourself at a registrar (identity provider) of your choice. InfoCards also have good privacy controls — if you don’t want a relying party (e. g. , securitymetrics. org) to see your e-mail identity attribute, you don’t have to release that information. So, InfoCards have promise. But they use the WS-* XML standards for communication (think: big, hairy, complicated), and they require a client-side supplicant that allows users to navigate their InfoCards and present them when asked. It’s nice to see that there’s a Firefox InfoCard client, but there isn’t one for Safari, and older versions of Windows are still left out in the cold. CardSpace will make its mark in time, but it is still early, methinks. OpenID holds more promise for me. There are loads more implementations available (and several choices for Java libraries), and the mechanism that identity providers use to communicate with relying parties is simple and comprehensible by humans. It doesn’t require special software because it relies on HTTP redirects to work. And best of all, the thing the identity is based on is something “my kind of people” all have: a website URL. Identity, essentially, boils down to an assertion of ownership over a URL. I like this because it’s something I can verify easily. And by visiting your website, I can usually tell whether the person who owns that URL is my kind of people. OpenID is cool. I got far enough into the evaluation process to do some reasonably serious interoperability testing with the SXIP and JanRain libraries. I mocked up a web server and got it to sucessfully accept identities from the Technorati and VeriSign OpenID services. But I hit a few snags. Recall that the point of all of this fooling around is to figure out a way to balance privacy and authenticity. By “privacy”, I mean that I do not want to ask users to disgorge too much personal information to me when they register. And correspondingly, I do not want the custodial obligation of having to store and safeguard any information they give me. The ideal implementation, therefore, would accept an OpenID identity when presented, dyamically collect the attributes we want (really, just the full name and websute URL) and pull them into our in-memory session, and flush them at the end of the session. In other words, the integrity of the attributes presented, combined with transience yields privacy. It’s kind of like the front-desk guard I used to see when I consulted to the Massachussetts Department of Mental Health. He was a rehabilitated patient, but his years of illness and heavy treatment left him with no memory for faces at all. Despite the fact I’d visited DMH on dozens of occasions, every time I signed in he would ask “Have you been here before? Do you know where you are going?” Put another way, integrity of identity + dynamic attribute exchange protocols + enforced amnesia = privacy. By “authenticity” I mean having reasonable assurance that the person on my website is not just who they say they are, but that I can also get some idea about their intentions (or what they might have been). OpenID meets both of these criteria… if I want to know something more about the person registering or posting on my website, I can just go and check ‘em out by visiting their URL. But, in my experiments I found that the attribute-exchange process needs work… I could not get VeriSign’s or Technorati’s identity provider to release to my relying website the attributes I wanted, namely my identity’s full name and e-mail addresses. I determined that this was because neither of these identity providers support what the OpenID people call the “Simple Registration” profile aka SREG. More on this later. Needless to say, I am encouraged by my progress so far. And regardless of the outcome of my investigations into InfoCard and OpenID, my JSPWiki workflow library development continues at a torrid pace. Bottom line: once we have a decent workflow system in place, I’ll open registrations back up. And longer term, we will have more some open identity system choices. "
    }, {
    "id": 55,
    "url": "http://localhost:4000/blog/2006/12/14/Fortifys-Java-Open-Review-Project-a-Nascent-Security-Benchmarking-Effort/",
    "title": "Fortify’s Java Open Review Project&#58; a Nascent Security Benchmarking Effort?",
    "body": "2006/12/14 - Today I stumbled upon Fortify’s Java Open Review Project, whose goal is to count security defects in popular Java projects. I’d like to tip my cap to Brian Chess and the folks at Fortify for this. It’s not quite a proper benchmark, but it is very interesting indeed. I’d like to ask him (or others associated with the project) for his perspective on the project — how it came about, where it’s going, and what the feedback has been like. Also — and Brian, feel free to file this in the “unsolicited advice” drawer — I think Fortify could turn the crank on this a little and get some really interesting insights. For example:  It would be great to have a bivariate plot showing size of code base (KLOC) versus the defect rate Maybe the plot divides itself into a 2x2 grid (“complex, hairy, ugly, big” v. “simple and secure” v “large-scale engineered” v. “small and sloppy”) Would like to see a friendlier format for defects (defects/KLOC is ok, but defects per million might be better) I’d love to see Coverity, Secure Software, Ounce Labs, Klocwork and others run their tools on the same code bases and see what they findBut these are nits. Overall, I am a big fan of public data. Nice work! I nominate Brian to present a prettified version of this work at mini-Metricon at RSA 2007. "
    }, {
    "id": 56,
    "url": "http://localhost:4000/blog/2006/11/18/Metrics-Rothman-and-Gaming-the-System/",
    "title": "Metrics, Rothman and Gaming the System",
    "body": "2006/11/18 - As usual, the purposefully provocative, belligerently blogging Mike Rothman has gone and done it again — aimed his treacly firehose at security metrics, Most recently, he’s waded into the post-fest on the subject, of which Amrit Williams, Rich Mogull, Pete Lindstrom and Alex Hutton have been willing participants. Now, I recognize that Mike’s stock-in-trade is hyperbole. He generally tells you exactly what he thinks, albeit with some slight exaggeration to get people’s blood pumped up and their tongues wagging in reply. He wants spirited debate, and if it takes a little baiting to get it, he’ll do it. Alrighty then. I’ll take that bait. Mike’s been kind enough to reference my ongoing security metrics work over the last few months, and has been egging me on (privately) to convince him exactly what it is about security metrics that I find so compelling. Why, in essence, would I spend time researching a topic that has (for him) so little obvious value? I can’t possibly convert Mike over to my way of thinking in a single blog post. But what I can do is tackle his latest “incite. ” Mike’s most recent line of argument boils down to this: security metrics aren’t too useful because they can be manipulated. His argument:  We need to figure out what is the right security behavior and what metrics reflect that, and minimize the chances are that the system can be gamed. Time to patch and AV updates just don’t feel right. I know that some folks (like Yankee’s Andy Jaquith) are working on this, but suffice it to say - we need to be very careful. Define the wrong metrics and you’ll be paying for a long time to come. Mike frames the problem well… but he also provides strong clues about what the correct answer is. The key phrase here is “behavior. ” I think we would both agree that every metric has inputs that can be manipulated. And every measurement regime produces behavioral side-effects, some of which can be peverse. But how is security different in this respect than any other management concern? First, let’s consider the inputs to metrics and their potential for abuse by way of an example. Nearly every firm with an e-commerce presence watches a metric related to website uptime. It is common practice to tie operations bonuses to uptime. But carrots (bonuses) aren’t the only way this can be done; sticks are often employed too. I know of a prominent Wall St. financial services firm that personally fines senior operations management when their key systems have unscheduled downtime. Now, I would pose this question: do we think that operations managers game the uptime numbers? Sure they do. When the rules for counting downtime periods aren’t clear, it makes it easy to fool around with the numbers that go into the uptime formula. But does that mean that because the potential for abuse exists, we would be better off simply tossing out the metric? Nobody I know would make that argument. That’s because many organizations put checks in place to ensure that venality doesn’t pollute reliability measures. And by “checks” I mean clear rules that govern what can and can’t be counted as downtime, how to measure increments of time, and the scope of covered systems. Note also that even those firms that don’t do these things well still track uptime, because it provides a directionally correct historical record of their e-commerce systems’ reliability. If we accept that uptime statistics can be gamed somewhat, but we still use them anyway, why would security be any different? In other words: what we should shoot for are metrics whose utility exceed the harm caused by potential manipulation. Good managers put in scope, counting and procedural rules so that the ratio of (utility / potential harm caused by abuse) exceeds 1. 0. Now, on to the second part of Mike’s objections: the observation that the metrics we select might cause organizations to exhibit perverse behaviors. The implication is that organizations aren’t clever or wise enough to choose metrics that get us the behaviors we want. This is similar to the argument made by NEA members that we shouldn’t use standardized tests to measure pupils’ achievements because teachers will simply “teach to the test. ” Of course they will, and more to the point, they should. Nearly everyone across the political spectrum in Massachusetts I’ve spoken to (I’ll allow that there’s a selection bias here) seems to agree that despite its behavior-influencing side-effects, standardized testing and measurement is a good thing. Furthermore, most reasonable people agree that if we are testing our kids for the wrong things, we can also agree to change the tests. Again, why should security be any different? Mike feels that because of the side-effects, the the only metric that we ought to be measuring is one that measures whether a firm has been compromised. I disagree, because that single metric is entirely backward-facing, and does not measure whether an organization has been successful in getting its staff to do the right things. Just what are those “right things”? To my eyes, there are four perspectives that matter: financial, customer, internal process and learning/growth. For the sake of avoiding a rathole in this particular post I’m going to focus on the latter two. For internal processes, which is where the bulk of security operations fall into, the behaviors we want are those that allow organizations to let trusted parties in, keep unwanted persons out, and reduce the chances that bad things will happen. Implied objectives are these:  Protecting information asssets Ensuring the physical safety and security of people and assets Decreasing the damage incurred from security incidents Maximizing co-operation between the security team and business units Identifying security vulnerabilities Quickly granting or revoking access to systems and users Maximizing users’ access to appropriate systems Minimizing excess privileges granted to users and systems Fixing the highest-risk security vulnerabilities Identifying the highest-risk assets Maintaining a stable “baseline” for information assets Verifying the effectiveness of security controls Increasing the ability to respond to technological change Maximizing the reach of security controlsLikewise, for learning and growth, we want to spread responsibility for security, equip employees with the right security knowledge and skills, and promote adaptability in the face of changing threats. These themes imply the following typical objectives:  Delegating responsibility for authoring user activities to business units Increasing collaboration between IT security and business units Ensuring effective levels of security certifications for security staff Promoting security awareness throughout the organization Integrating secure behaviors into employee’s everyday activities Ensuring that security features are easily understood and adopted Heightening awareness of emerging security threats Exploring discretionary security frontiers Giving employees the skills needed to properly handle security incidentsI’ve purposefully listed the objectives for both perspectives (internal process and learning/growth) as objectives, not metrics, because they illustrate the behaviors that organizations should be encouraging. Every one of them can be readily mapped to process metrics — key indicators — that show whether an organization is achieving those objectives. Avoiding Joel Spoelsky’s “function point Olympics” (gaming the system) depends on choosing metrics that bias an organization towards the behaviors they want, and away from the ones they don’t. I go into this topic (the Balanced Security Scorecard) in much more depth in my forthcoming book: Security Metrics: Replacing Fear, Uncertainty and Doubt, coming from Addison-Wesley in early 2007. Nobody said this stuff would be easy. If it was, we’d have done it already. Frankly, I get tired of hearing that security metrics are too hard. It’s a lot like going to dinner with a fussy friend who’s been eating the same things since childhood. Instead of venturing forth and sampling from more adventurous menus, we slide back into the comfort food of “defense in depth”, and “security is a process not a product. ” Those are the meat and potatoes of the security world, and as with everything, too much of it is bad for you. I will post on this topic again soon. "
    }, {
    "id": 57,
    "url": "http://localhost:4000/blog/2006/10/15/Good-Metrics/",
    "title": "Good Metrics",
    "body": "2006/10/15 - Note from Andrew Jaquith: this essay is adapted from Chapter 2: Defining Security Metrics of my forthcoming book, Security Metrics: Replacing Fear, Uncertainty and Doubt from Addison-Wesley and Symantec Press, expected in early 2007. Small portions of this appeared in The Future Belongs to the Quants, an IEEE article co-authored by me, Dan Geer and Kevin Soo Hoo. Information security is one of the few management disciplines that has yet to submit itself to serious analytic scrutiny. In security, business leaders ask:  How secure am I? Am I better off than I was this time last year? How do I compare with my peers? Am I spending the right amount of money? What are my risk transfer options?Were we talking about some other field, we could look to prior art and industry-specific knowledge — for example, derivatives pricing in vertical industries like finance, health and safety in pharmaceutical manufacture, and reliability in power distribution. Likewise, most enterprises’ horizontal functions — human resources, finance, manufacturing, supply chain, call center, e-commerce and operations — measure their performance by tracking a handful of key performance indicators. These indicators include statistics such as call volumes per associate, inventory turns, customer conversion percentages, manufacturing defect rates and employee turnover. Discipline or Vertical MarketKey MetricsFreightFreight cost per mile  Load factorWarehousingCost per square foot   Inventory turnsE-CommerceWebsite conversion rateCable and satelliteSubscription cost-to-acquireThese indicators all share two characteristics. First, they are simple to explain and straightforward to calculate. Their transparency facilitates adoption by management. The second characteristic these indicators share is that they readily lend themselves to benchmarking. On occasion, enterprises will share them as part of a management consulting survey, and will attempt to compare their own key indicators against those of other companies they know. In so doing, they gain insights about their own performance relative to peers and other industries. A quick glance at the Harvard Business Review or McKinsey Quarterly confirms that benchmarking in enterprises continues to be a healthy, vibrant, established pillar of modern management. Information security has no equivalent of McKinsey Quarterly, nor of the time-honored tradition of benchmarking organizational performance. Analytical rigor receives little attention, while nebulous, non-quantitative mantras rule: “defense in depth”, “security is a process” and “there is no security by obscurity” to name a few. What numbers do exist, such as those provided in vulnerability and threat reports from Symantec, Webroot, Qualys and others, provide macro-level detail about the prevalence of malware but little else that enterprises can use to assess their effectiveness comparatively against others.  Numbers provided by anti-malware, vulnerability management systems, and SIM/SEM systems certainly add value — but to date, no entity has yet attempted to aggregate and compare these data across enterprises. So what makes a good metric, and what should we measure? Let’s address the first part of that question in this post — we will address the second in the subsequent one. “Metric” defined: I was curious to see if I could find a consensus definition of what a “metric” is.  According to Oxford’s American Dictionary, a metric is “a system or standard of measurement. ” In mathematics and physics, it is “a binary function of a topological space that gives, for any two points of the space, a value equal to the distance between them, or to a value treated as analogous to distance for the purpose of analysis. ” Specific to IT metrics, Maizlitsh and Handler further discriminate between metrics used for quantifying value versus those used to measure performance:  There are two fundamental types of metrics that must be considered before commencing with IT portfolio management: value delivery and process improvement. Value delivery consists of cost reduction, increase in revenue, increase in productivity, reduction of cycle time, and reduction in downside risk. Process improvement refers to improvements in the IT portfolio management process. While the metrics are similar and in many ways interrelated, process metrics focus on… effectiveness. Is the process improving? Is the process providing perceived value? Is the process expanding in scope? More and more, leaders are looking into the metrics microscope to eliminate non-value-added activity and focus on value-added activity.  – B. Maizlitsh and R. Handler, “IT Portfolio Management: Step By Step”, p. 53. These two definitions certainly help, but like most definitions it grants us a rather wide scope for discussion. Just about anything that quantifies a problem space and results in a value could be considered a metric. Perhaps we ought to re-focus the discussion on the goals of what a metric should help an organization do. The primary goal of metrics is to quantify data, thus yielding insight. Metrics do this by:  Helping an analyst diagnose a particular subject area, or understand its performance Quantifying particular characteristics of the chosen subject area Facilitating “before-and-after,” “what-if” and “why/why not” inquiries Focusing discussion about the metrics themselves on causes, means and outcomes rather than on methodologies used to derive themAs an analyst, I’m keenly interested in making sure that persons examining a “metric” for the first time should see it for what it is — a standard of measurement — rather than as something confusing that prompts a dissection of the measurer’s methods. Metrics suffer when readers perceive them to be vague. For example, I have seen a widely publicized paper that proposes benchmark security effectiveness, but in its key graphical exhibit, the author’s metric is described only as a “benchmark” with a scale from 1 to 5; it does not contain a unit of measure or further explanation. The authors undoubtedly intended to spark discussion around the causes and drivers for the metric — but the exhibit instead makes readers scratch their heads about what the metric is and how it was defined. To keep organizations from trapping themselves in tar-pits of hand-wavery and vagueness, metrics should be clear and unambiguous. Specifically, good metrics should be consistently measured, cheap to gather, be expressed as a number or percentage, and expressed using at least one unit of measure. A “good metric” should also ideally be contextually specific. Consistently measured: Metrics confer credibility when they can be measured in a consistent way. Different people should be able to apply the method to the same data set and come up with equivalent answers. “Metrics” that depend on the subjective judgments of those ever-so-reliable creatures, humans, aren’t metrics at all. They’re ratings. The litmus test is simple: if you asked two different persons the same measurement question, would they produce the same answer? Metrics will either be computed by hand or by machine. In the former case, one can ensure consistency by documenting the measurement process in a transparent and clear way. When people understand how and why to do something, they tend to do it in a more consistent fashion. Keeping measurement questions short and factual (yes/no oriented) helps, too. Even better than manual data sources, however, are automated ones. One programmed, machines will faithfully execute their instructions as provided by their programmers. They will execute their tasks the same way each time, without mistakes. Cheap to gather: Every metric takes time to compute. All metrics start their lives as raw source data, then— through the magic of computation — become something more insightful. That means that somebody or something needs to obtain the data from a source, massage and transform the data as needed, and compute and format the results. For some metrics, these steps collapse into a single, fast process; a simple SQL statement or API method call delivers the goods. But other metrics require screen-scraping, phone calls, and spreadsheet hackery. Inefficient methods of gathering data cost organizations valuable time that they could have put to better use on analysis. I firmly believe that metrics ought to be computed often. Metrics with short sampling intervals help companies analyze their security effectiveness on a day-to-day and week-to-week basis rather than through a yearly rear-view mirror. It stands to reason that if a metric needs to be frequently computed, the source data for the metric should be cheap to gather in terms of time or money. Before-and-after comparisons aren’t something organizations should be forced to do once a year because of inefficient data gathering. For a given metric, ask yourself: could you compute it once a week? How about every day? If not, you might want to re-consider the metric — or consider methods of speeding up the measurement process. As with the point about consistency, the criterion that good metrics ought to be cheap to gather favors automation. Expressed as a number or percentage: Good metrics should be expressed as a number or percentage. By “expressed as a number,” I mean a cardinal number — something that counts how many of something there are — rather than an ordinal number that denotes which position that something is in. For example, “number of application security defects” evaluates to a cardinal number that can be counted. By contrast, high-medium-low ratings that evaluate to 1, 2 and 3 are ordinal numbers that grade relative performance scores but don’t count anything. Metrics that aren’t expressed as numbers don’t qualify as good metrics. “Traffic lights” (red-yellow-green) are not metrics at all. They contain neither a unit of measure nor a numerical scale. Expressed using at least one unit of measure: Good metrics should evaluate to a number. They should also contain at least one associated unit of measure that characterizes the things being counted. For example, the metric “number of application security defects” expresses one unit of measure, namely defects. By using a unit of measure, the analyst knows how to consistently express results of a measurement process that looks for defects. My definition of a good metric holds that it’s often better to use more than a single unit of measure. The single unit of measure for “number of application security defects” metric makes it hard to compare dissimilar applications on an apples-to-apples basis. But if one unit of measure is good, two is better. For example, a better metric might be “number of application security defects per 1000 lines of code,” which provides two units of measure. By incorporating a second dimension (dividing by KLOC), we have constructed a metric that can be used for benchmarking. Contextually specific: Good metrics mean something to the persons looking at them: they shed light on an underperforming part of the infrastructure under their control, chronicle continuous improvement or demonstrate the value their people and processes bring to the organization. Although specificity is not required for all good metrics, it helps to keep each of them scoped in such a way that a reader could receive enough insight to make decisions based on the results. “Contextually specific” is a shorthand way of saying that a good metric ought to pass the “smell test. ” You don’t want managers wrinkling their noses and asking belligerent questions like “and this helps me exactly… how?” For example, defining an “average number of attacks” metric for an entire organization doesn’t help anybody do their jobs better — unless the indirect goal is an increased security budget. But scoping the same metric down to the level of a particular business unit’s e-commerce servers can help much more, because they can make specific decisions about security provisioning and staffing for these servers based on the data. Wrap-up: Well, that’s it for this post. Next time, we will consider what makes a bad metric. I reserve special venom for ALE, so you won’t want to miss it. "
    }, {
    "id": 58,
    "url": "http://localhost:4000/blog/2006/05/02/SANS-Schadenfreude-and-the-Mac/",
    "title": "SANS, Schadenfreude and the Mac",
    "body": "2006/05/02 - I’ve got to wonder about the what planet the SANS people live on these days. Apparently, in an effort to make their semi-annual Top 10 list of vulnerabilities appear “fair and balanced,” they’ve decided to whip up panicky sentiments towards Mac OS X. I won’t offer either of the usual obligatory and contrary platitudes on this subject (“Dood! Macs are invulnerable” or “of course, no system can ever be 100% secure”), because you know them already. Other folks, like Scott Bradner have made the latter argument well. But I will say that I think this stuff is a tempest in a teapot, designed to get some press for SANS. I got a call from a reporter seeking comment on the SANS story. Here is what I told her:  SANS findings are broadly in agreement with our own observations. They’ve done a good job describing some of the key trends on the threat landscape.  That said, much of what they say is scaremongering. For example, they allege that cyber crime is “extremely lucrative” and name a figure of $10 billion. How did they come up with that number? I think they made it up, since they don’t substantiate it anywhere or cite sources.  I also think they’re jumping on the Mac-bashing bandwagon. It’s odd that they lead their report with a hysteria-inducing lead that gravely intones the “rapid growth” in Mac vulnerabilities… and then name exactly three vulnerabilities, one of which is zero-day. Somehow this enables them to claim that the Mac’s “bullet proof” reputation is in “tatters”. Oh really?  Meanwhile, two bullets down they rather casually mention that Internet Explorer has seen eleven new vulnerabilities, plus another SIX affecting Windows itself. So, if three vulnerabilities means the Mac’s “reputation” is in “tatters”, does that means Windows’ has been ground into a fine white powdery substance? Schadenfreude comes to us via German, and is defined approximately as “taking delight in another’s suffering. ” Ed Skodis’ rather snotty comment to ComputerWorld should tell you where his head is at: “Users often feel invincible when they have their shiny silver-colored Apple and they are surfing the Web with it. ” Which users might that be, Ed? Want to source one for me? This is a standard journalistic technique: when you can’t find someone to substantiate your own assertions, trot out your good friend Many People and his boon companion Experts Say. Ed’s characterization of Mac user attitudes are totally at odds with my personal experience. Nobody I know who uses a Mac feels “invincible,” and that includes my mom, my brother, my sister, my good friend Dave G. , Dildog, Dan Geer, Kevin Soo Hoo, my ex-Yankee colleague Phebe Waterfield and many others. Symantec went through a similar Mac-bashing period a while back. About six months ago, in their otherwise excellent Internet Security Threat Report they accused Mac users — as a single, undifferentiated species — of living in a “false paradise”. But they seem to have realized that painting with such a broad brush wasn’t doing much to enhance their credibility. In their latest report, there is not a single word about Macs. As in, the word “Mac” does not appear even once, nor does the phrase “OS X. ” The report is a hundred-plus page document. Security is about common sense, not just what makes for the best headline copy. SANS’ credibility with me has just dropped significantly. Facts are good. Overly dramatic headlines, scare-tactic language and slippery quotes are not. "
    }, {
    "id": 59,
    "url": "http://localhost:4000/blog/2006/04/24/Open-Letter-to-SC-Magazine/",
    "title": "Open Letter to SC Magazine",
    "body": "2006/04/24 - Sent from my YG account 25 April 2006:  Dear Sir,  Please stop printing sensationalist headlines.  The headline of your article on 21 April 2006, “Report: Non-Windows attacks on the rise” gives the misleading impression that non-Windows platforms are increasingly being “attacked”, and cites a recent Kaspersky report by Konstantin Sapronov as evidence. In fact, the report says nothing of the kind.  The report states that the number of malware samples Kaspersky Lab collected increased in 2005 to 863, from 422 in 2004. That is indeed a substantial increase, but the report is very careful not to characterize these as “attacks” — defined per Oxford as “aggressive and violent actions against persons or places. ” Obtaining malware programs is not the same as reporting an attack. Indeed, Sapronov cites only one bona fide infection, noting that with respect to actual Linux incidents, “it was a quiet year…as for other Unix platforms, the situation is even quieter. ” Other than that, no other evidence of specific actions against any particular person or place was cited in Kaspersky’s report, and I am sure they did not intend to imply any.  In addition, it would be entirely unfair to mention Kaspersky’s malware report on non-Windows platforms without mentioning their accompanying Windows report also. That report states that the number of Windows-related malware programs detected by Kaspersky Lab increased to 53,950 in 2005 from 2004’s 31,726. That is a number 60 times larger than the Linux figure, and is certainly more worrying than a few hundred new malware samples, very few of which have been claimed to cause any harm to anybody.  Best regards,  Andrew "
    }, {
    "id": 60,
    "url": "http://localhost:4000/blog/2006/03/03/Good-Patch-Management-Metrics/",
    "title": "Good Patch Management Metrics",
    "body": "2006/03/03 - Earlier today I stumbled across the NIST patch management pub; it was released in November 2005. There’s lots of goodness in this document. What I like best are the recommended program metrics, which I reprint here, lightly edited: Metric NameUnitsMaturity LevelVulnerability ratioVulnerabilities/Host3Unapplied patch ratioPatches/Host3Network services ratioNetwork Services/Host3Response time for vulnerability and patch identification (triage processes)Time4Patch response time (critical)*Time4Patch response time (noncritical)*Time4Emergency configuration response time (for zero-days)Time4Cost of patch vulnerability groupMoney (labor)5Cost of system administration supportMoney (labor)5Cost of softwareMoney5Cost of program failuresMoney5*In my own work, I’ve referred to these metrics as “patch latency”. Now, there are certainly folks who will state that patching doesn’t matter much. They may be right. But if you view patching as a discipline you need to get right regardless, I would suggest that these are pretty good metrics. Update 1: Interestingly, our friends at Microsoft, who (ahem) have some experience patching systems in large, complex environments, add these metrics to the mix:  Two of the key business metrics that we use to measure our success is the number of servers that are patched outside of their maintenance windows. For example, if we apply a patch outside of a server’s pre-defined maintenance window, that’s going to be a ding against our success. Another metric is going to be the number of servers patched by deadline. Update 2: We’ve had quite a bit of chatter about this on the securitymetrics. org mailing list. The inestimable Fred Cohen stated that some of these metrics are too hard to gather, or may not tell an enterprise much when taken in aggregate. He’s right, in part. But I would suggest that the reason, say, the “cost of program failures” is rated a 5 on the process maturity scale is precisely because it is difficult to measure. Indeed, the numerical process maturity rating could be taken as a proxy for measurement difficulty. As for his question about the relative value of average measurements taken in aggregate (across multiple contexts, business units and threat environments) — that is why cross-sectional analyses exist. Good stuff. "
    }, {
    "id": 61,
    "url": "http://localhost:4000/blog/2006/02/12/Charging-for-Guaranteed-Spam-Better-Than-It-Sounds/",
    "title": "Charging for Guaranteed Spam&#58; Better Than It Sounds?",
    "body": "2006/02/12 - Much ink has been spilled over the recent AOL and Yahoo announcements that they will charge marketers five cents per e-mail to guarantee delivery of their mail, thus bypassing their spam filters. Lots of people been rendered spitting mad by the plan. Three things seem obvious to me about how and why these plans came about:  The ahem, marketing companies are clearly frustrated by the fact that their mail is getting blocked more and more often. If you believe the numbers from MessageLabs and others, spam is now 80-85% of all Internet e-mail.  The existing spam filters of Yahoo and AOL are clearly annoying some of their most-phished customers (read: banks) by blocking their legitimate communications AOL and Yahoo clearly think that can make a buck on thisI can’t stand spam in any form. It’s why I switched my private e-mail from these guys to another provider. I used to get so many e-mails containing viruses, worms, trojan horses and other nasties that I almost longed for the simple “would you like some V1@grA?” type. Clearly, the deluge of spam is largely being fueled by the botnet boom, and the malware-laden variety is crushing the stuff that’s merely solicitous. And there’s the rub. There are genuine businesses out there, like banks, who want to communicate with their customers. And there are other sorts of businesses who simply want to bombard us with come-ons for lots of stuff we don’t need and didn’t ask for. Yahoo and AOL clearly don’t think it’s worthwhile to try to distinguish between the two, so it’s easiest to simply say: make ‘em all pay. That’s just fine with me. The larger banks can clearly afford to pay, while the Spanish-fly-by-night yahoos (oops) will only do so if they think the risk/return is worth it. As for the latter type, I’m happy to let AOL and Yahoo drain their marketing budgets dry. But of course, as a consumer I still don’t want to get this stuff. Therefore, if AOL and Yahoo are going to make an unholy pact with Viagra-pedding lümpenmarketers so that they can cram their spam in our pliant craws, then it seems to me that the consumers whose craws are being crammed ought to have some right of redress. Specifically:  Marketers who pay Yahoo and AOL to guarantee delivery of their spam must also offer a verifiable opt-out provision. And here’s the good news: it seems that the proposed system does exactly that. The system AOL and Yahoo will be using claims to offer a “certified unsubscribe” feature, as well as a spammer-authentication system. This, I think, is the missing headline from this whole story. Even if there’s more spam (ugh), at least you know whose throat you can choke. You can tell them to go away and feel pretty confident that they will. And you can feel all warm inside knowing that they are slowly and assuredly going broke. That said, there are going to be plenty of ways to game the system. So I guess I’m glad I’m not a Yahoo or AOL subscriber. "
    }, {
    "id": 62,
    "url": "http://localhost:4000/blog/2006/02/11/Blended-Threats-Hemlock-Smoothies/",
    "title": "Blended Threats == Hemlock Smoothies",
    "body": "2006/02/11 - An open letter to all anti-virus software makers:  February 2, 2006  Dear Antivirus Industry,  Why are you so addicted to the term “blended threat”? It seems to mean something special to you… but it means nothing to anybody else. Certainly not to Grandma or to security professionals who don’t work for anti-virus companies.  To the lay person, a “blended threat” might be what happens when someone slips arsenic or hemlock into their Starbucks frappucino. That’s what you meant, right?  Oh, silly me. You meant “a complex program that targets multiple weaknesses in computer networksand uses multiple distribution methods to spread” (Trend Micro’s definition). But doesn’t that describe the behavior of every sort of malware that’s seen today?  Grandma doesn’t get infected by “blended threats” – she gets infected by:    Adware that spies on her and makes her computer sluggish and unusable  Viruses and worms that wreck her hard drive  Keyloggers and trojan horses that steal passwords and credit card numbers and send them to nasty mean people in Lower Slobovia  We don’t get it. Characterizing malware as using more than one vector of attack may be technically correct but it isn’t the point – it’s the consequences that matter.  The term “blended threat” might have been useful toyour marketing efforts in 1998, but it seems a bit quaint in 2006 – rather like describing today’s automobiles as “horseless carriages. ”Please stop.  Love,  Everyone Else "
    }, {
    "id": 63,
    "url": "http://localhost:4000/blog/2005/12/07/The-Vulnerability-Supply-Chain/",
    "title": "The Vulnerability Supply Chain",
    "body": "2005/12/07 - Yankee Group research may not be as well-subscribed as say, Gartner’s, but I like to think that it compares favorably with it. Earlier this year I wrote a research note titled Fear and Loathing in Las Vegas: the Hackers Turn Pro about the increasing number of vulnerabilities found in security products. The paper documents the flaws found over the last 18 months in a variety of security products, and give prescriptive guidance on what security product vendors and enterprise customers must do. In the ‘note, I made it clear that the mere act of finding security vulnerabilities implies neither malicious intent on the part of researchers, nor of the inevitability of attacks. That said, it is equally clear that there is a relationship between vulnerabilities discovered upstream by the research community and the mass-attacks that occur later downstream. Put simply, what we are witnessing is the formation of a fully developed vulnerability supply chain. Raw materials (theoretical breaks) become intermediate products (proof of concept code) and are then assembled into finished goods (mass exploits). Supply Chain StageActorProductConstraintsRaw materialsVulnerability researcherPublished vulnerabilityTime to reverse engineer, technical skillSubcomponent assemblyVulnerability researcher,  proof of concept  websitePublic posting of POCVendor pressureFinished goodsScript assemblerScripted exploitTime to write scriptsDistributionOrganized crimeMass exploitsTime to add to botnet payloadsIt is often said by old hands in the security game that there is no “security by obscurity”; that in time, even the best-hidden protections will inevitably yield to the scrutiny of the curious and the determined. While that’s true, what proponents tend to miss is that little phrase in time. Time matters, because the time required to re-research and reverse-engineer someone else’s public vulnerability requires a non-zero amount of time. That’s time an organization can use profitably, to patch affected systems or implement alternative countermeasures. Let’s look at a worked example, and see what conclusions we can draw about the “obscurity” argument. Below is the lineage and pedigree of a mass-exploit security vulnerability, namely the Veritas Backup Exec remote agent overflow (CAN-2005-0773). DateStageActorEventUnknownRaw materialsAnonymous researcherVulnerability researcher discloses security flaw to aggregator iDefenseJan 11, 2005Subcomponent assemblyThor DoomenScript assembler releases proof-of-concept code&gt; on low-traffic website&lt;/td&gt;&lt;/tr&gt;Mar 16, 2005-iDefense, researchersAssembler and aggregator contact vendorMar 30, 2005-iDefense, VeritasVendor responds and begins fixing productJun 21, 2005Advanced ship noticeFrSIRTFrench exploit code publisher FrSIRT publishes POC codeJun 22, 2005-iDefenseSecurity aggregator publishes advisory as part of coordinated response with vendorJun 24, 2005Finished goodsMetasploitExploit plugin for the Metaslploit automated security assessment tool released as part of major release 2. 3Jun 27, 2005DistributionSANS ISCHandler's diary documents widespread reports of scans on port 1000Jun 29, 2005-US CERTUS CERT releases security alert advisory*Jul 2, 2005-BleedingSnortIDS portal BleedingSnort commits signature] for detecting exploit via Snort IDS&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;(*) yes, I've included this one for ironyThere are several interesting observations about this chain of events. It appears that the author of a proof-of-concept exploit published his code publicly well in advance (5 months) of the broader vulnerability announcement in June. But it's the incorporation of the code into the Metasploit attack_\\h\\h\\h\\h\\h\\h_ security assessment tool, two days later, that really kicked things into gear. According to the SANS Internet Storm Center, less than 48 hours later after the release of Metasploit 2. 4, the number of scans on port 10000 spiked upwards by four orders of magnitude, to over 250,000 daily.  _Source: SANS/Internet Storm Center_But that's not all. The number of unique IP addresses that were the _sources_ of the scans increased by several orders of magnitude also. The number of source machines were 88 hosts on 6/24, 836 on 6/25, and 5,636 on 6/26. On average, these machines scanned 81, 125 and 45 hosts. These are not the sort of simple scans that an administrator might do on his or her servers to verify a machine's susceptibility to an attack. These are exploits. By way of comparison, the  normal  ratio of sources to targets earlier in the year was almost exactly 1:1. Yes, you might be asking, but what about the proof-of-concept code circulated on Jan 11? Curiously, the number of target machines averaged about 49 before 1/11, then increased sharply to 254 on the day of posting. That number spiked to 7,882 on 1/14, then to 40,393 on 1/16 before settling back down more-or-less in the high dozens. That may sound like a bad thing, and it is, but the ratio of sources to targets stayed fairly low (1:1 or thereabouts) except on 1/16 (1000:1). The _average_ ratio of sources to targets for the entire year up through 6/23 (the day before the Metasploit release) was 22:1, and the median was about 1:1 . (I haven't bothered to graph the earlier data, because the data isn't too interesting. . . the chart would appear as a big flat line, with a few irregular spikes on it. )What do I conclude from all this? Not much, since this is just a single isolated example. But if somebody put a gun to my head and asked me to generalize from an _n_ size of 1, I'd say four things:* First, the data in this example puts the shaft to arguments made by [these](http://www. securiteam. com/) [people](http://www. coresecurity. com/home/home. php) and others, who passionately defend the right to post proof-of-concept code on their websites. POC code is clearly not being used primarily for diagnostic purposes or security assurance -- it's for attacking others. * Second, obscurity works -- sort of. The posting to milw0rm. com correlates well with an increase in scanning traffic, but the Metasploit release was followed by an order of magnitude more scans. In short, how and where POC code is posted affects attack trends. In particular, it appears that the vulnerability publication event itself might be a milestone event, and that POC code published in associated with it amplifies the effect. It begs an obvious question, which I'll leave as an exercise to the reader -- or [Pete Lindstrom](http://spiresecurity. typepad. com/spire_security_viewpoint/), if you prefer. * Third, scripted attack tools play a catalytic role. It wasn't until the POC was incorporated into a sort-of mainstream assessment tool that SANS and others observed significant numbers of scans on their sensors. If you make the assumption that self-assessing admins generate only 1:1 traffic, then you'd have to conclude that attack traffic comprised 95-99% of the scans done after POC release. Thus, automation is a powerful risk multiplier: in this case, 20-100x.  * Fourth, the vulnerability supply chain's later stages (finished goods and distribution) operate with tremendous efficiency. It's not too dissimilar to the way Dell makes PCs; raw materials supply lines from Asia Pacific can stretch back for months, but final assembly in Round Rock takes just a few days. The last thing I'd point out is that back-of-the envelope calculations are important, but they aren't the whole story. We don't know, for example, how long the exploit code was circulating prior to posting on milw0rm. We also don't know from the data whether the criminal underground incorporated the Metasploit exploit into something wormable, or whether the scans came just from a stock Metasploit package. There's a human backstory here that I'm not privy to. Still, the data can tell us a lot. ----_Coda: my esteemed colleague Chris Wysopal reminded me of Bill Arbaugh's excellent paper [Windows of Vulnerability](http://www. cs. umd. edu/~waa/pubs/Windows_of_Vulnerability. pdf), published several years ago. That work is one of the best information security papers I have ever read, period, and it can rightly be said that this post is largely inspired by his previous work. All empirically-driven security researchers owe him, and Mssrs. Fithen and McHugh, a great deal for  showing how it's done.  _ Thanks, Chris, for pointing this out. "
    }, {
    "id": 64,
    "url": "http://localhost:4000/blog/2005/11/29/The-Natives-are-Restless/",
    "title": "The Natives are Restless",
    "body": "2005/11/29 - Many readers know that my day job is as a security technology analyst for Yankee Group. Well, it’s about that time of year where we start to wind down our research calendar. One of the things we’re getting out the door is the 2005 Yankee Group Security Leaders and Laggards Survey, in which we ask a statistically relevant number of enterprises (500+) about their spending habits, preferred security suppliers, and future plans. This year, we added a special set of questions designed to force choices between several competing alternatives. For example, we asked whether enterprises preferred to work with resellers, or direct with security vendors. Probably the most interesting (read: mischievous) question we asked was this one: Tell us what outcome is desirable: 1) backporting future Vista security features to older Windows versions (XP, 2000) versus.  2) Enterprise migration to Vista Now, you might think that companies would be jazzed up about the security improvements Microsoft has promised for Vista, and that upgrading would be something companies would prefer to do. Our data shows exactly the opposite:  5% of customers found upgrading “extremely desirable”: 12% said it was “desirable” 30% were “neutral” An even 26% said that backporting was either “desirable”; ditto with “extremely desirable”But wait, there’s more! When we look only at what we call security “leaders” – those companies that spend the highest percentage of their IT budgets on security – the differences are even more pronounced. Fully 40% of Leaders felt that backporting was extremely desirable; after adding in the “desirable” percentage, the total favoring backporting is a sky-high 65%. That is a stunning number. Now consider the additional fact that Vista won’t run acceptably on hardware older than about a year. Consider also Joe Wilcox’ observation that Microsoft has missed as hardware upgrade cycle. When you put all of those things together, it tells me that customers don’t want a forklift upgrade to a more secure operating system forced on them. Does this just reflect common sense? Probably. I just didn’t expect the numbers to work out quite this unequivocally. "
    }, {
    "id": 65,
    "url": "http://localhost:4000/blog/2005/11/14/The-Devils-Information-Security-Dictionary/",
    "title": "The Devil&rsquo;s Information Security Dictionary",
    "body": "2005/11/14 - Just saw the very funny Devil’s InfoSec Dictionary on the CSO site. Of course, I had to add a few definitions of my own:  Blended threat: a hemlock smoothie Process, Security Is A: a throw-away line that explains why security measurement is impossible Risk management: a repeated process around the Hamster Wheel of Pain that vendors use to enumerate vulnerabilities you didn’t know you had, followed by serial remediation of same. See “remediation” Remediation: furious arm-flapping and showy activity designed to convince bosses that something is actually being done about vulnerabilities identified by third parties Spear phishing: a sport undertaken by illiterate anglers"
    }, {
    "id": 66,
    "url": "http://localhost:4000/blog/2005/11/09/Making-the-wrong-development-choices/",
    "title": "Making the wrong development choices",
    "body": "2005/11/09 - I hate to be a curmudgeon about this, but this fellow needs a beat-down: Fixing AJAX: XmlHttpRequest Considered Harmful I offer this as exhibit A (as in AJAX) about why application security may well be intractable, in part because we’ve got mainstream technical outlets teaching techniques to evade well-founded security principles.  AJAX applications wouldn’t be possible (or, at least, wouldn’t be nearly as cool) without the XMLHttpRequest object that lets your JavaScript application make GET, POST, and other types of HTTP requests from within the confines of a web browser…. But the kind of AJAX examples that you don’t see very often (are there any?) are ones that access third-party web services, such as those from Amazon, Yahoo, Google, and eBay. That’s because all the newest web browsers impose a significant security restriction on the use of XMLHttpRequest. That restriction is that you aren’t allowed to make XMLHttpRequests to any server except the server where your web page came from… Too bad – your application is on www. yourserver. com, but their web service is on webservices. amazon. com (for Amazon). The XMLHttpRequest will either fail or pop up warnings, depending on the browser you’re using. (quick cut to Andy spitting up his coffee in disbelief at what he thinks he’s about to read)  On Microsoft’s IE 5 and 6, such requests are possible provided your browser security settings are low enough (though most users will still see a security warning that they have to accept before the request will proceed). On Firefox, Netscape, Safari, and the latest versions of Opera, the requests are denied… (sounds like good security engineering to me… what’s the problem?)  There is hope, or rather, there are gruesome hacks, that can bring the splendor of seamless cross-browser XMLHttpRequests to your developer palette. The three methods currently in vogue are: (Danger, Will Robinson! Glad I didn’t refill the coffee cup…)  Application proxies. Write an application in your favorite programming language that sits on your server, responds to XMLHttpRequests from users, makes the web service call, and sends the data back to users.  Apache proxy. Adjust your Apache web server configuration so that XMLHttpRequests can be invisibly re-routed from your server to the target web service domain.  Script tag hack with application proxy (doesn’t use XMLHttpRequest at all). Use the HTML script tag to make a request to an application proxy (see #1 above) that returns your data wrapped in JavaScript. This approach is also known as On-Demand JavaScript.  The basic idea of all three hacks is the same: fool your user’s web browser into thinking that the data is coming from the same domain as the web page. (Excellent. A good summation of potential threat vectors. But I can’t believe we’re about to read a serious discussion, signed-off by a serious publisher, about how to evade security protections. )  A word of caution here: there is a good reason why XMLHttpRequests are restricted. Allowing them to freely access any domain from within a web page opens up users to potential security exploitation. Not surprisingly, these three hacks, which offload the request to your web server, potentially threaten to disparage your web server’s identity, if not its contents. Caution is advised before deploying them. (The obligatory “Kids, don’t try this at home” message…. followed by the Snake River motorcycle jump. ) "
    }, {
    "id": 67,
    "url": "http://localhost:4000/blog/2005/11/07/Graphical-Integrity-Part-I/",
    "title": "Graphical Integrity, Part I",
    "body": "2005/11/07 - The folks at the NY Times have put together a nifty interactive graphic that diagrams the various data breach cases that have been disclosed since January. It breaks down when each incident occurred, and categorizes the incidents by industrial sector and geography of the disclosed parties: Stolen Identites: data breaches since January 2005 The graphic is interesting for two reasons. First, there’s the obvious one: it’s gives us a sense of the problem, quantitatively speaking. For example – not surprisingly, financial services appears to have suffered the most: 46m, or 12% of the total account disclosures. With regard to type of account, most of the problems were related to credit cards, phone records and the like. E-mail/online payment accounts only comprised about 14% of the issues. The second reason this graphic is interesting – and this is, in fact, the real point of this post – is the third graphic (“Case By Case”). The incidents are arranged horizontally, left-to-right, with shaded bubbles placed on the timelines at their respective dates. Cases with larger numbers of affected records have bigger bubbles. This is a pretty good time-series technique, in my opinion. Nice, but do you want to know what’s even better than that? Here’s where our friends at the Times get all sorts of bonus points. As I mentioned, the bubbles are sized in proportion to the number of affected account records. But the bubble’s size is not a simple linear mapping of records to diameter – but to AREA. Thus, for a given number of records, the radius of the circle is sqrt( records / ( _n_ x Pi ) ), where n is a scaling factor. Per Edward Tufte, scaling in proportion to area is a much more honest and appropriate technique to use than by diameter, due to the way the eye perceives such things. When @stake introduced its infamous 2x2 bubble chart back in early 2000, we scaled the “business impact” ratings in exactly the same way, using area scaling. We wanted issues scored a “5” to be perceived as 5x bigger than the “1” issues, not 25x. On a side note, Tufte has been known to consult for the NY Times on occasion. This graphic would seem to have his fingerprints on it; I wonder if he had a hand in it. Anybody know? "
    }, {
    "id": 68,
    "url": "http://localhost:4000/blog/2005/11/01/The-Cybertrust-Zotob-Study-Read-Between-the-Lines/",
    "title": "The Cybertrust Zotob Study&#58; Read Between the Lines",
    "body": "2005/11/01 - Rudolph Araujo, a contributor to the securitymetric. org mailing list, forwarded on a link to a Red Herring article about a new Cybertrust study on the impact of the Zotob worm by Russ Cooper. Cybertrust has an interesting model… when major security incidents happen, they make a habit of canvassing a wide group of companies that have agreed to participate. Looks like they are up to about 700 or so participants, not all of which are their customers. I actually really like and appreciate that Cybertrust takes the time to do this, although in this particular example I think they raised more questions than they answered. Exhibit A. Where’s the report, exactly? Annoyingly, the Cybertrust press release references the study, written by Russ Cooper, but doesn’t tell you how to get it. No hyperlink, no phone number, nothing. All of which makes me suspicious. And since I’m suspicious, let me throw some darts. Let’s look at the money quote:  Specifically, about 13 percent of organizations surveyed reported that they experienced at least some adverse impact from Zotob, defined as spending time, resources or money fighting or recovering from the worm. About 6 percent had a moderate or major impact from Zotob – more than $10,000 in losses and at least one business critical system affected (e. g. email, commerce, Internet connectivity). This compares to a moderate or major impact of more than 60 percent of organizations due to Nimda, and more than 30 percent of organizations due to Blaster. No Cybertrust customers reported moderate or major impact to their organizations due to Zotob. I don’t have the survey, so I can’t really tell what’s the difference between “at least some adverse impact” and “moderate or major impact,” other than the broad thresholds of $10K in “losses” and “one critical business system” affected. But these thresholds are less useful than they appear. For example, Citigroup has a lot of servers, and many are internet-facing. It’s conceivable that one minute of downtime might cost them $10,000. So is that a “major” impact relative to their daily revenue-generating, or just “moderate” – or even “minor”? On the other hand, I’m less bothered by the potential slipperiness of their non-definition of “business-critical system”, since that’s likely to be left to the customer to define – as it should be. From a spin-meister’s perspective, it’s always interesting when a vendor makes an absolutist claim like “No Cybertrust customers reported…” because it all depends on where one draws the lines – and one can always figure out a way to draw a line that facilitates such a claim. In this case, it’s between the “low” and “moderate” impact buckets. Although Cybertrust doesn’t tell us how many of the 700 organizations it surveyed are its customers, it’s clearly a small subset: let’s assume 1/3. Does that mean that all 230 of its customers could have sustained time, resource or hard-dollar losses of $9,999 and also the loss of, say, 100 “non-critical” systems each? That would still allow Cybertrust to credibly claim that “none” of its customers were particularly affected. Then there’s this quote at the bottom:  Cybertrust customers experienced about 50 percent less time to recover from Zotob, 55 percent less costs of recovery, 66 percent less network performance degradation, and 76 percent less desktop impact. In addition, Cybertrust customers had zero impact to critical business partner, email, Internet connectivity, remote user connectivity or other critical business functions. I won’t get into a detailed discussion of this one, other than offer a six word critique of what’s missing here: numerators, denominators, and units of measure. I take that back; two more words are needed: secret sauce, or rather lack thereof. Even if Cybertrust customers did experience fewer issues than other companies, surely the reason wasn’t merely because of the fact that they had a commercial relationship with Cybertrust. They must have, you know, done something to make themselves more secure – changed their processes, taken advantage of some marvelous Cybertrust technology, or availed themselves of their big brains. What might these things be? To be fair to Cybertrust, they’ve got a marvelous asset that they’re anxious to exploit: a well-honed mechanism for gathering security operational practice data, and on a scale that’s large enough to suggest that it might be representative. No doubt it provides Cybertrust with a wealth of information. But all of that tantalizing goodness is obscured by the opacity of their methods and data. Let in the sunshine! Now, it’s easy to trash-talk someone when they aren’t here to defend themselves. (Says HomerSimpson: Fun, too! ARJ: yes, sometimes) That’s not what I meant to do. But I would like to suggest, simply, that studies like these aren’t as helpful as they could be – nay, should be. Any studies deemed fit to publicize over the newswires should also be fit for scrutiny from academe, competitors and uh, cranks like me. Anybody got the survey on hand? I’d be happy to eat my words. "
    }, {
    "id": 69,
    "url": "http://localhost:4000/blog/2005/11/01/Fun-with-Spam/",
    "title": "Fun with Spam",
    "body": "2005/11/01 - Collecting Hamster Wheels of Pain is certainly a fun hobby. So is collecting the rather amusing e-mail addresses chosen by spammers to evade e-mail filters. Here are some good’uns from the 305 spam-grams from the past week:  Amyroh (x2; aka the name of a talented Sun Java engineer; clearly mined from apache. org dev lists) brian@domain. com (x2) C. Michael Patterson, M. D. (“Dear valued patient: get the safest prescription at the lowest cost here. ”) Deandre Pryor (“Curious? Come explore”) Donny Cervantes (““Inexpensive tablets here”… yes, but are the windmills cheap too?) drew. varner@oracle. com (subject: “Status”, plus viral payload. It’s not from Mary Ann…) Earthlink Customer Support (x100, self-congratulatory e-mails about blocked viruses; legitimate sender) eBay (“fraud alert”, with link to a not-very-disguised Australian server) Elmo Glass (subject: “you travel as horsemeat”, with inline image advertising diet/sex/memory pills) java2d-interest@sun. com (with Lovegate virus stripped out by Earthlink) jimmy@allaire. com (subject: “owmfj”) Lyle Kramer (subject: “Hello Football Raunchy Blond Mom Gets Hardcore C**tbang”) Micah Becker (subject: “Re: on begin to tether”) Parker Hardy (admittedly this is very funny, at least for those of you who have a memory for late 70’s primetime TV) Santos McGee (Wavmokqlq@optonline. com; advertising cheap mortgages; as if anyone with a brain would entrust financial matters with someone willing to disguise their identity. Assume it was a real name: interesting wedding that must have been…) Sherry Comer (subject: “That run an fondu”) trebor. a. rude@lmco. com (Interesting. With my A&amp;D background, I interpret this as “Trebor A. Rude at Lockheed Martin Corporation)Yeeeeargh. There aren’t any obvious patterns here, other than to note that we’ve got plenty of evasion methods on display: random dictionary constructions, website impersonation, old-fashioned come-ons and white-page-concatenation. From a metrics standpoint, over the one-week span I’ve received 305 spams in my e-mail inbox, plus another 512 blocked by my ISP (since October 28; which equates to about 1000 weekly), for a total of about 1300 spams per week. I have perhaps received (max) 100 legitimate e-mails in the same period, and probably closer to 50. That means my e-mail inbox is over 96% spam. Swell. "
    }, {
    "id": 70,
    "url": "http://localhost:4000/blog/2005/10/19/security-metrics-scorecards/",
    "title": "Security Metrics&#58; Scorecard Design",
    "body": "2005/10/19 - Author’s note: the chapter is not finished. It has some organizational and structural flaws that won’t be ironed out until later in the editing process. There are also some parts that need additional fleshing out. However, it should give you a good idea where my head is at. What does this mean to you? Simple. Since you’ve come to this website, you are by definition someone deeply interested in both measurement and security. I’d like to get your comments and feedback on the manuscript. The preferred method for giving me feedback is via the wiki. If you’ve got an account on the securitymetrics. org wiki, you can “mark up” the wiki page itself with your comments. Just put your comments underneath the relevant pages. [Ed: As of early 2013, comments have moved to Discus, below this page. ] I’m also happy to receive feedback privately via e-mail – especially if you work for an enterprise and would rather keep out of the spotlight.           A minor comment: I would add at least a paragraph for each of those metrics to explain what those are and why they are needed.        "
    }, {
    "id": 71,
    "url": "http://localhost:4000/blog/2005/10/13/WARNING-MESSAGE-YOUR-SERVICES-NEAR-TO-BE-CLOSED",
    "title": "WARNING MESSAGE&#58; YOUR SERVICES NEAR TO BE CLOSED.",
    "body": "2005/10/13 - I’m not a violent man. But I want the person who invented this spam subject line to be killed. Preferably by some method that is at once gruesome and medieval. Drawing in quarters would do nicely. Part of the reason I’m torqued is that I get this particular message so often. But I’m also an armchair grammarian, and I find the diction almost as offensive as its payload. I’ve heard it said that there are more people speaking or learning English as a second language as there are native speakers. This slime-covered, vermin-ridden little weasel’s effluvium is just Exhibit V. Oh, and while I’m at it — Earthlink can kiss my pucker. I don’t need a notification for every virus you block. Stop congratulating yourselves on the fact that your DAT files are up to date. Come to think of it, just don’t send me the notification at all. 95% of my e-mail (no kidding, I measured) is of the self-congratulatory “we stopped a virus; be grateful” variety. It’s 2005; if my friends are sending me viruses, it means they are 0wned. I can help them with that; but what I can’t do, apparently is rid myself of a cure — these incessant notifications — that is worth than the disease. Why is this so hard to figure out? Mr. Q, meet Mr. A. Shake hands and play nice. Now go sit down over there next to Mr. Usability. "
    }, {
    "id": 72,
    "url": "http://localhost:4000/blog/2005/10/13/Hamster-Wheels-of-Pain/",
    "title": "Hamster Wheels of Pain",
    "body": "2005/10/13 - A while ago I wrote a blog post called Escaping the Hamster Wheel of Pain decrying the lather-rinse-repeat cycle that the security industry seems to be fixated on. Here are some hamster wheels I’ve been collecting. I’ve obscured the names, sort of. I make no comment on these other than to note with amusement that they all resemble each other.      "
    }, {
    "id": 73,
    "url": "http://localhost:4000/blog/2005/09/30/A-Picture-is-Worth-1000-Words/",
    "title": "A Picture is Worth 1,000 Words",
    "body": "2005/09/30 - We’ve had some interesting chatter on the securitymetrics mailing list today about sparklines: tiny, intense, word-size graphics. This is one of Edward Tufte’s latest confections. His formal definition is here. An excellent example is the Nasa “ozone hold graphic”. Pete Lindstrom asked whether sparklines could have some use in the information security field. Here’s an example, from my (as yet unpublished) book, Security Metrics (Addison-Wesley Publishing, expected 1H 2006). The first graphic (below) shows the distributed network intrusion detection project DShield. The small multiples are in the column labeled “Activity Past Month”; these show the relative number of hostile scans encountered for the network services enumerated in the “Service Name” column. Although the X- and Y-axis labels are not shown, it seems clear enough what they must be: the vertical axis scale starts at zero and increases at a linear – or possibly logarithmic – rate to maxima held constant in all graphs. The X-axis shows how the relative number of scans varies over time. Minor quibbles about labeling aside, the use of small multiples in this exhibit enables the reader to quickly get a sense of the which ports are most likely to be scanned: in this case, 445 and 135 – two ports associated with Windows services that are often prone to compromises. A network administrator running an all-Windows environment, for example, might see this exhibit and decide to push out a group policy temporarily restricting access to these ports.  One can easily imagine how this exhibit could be enhanced. Instead of simply showing the “top 10” most-scanned ports, we could show the top 100, or a subset of the most common well-known ports. Doing so would require some graphical nips and tucks; the “Explanation” column would need to vanish, and we would want to combine the “Service Name” and “Port Number” columns. From the point of view of aesthetics, representing the scan results as solid filled area charts on a white background (instead of black) could increase the small-multiple format’s readability. An intriguing small-multiple format that would work well here is the sparkline – a minimalist “simple, intense, word-sized graphics” format invented by Tufte.  The figure below shows a fictitious re-drawing of the previous exhibit using sparkline format, constructed using Excel. Each mini-chart includes a dark gray line to show the trend for each cross-section, as well as a light gray band denoting the “normal” range; that is, the mean value plus or minus the standard deviation.  So that the reader can understand the plot lines in context, the final data point in each series is highlighted with a red marker and numeric label.  "
    }, {
    "id": 74,
    "url": "http://localhost:4000/blog/2005/09/25/Information-Security-OCD/",
    "title": "Information Security OCD",
    "body": "2005/09/25 - Just saw the Scorsese/DiCaprio film The Aviator for the first time. The film is remarkable not just for its lush cinematography, crisp writing and convincing special effects, but also for its frankness in documenting Howard Hughes’ Obsessive-Compusive Disorder (OCD). At the risk of sounding a bit like this guy, the OCD motif, particularly the bit at the end where he repeats, ad nauseum, “the way of the future… the way of the future… the way of the future…” reminds me a bit of the repetitive, compulsive fixation of late on so-called intellectual property leak detection. One might say that perfidious insiders are a germ of a different sort, from which we can never get clean. Repeat after me: the insider threat the insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threatthe insider threat… "
    }, {
    "id": 75,
    "url": "http://localhost:4000/blog/2005/09/22/The-Symantec-Threat-Report-Read-Between-the-Lines/",
    "title": "The Symantec Threat Report&#58; Read Between the Lines",
    "body": "2005/09/22 - Like many other people, I’ve downloaded and read the semi-annual Symantec Threat Report. I’ve always been a fan of this publication, which provides a level of texture, richness and depth about malware and threat trends that isn’t easy to get anywhere else. Symantec understands they’ve got an exploitable asset – their DeepSight sensor network – and they’re flogging it for all it’s worth. Good on ‘em. There’s been plenty of ink spilled in the press (e. g. , Computerworld, El Reg ) about what the latest report means. Controversies and headlines abound: is Firefox really less secure than IE? Are Mac users living in a “false paradise” as the report claims? Are botnets running the universe? All of these are important questions, and the report gives information on all of them. I recommend you read the report for yourself, and reach your own conclusions. That said, I find the report more interesting for what it doesn’t say. Reading between the lines is the best way to read the Symantec Threat Report. First, understand the report’s motive: to sell Symantec gear, software and services. Throughout the report, Symantec doesn’t miss a single opportunity to sell its wares. Enterprises are explicitly advised to purchase:  antivirus software vulnerability alerting services traffic shaping hardware/software anti-spam software e-mail filtering software managed security services personal firewalls…all of which SYMC sells. Now, they aren’t quite brazen enough to recommend their own products, but it is clearly implied. Interestingly, SYMC does not recommend patch management software – something that isn’t in their portfolio. Second, the best way to interpret the Symantec data itself is for trending information, not for absolute numbers. For example, it’s interesting that:  the number of unique Spybot variants has increased by nearly 6,300 variations, to over 8,800 the percentage of vulnerabilities that were “easy to exploit” (no exploit needed, or had exploit code) was 73% 13% of vulnerabilities had exploit code available 8 out of 10 adware/spyware products were installed via ActiveX or browser help objects (BHOs) Rental pricing for a 150k-strong botnet is $300 (for some unclear period of time) Implied DDOS attacker profile is 15 bots (15k average active bots daily divided by 1,000 detected attacks/day)But some of the other statistics raise more questions than answers. Symantec tells us, for example, that it detects just under 10,000 active zombie PCs on a daily basis. That sounds impressive, but it’s hard to tell what percentage of their sample base this represents. It’s not even clear that the measured increase in the number is relevant either, since we don’t know whether they’ve increased their sensor network during the reporting period. Thus, we have no real way of knowing exactly how big the botnet problem really is, nor can we tell whether there are more botnets, or fewer. Third, if you interpret the numbers a certain way, they would appear to expose some deficiencies and blind spots in Symantec’s portfolio. For example, the huge increase in the number of malware variations begs the question of exactly how close to the limits of signature-based technologies we might be. I was at Peter Szor’s recent presentation last week at the RSA Power Day event in NY, and he told me that Symantec is seeing close to 4,000 new variations a month. If you fast-forward a bit, suppose the number trends to 10,000 a month.  That’s an implied net number of new signatures of 120,000 annually. There’s just no way a signature-based product is going to be able to cope with the memory demands this implies, never mind the disk space usage. Of course, signatures are at the core of SYMC’s business. Another potential blind spot is exposed by their spam numbers. In contrast to every other message security vendor I’ve spoken to, SYMC says that the percentage of mail that is spam, as reported by BrightMail is actually decreasing. They attribute the decrease to the use of traffic-shaping products (more subtle plugging, this time for TurnTide boxen). I think attributing the decrease to traffic-shaping may be disingenuous; it could also signal a decrease in effectiveness of their core content-filtering software. This is almost certainly true in any case; detection strategies that look at sender traffic patterns are a much more prominent part of modern anti-spam architectures. Thus, we have yet another example where the numbers tell us more about Symantec’s aging product line than they do about the actual threat landscape. Fourth, I can’t figure out why Symantec seems to have a bee in their bonnet with respect to Apple. For the second straight report, they take great pains to point out how Mac users ought to be quavering in their boots, instead of blissfully living in a “false paradise”. What, is Cupertino, California not big enough for the both of them? They seem to be perfectly willing to grant Mozilla a pass, in spite of the increased number of vulnerabilities relative to IE. But when it comes to Macs, they gravely intone the specter of “increased vulnerabilities” and attacks. (NB: they do not, in fact, document a single actual attack. ) So why the double standard? To be fair, Symantec does mention in passing a bona fide rootkit for OS X (“Weapox”). But there’s no analysis whatsoever about its threat vectors, infection prerequisites, propagation frequency, or prevalence in the wild. There’s no question that this rootkit is real, but its significance is minimal. In general, Symantec’s Ahab-like quest to harpoon Jobs’ White Whale seems like FUD FUD FUD to me. Joe Wilcox has arguably the best take on the subject:  Then there is the Mac question. Apparently, the report warned that Mac users are living in lala land, assuming that they are safe from all those pesky viruses, worms and spyware infections. Symantec warned that Mac OS X’s day of reckoning is coming, when virus writers turn their attention to Apple’s Unix-based operating system. OK, so why is it then that Symantec has basically pulled out of the Mac security market, or that the report conceded that there was no real change in Mac OS X security risk from the previous six months? Why not turn the issue on its head? If the Mac is free of malware at the present time and will be into the near future, then it’s free of malware. It seems to me the most intellectually honest recommendation Symantec could make – after enumerating in ridiculous detail all of the various species, genotypes and mutations of Windows malware – would be to tell customers to buy Macs. Yet another thing Symantec’s Threat Report doesn’t say. "
    }, {
    "id": 76,
    "url": "http://localhost:4000/blog/2005/09/22/Odds-and-Ends/",
    "title": "Odds and Ends",
    "body": "2005/09/22 - At the risk of turning this into a link blog, here are two nifty articles that drifted across my field of view today:  Google: Putting Crowd Wisdom to Work. Interesting article about how Google uses “prediction markets” inside Google to forecast the likelihood of future events. There’s been a lively discussion on the mailing list about this lately, so it’s quite timely.  User triangulation: how to listen to customers. Breaks down customers into Influential Adopters, Leading Adopters, Middle Adopters, and Late Adopters. I’ve seen articles on this subject before, but this one is short, sweet, and well-written. A useful tonic to product managers everywhere. Both are well worth reading. "
    }, {
    "id": 77,
    "url": "http://localhost:4000/blog/2005/09/02/IE7-Anti-Phishing-is-Land-Grab-in-Disguise/",
    "title": "IE7 Anti-Phishing is Land-Grab in Disguise?",
    "body": "2005/09/02 - Have you been following Microsoft’s plans for IE7? I have, and a blog post I read about their anti-phishing plans just about made me spit out my coffee. I’m going to do a research note for Yankee Group (my employer) on this soon, but in the interim I thought I’d pass this along. The gist of it is that the IE browser will keep a cache of “trusted sites”, and if you browse to a site that’s not in the cache your browser will phone Microsoft and ask. Pretty scary stuff, in the sense that a monopolist (MS) will be judging who’s trustworthy (or not). Entirely proprietary, and not peer-reviewed as a standard or anything. At least digital certificates have a standard real-time protocol for identifying revoked certificates: OCSP. (Not that people use it much. ) Oh, and check out this excerpt from the IE7 privacy statement about Phishing Filter:  Phishing Filter can warn you if the Web site you are visiting might be impersonating a trusted Web site. Phishing Filter does this by first checking the address of the Web site you are visiting against a list of Web site addresses stored on your computer that have been reported to Microsoft as legitimate (“legitimate list”). The first time you attempt to visit a Web site that is not on the legitimate list, you will be asked whether you would like to have Phishing Filter automatically check all Web sites you visit. If you opt in, addresses not on the legitimate list will be sent to Microsoft and checked against a frequently updated list of Web sites that have been reported to Microsoft as phishing, suspicious, or legitimate Web sites. You may also choose to use Phishing Filter manually to verify individual sites.  When you use Phishing Filter to check Web sites automatically or manually, the address of the Web site you are visiting will be sent to Microsoft, together with some standard information from your computer such as IP address, browser type, and Phishing Filter version number. To help protect your privacy, the address information sent to Microsoft is encrypted using SSL and limited to the domain and path of the web site. Other information that may be associated with the address, such as search terms, data you entered in forms, or cookies, will not be sent.  For example, if you visited the MSN search web site at http://search. msn. com and entered “MySecret” as the search term, instead of sending the full address “http://search. msn. com/results. aspx?q=MySecret&amp;FORM=QBHP”, Phishing Filter would remove the search term and only send “http://search. msn. com/results. aspx”.  Anonymous statistics about your usage of Phishing Filter will also be sent to Microsoft such as the time and total number of Web sites browsed since an address was sent to Microsoft for analysis. This information, along with the information described above, will be used to analyze the performance and improve the quality of the Phishing Filter service. Microsoft will not use the information it receives to personally identify you. This is a pretty large land grab, and nobody seems to have grasped it yet. Among other things they’ll be able to track where people surf (by IP address and some other “standard information”, but “without personally identifying you”). All of this in the guise of “safety. ” The cheek – no wonder they didn’t buy Claria. They won’t need to. Of course, I could just be paranoid. "
    }, {
    "id": 78,
    "url": "http://localhost:4000/blog/2005/09/01/The-0wnership-Society/",
    "title": "The 0wnership Society",
    "body": "2005/09/01 - Webroot has lately been producing a series of quarterly statistics on infection rates for four types of badness:  Adware Trojan horses – botnet software falls into this category System monitors – includes key loggers Tracking cookiesNow, one could certainly raise objections about selection bias. But the data is still pretty interesting nonetheless. The latest figures show that trojan horses and system monitors (combined) were found on 7% of corporate PCs in Q2 2005. In Q4 2004 and Q1 2005, in which they break out trojans separately, the percentages are the same (7%). Webroot does not break out how many PCs were scanned during the reporting period, although in the first two quarters it totaled about 35,000 systems. Cumulative is 60,000, thus the latest period is 25,000 scans. Median number of machines scanned per corporation is eight, which suggests their “enterprise” business — isn’t. Consumer figures are scarier, as you might imagine. For Q2 2005:  6% of consumer PCs had system monitors on them 16% had trojans on them (in Q1 2005 it was 19%; Q4 2004 15%; Q3 2004 11%; Q2 2004 13%; Q1 2004 15%)Sample base was &gt; 1M scanned PCs in Q2 2005. As for cookies and adware – 60-70% of companies and consumers had some of each. Thus, if you believe Webroot, about 15% of consumer PCs and 7% of corporate PCs were probably 0wned when they did their scans. Putting it another way, consumers are twice as likely to be 0wned as corporations. Given their average sample size for enterprises, however, I would suspect that the largest companies (who can afford HIPS products, and anti-spyware rollouts etc) are under-represented – were they present in the data in larger numbers, that would skew the numbers downward more. "
    }, {
    "id": 79,
    "url": "http://localhost:4000/blog/2005/05/04/Escaping-the-Hamster-Wheel-of-Pain/",
    "title": "Escaping the Hamster Wheel of Pain",
    "body": "2005/05/04 - Risk Management is Where the Confusion Is: Lately I’ve been accumulating a lot of slideware from security companies advertising their wares. In just about every deck the purveyor bandies about the term “risk management,” and offers their definition of what that is. In most cases, their definition does not match mine. I first encountered the term “risk management” used in the context of security in 1998, when I read Dan Geer’s landmark essay Risk Management is Where the Money Is. At the time, I was completely gobsmacked by its thoughtfulness and rhetorical force. It was like a thunderbolt of lucidity; several days later, I could still smell the ozone. Seven years later, I’ve found that much of his essay stands up well. Unfortunately for Dan’s employer at the time, purveyors of bonded identities did not flourish and monetize electronic commerce security as he had predicted. But he was absolutely correct in identifying digital security risk as a commodity that could be identified, rated, mitigated, traded, and above all, quantified and valued. Most of the decks I see miss this last point. Nearly everyone shows up with a doughnut-shaped “risk management” chart whose arrows rotate clockwise in a continuous loop. Why clockwise? I have no idea, but they are always this way. The chart almost always contains variations of these four phases:  Assessment (or “Detection”) Reporting Prioritization MitigationOf course, the product or service in question is invariably the catalytic agent that facilitates this process. Follow these four process steps using our product, and presto! you’ve got risk management. From the vendor’s point of view, the doughnut chart is great. It provides a rationale for getting customers to keep doing said process, over and over, thereby remitting maintenance monies for each trip around the wheel. I can see why these diagrams are so popular. Unfortunately, from the perspective of the buyer the circular nature of the diagram implies something rather unpleasant: lather, rinse, repeat – but without ever getting clean. (I’ve been collecting these charts, by the way. Soon, I hope to be able to collect enough to redeem them for a Pokemon game. ) Here’s the cynical version of the “risk management” doughnut, which I call the “Hamster Wheel of Pain”:  Use product, and discover you’re screwed Panic Twitch uncomfortably in front of boss Fix the bare minimum (but in a vigorous, showy way). Hope problems go awayThe fundamental problem with the Hamster Wheel model is simple; it captures the easy part of the risk management message (identification and fixing things) but misses the important ones (quantification and triage based on value). Identifying risk is easy because that is what highly specialized, domain-specific diagnostic security tools are supposed to do. They find holes in websites, gaps in virus or network management coverage, and shortcomings in user passwords. Quantifying and valuing risk is much harder, because diagnostic tool results are devoid of organizational context. To put this simply, for most vendors, “risk management” really means “risk identification,” followed by a frenzied process of fixing and re-identification. That’s not good enough, if you believe (as I do) that risk management means quantification and valuation. If we were serious about quantifying risk, we’d talk about more than just the “numerator” (identified issues). We’d take a hard look at the denominator: that is, the assets the issues apply to. Specifically, we would ask questions like:  What is the value of the individual information assets residing on workstations, server, and mobile devices? What is the value in aggregate? How much value is circulating through the firm right now? How much is entering or leaving? What is its velocity and direction? Where are my most valuable (and/or confidential) assets? Who supplies (and demands) the most information assets? Are my security controls enforcing or promoting the information behaviors we want? (Sensitive assets should stay at rest; controlled assets should circulate as quickly as possible to the right parties; public assets should flow freely) What is the value at risk today – what could we lose if the “1% chance” scenario comes true? What could we lose tomorrow? How much does each control in my security portfolio cost? How will my risk change if I re-weight my security portfolio? How do my objective measures of assets, controls, flow and portfolio allocation compare with others’ in my peer group?These aren’t easy questions. Pete Lindstrom’s earlier post to this list on asset valuation (and the ensuing thread) highlighted how difficult achieving consensus on just one of these questions can be. I don’t claim to have any of the answers either, although some of the folks I’ve invited to be on this list have (and are) taking a shot at it. What I do know is that none of the commercial solutions I’ve seen (so far) are even thinking about these things in these terms. Instead, it’s the same old hamster wheel. You’re screwed; we fix it; 30 days later, you’re screwed again. Patch, pray, repeat. The security console sez you’ve got 34,012 “security violations” (whatever those are). And uh, sorry to bother you, but we’ve got 33 potential porn surfers in the Finance department. We’re so far away from real risk management that it’s not even funny, and I’m wondering if it’s even worth the effort in the short term. Is it time to administer last rites to “risk management”? I think it is.  This is not as controversial a statement as it might sound. For example, risk management is often confused with risk modeling–an entirely different but vital part of security analysis. Risk modeling research helps us understand how security works (or doesn’t). Research from folks on this list, (Geer, Soo Hoo, Kadrich, Shostack, Lindstrom, Eschelbeck) and others continues to astound and amaze me. Economists we need; what we don’t need are traveling salesmen bearing Wheels of Pain. My point is that “risk management” as an overarching theme for understanding digital security has outlived its usefulness. Commercial entities abuse the definition of the term, and nobody has a handle on the asset valuation part of the equation. Key Indicators Supplant Risk Management: If “risk management” isn’t the answer, what is? Even though he was not the first to say it, Bruce Schneier said it best when he stated that “security is a process. ” Surely he is right about this. But if it is a process, exactly what sort of process is it? I would submit to you that the hamster wheel is not what he (or Dan) had it mind. Here’s the dirty little secret about “process”: process is boring, routine, and institutional. Process doesn’t always get headlines in the trade rags. In short, process is operational. How are processes measured? Through metrics (numbers about numbers) and key indicators (consistently-measured health diagnostics). In mature, process-driven industries, just about everyone with more than a few years tenure makes it their business to know what the key barometers of the firm are. Take supply chain, for example. If you are running a warehouse, you are renting real estate that costs money. To make money you need to minimize the percentage of costs spent on that real estate. The best way to do that is to increase the flow of goods through the warehouse; this spreads the fixed cost of the warehouse over the higher numbers of commodities. Unsurprisingly, the folks running warehouses have a metric they use to measure warehouse efficiency. It is called “inventory turns”; that is, the average number of times the complete inventory rotates (turns) through the warehouse on a yearly basis. When I was at FedEx in the mid-90s, the national average for inventory turns was about 10-12 per year for manufacturers. But averages are just that; all bell curves have tails. The pathetic end of the curve included companies like WebVan, who forgot they were really in the real estate business and left one of the largest dot-com craters as a result. No density = low flow-through = low turns = death. Dell occupies the high end: back in 1996, their warehouses did 40 turns a year. 40! Dell’s profitability flows directly from their inventory management and supply chain excellence; Kevin Rollins surely eyes this number like a hawk. I use this (simple) example only because it illustrates how a relatively simple key indicator speaks volumes about company operations. It isn’t the only number warehouse operators watch, but it is one of the most important. Supply chain operators also tend to look at freight cost per mile, percentage of “empty” (non-revenue generating) truck miles, putaway and pick times, and distribution of “A”/ “B”/”C” velocity SKUs, among others. Several themes emerge from the supply chain key indicators list. First of all, every example in the list incorporates measures of time or money – and since time multiplied by labor costs yields money, it’s fair to say that they all ultimately reduce to monetary measures. Second, all of the indicators are well-understood across industries and consistently measured. That means they lend themselves to benchmarking, in the sense that a high-priced management consultancy could go to a wide variety of firms, gather comparable statistics, gain useful analytical insights, and make a buck on the analysis. Third, although the numbers themselves may not always be straightforward to calculate, they can nonetheless be gathered in an automated way. I think this is where we need to go next. What I want to see nowadays from vendors or service providers is not just another hamster wheel – what I want is a set of key indicators that tells customers how healthy their security operations are, on a stand-alone basis and with respect to peers. Barbering on about “risk management” just misses the point, as does fobbing off measurement as a mere “reporting” concern. If solution providers believed that particular metrics were truly important, they would benchmark them across their client bases. But they don’t; every vendor I’ve posed this idea to looks at me like I have two heads. (Gerhard’s firm might be the lone exception, bless him. ) My short list of (mildly belligerent) vendor questions, therefore, includes these:  What metrics does your product or service provide? Which would be considered “key indicators”? How do you use these indicators to demonstrate improvement to customers? Could you, or a management consultant, go to different companies and gather comparable statistics? Do you benchmark your customer base using these key indicators?I want to see somebody smack these out of the ballyard, but somehow I think I’ll be waiting a while. Time to get off the Hamster Wheel of Pain; next up, key indicators. I could go on, but you get the idea. Comments and spirited ripostes are welcome as always. And if you are part of (or know) an organization that is doing this well, I want to hear about it, off the record of course. "
    }, {
    "id": 80,
    "url": "http://localhost:4000/blog/2005/04/21/Web-Services-Confusion/",
    "title": "Web Services Confusion",
    "body": "2005/04/21 - Scobleizer points out that the WS ReliableMessaging specification has been submitted to OASIS. With all due respect to the incredibly bright folks at the WS-I, I find the world of web services standards to be rather confusing. In addition to this new spec, we also have the WS-I Basic Profile (the initial version 1. 0, plus the New and Improved!!! version that features “security”), WS-Federation (apparently a substitute for a perfectly good, and elegant, solution called the Liberty Alliance), WS-Transfer, WS-Conversation, WS-Security, and about a dozen others that I can’t remember. About the only thing that’s missing is the WS-LittleLessConversation specification, which (as I understand it) will feature hashed messages using secret, random salts that will ensure that nobody can talk to anybody else. I’m not a web services guy; I just dabble in these things when the mood (or lighting) makes it worthwhile. But even so, it seems fairly obvious to me that the whole edifice is in danger of collapsing under its own weight. I made this point to an XML security vendor I spoke with the other day; I asked him this: do you really want to hitch your wagon to the whole web services phenomenon, especially since folks like Google are making a mockery of the whole thing by cobbling together allegedly “inelegant” solutions that (shudder) solve real problems and look terrific? He agreed with me, and said that yes, the REST protocol makes a lot of sense because people understand what it is, and how to use it. Isn’t that the point? Adam Bosworth and Tim Bray had it right: keep it simple, and have a few laughs about the subject when you can:  I’m going to stay out of the way and watch the WS-visionaries and WS-dreamers and WS-evangelists go ahead and WS-build their WS-future. Because I’ve been wrong before, and maybe they’ll come up with something that WS-works and people want to WS-use. And if they do that, I’ll stand up and say “I was WS-wrong. ” The specs are certainly baroque enough, but from a security perspective, web services are a hard problem. The real issues boil down to mapping credentials expressed in message headers into the security regimes of the runtime environments. Put simply, you get a web services message with a header containing a Principal authorising the action. From the app’s perspective it would be really nice to be able to associate that with the roles you’ve spent so long baking into your application. The last time I checked, neither the WebSphere stack nor the . NET stack did a good job with this; there’s so many Principals floating around it’s like being back in high school. So at the end of the day this means we’re wading through stacks of schema, huge XML files and incredibly detailed message structures just to parse a bloody payload. The roll-your-own REST approach looks decidedly more attractive by comparison, and one hell of a lot faster. "
    }, {
    "id": 81,
    "url": "http://localhost:4000/blog/2004/12/02/Voting-With-Your-Feet/",
    "title": "Voting With Your Feet",
    "body": "2004/12/02 - Most people know that the dominant computing platform has a little problem with security. It’s a little problem with big consequences. Recently, research firm IDC released a report indicating that two thirds of PCs – including those deployed within companies – are affected by some sort of spyware. IDC’s report, Worldwide SpyWare 2004-2008 Forecast and Analysis: Security and System Management Sharing Nightmares, projects that companies will spend over $305 million on anti-spyware software and services by 2008, up from $12 million now. It is always useful to consider the source of this information – IDC’s targeted customer base is hardware and software companies, not end-customers. Caveat vendor. But take the figures at face value: $300 million is a lot of money – about 1/3 of the revenue of a software company the size of Novell, or 1/2 the size of Compuware. If the IDC projections hold up, that means that over the next four years, Windows platform vulnerabilities will indirectly fund the creation of many new companies seeking to feed at a $300 million trough. That is enough money for a medium-sized venture capital fund. But that’s not all. Over the last year, Windows anti-virus and patch management vendor revenues likely exceeded $650 million, based on my conservative estimates. Neither of these categories would have been able to grow to meaningful sizes unless there hadn’t been, ahem, a sizable market opportunity. Now go back and re-read that last paragraph. In case the point isn’t clear, nearly $1 billion in economic value has either been created (or destroyed, depending on your point of view) entirely by accident, because of what economists would call an externality. In the same way that industrial polluters created a market for smokestack scrubbers and emissions permits, so it has been with operating system security. The metaphor falls short, however, when one realizes that these new security markets did not need to exist at all. All Microsoft would have needed to do was design better products. It doesn’t have to be this way, and on operating systems such as Apple’s OS X, it isn’t. Consider three fundamental aspects of Windows (including 2000 and XP) that show just how much design, not installed base, drives vulnerabilities:  Windows registry. All users (and by extension all programs) need read-write access by default to a small number of files that are critical for system functioning: the Windows registry. All the houses in the neighborhood, so to speak, are emptying their sewage onto the same grassy field. Why commingle security concerns this way? In OS X, by contrast, applications manage their own preferences, and these are in almost all cases stored in the user’s home directory in separate files. This makes security issues potentially much easier to compartmentalize, because applications are (or can be) restricted at the file system level.  Vulnerable services run by default. Much ink has been spilled in other places about how Windows (especially pre-XP SP2) leaves vulnerable network services listening by default, even in an out-of-the box install. Under such conditions, the half-life of a virgin XP desktop is what, 15 minutes? In contrast, the Mac ships with exactly zero ports open.  No “speed bump” for administrative operations. Windows doesn’t have the concept of Unix sudo. Instead, users with administrative privileges can do anything without being challenged or even audited. Privileged users typically include Windows service accounts, application runtime accounts, and even Aunt Millie – who granted herself admin rights at install just like the nice wizard told her to do. This, combined with the integration of ActiveX controls into Internet Explorer, is the reason why spyware is pervasive. Compare this to OS X (or Linux). An operation requiring extra privileges forces the user to re-authenticate interactively; the command itself is logged for posterity. None of these issues have anything to do with the language they were coded in. For that matter, they could have been done in . NET. But they do help explain how certain design choices have helped create the Windows Security Pandemic. That monoculture’s one hell of a petri dish. What to make of all this? First of all, companies should stop what they’re doing and run the numbers. Add up what the total cost of desktop platform insecurity is costing you. Third-party desktop firewalls, anti-virus, anti-spyware and patch management software all count towards the total. Then, make a conservative estimate of what the monthly patch-and-pray ambulance drill is costing you in labor soft dollars. It’s a safe bet that the costs of coping with insecurity are “crowding out” expenditures in other areas. One person I spoke with at a large bank indicated that ASN. 1 SSL encoding vulnerability, for example, cost the firm more than $1. 5 million in hard and soft dollars to handle, including considerable expenses for temporary labor. That money was almost certainly carved out of the operational budget. Second, companies with survival instincts should seriously consider an alternative platform strategy. A small pilot program of alternative desktop operating systems could be a useful way of determining whether a less costly, albeit different, platform would make sense. My personal experience with Mac OS X and various Linux distributions has led me to conclude that OS X offers a larger variety of compatible desktop software and better user experience than Linux, but your mileage may vary. There is evidence that some forward-looking companies have reached similar conclusions. According to a story originally published in NewsFactor, AT&amp;T is currently evaluating both Linux and OS X to determine whether “hedging their bets” makes sense. In addition, IBM has previously stated that it will migrate to a Linux-based desktop by the end of 2005. Microsoft is not the devil, and Bill Gates does not have horns and a tail. But prior mistakes in the designs of the company’s products are costing some companies astounding amounts of real money – and making others rich. Customers should “get the facts” and consider alternative strategies. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-center ">
				<div class=" col-md-8  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/categories.html#security">security</a><span class="sep">, </span>
                        
                          <a class="sscroll text-danger" href="/categories.html#metrics">metrics</a><span class="sep">, </span>
                        
                          <a class="sscroll text-danger" href="/categories.html#passwords">passwords</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Passwords-O-Plenty</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" alt="Andrew Jaquith" src="/assets/7b99df-4721eb824b20a090bf9d14251f3212a6a7adacfda9b6a3fc541f523119b827c3.jpg">
                        
						<small class="ml-3"> Andrew Jaquith <span><a target="_blank" href="https://twitter.com/arj" class="btn btn-outline-success btn-sm btn-round ml-1">Follow</a></span>
                            <span class="text-muted d-block mt-1">Feb 05, 2008 · <span class="reading-time">
  
  
    2 mins read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
			</div>
		</div>
	</div>
</div>





<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">
        
        
        <!-- Share -->
		<div class="col-lg-2 pr-4 mb-4 col-md-12">
			<div class="sticky-top sticky-top-offset text-center">
				<div class="text-muted">
					Share this
				</div>
				<div class="share d-inline-block">
					<!-- AddToAny BEGIN -->
					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
						<a class="a2a_button_facebook"></a>
						<a class="a2a_button_twitter"></a>
					</div>
					<script async src="https://static.addtoany.com/menu/page.js"></script>
					<!-- AddToAny END -->
				</div>
			</div>
		</div>
        
        
		<div class="col-md-12 col-lg-8">
            
            <!-- Article -->
			<article class="article-post">                
			<p>Before the holidays I ran a quick, three-question, survey of the securitymetrics.org mailing list membership about the number of passwords people use. Here are the results, drawn from 51 responses (not bad, considering the list membership is about 400 people). I’d promised the respondents that I’d share the results… so here they are.</p>

<!--more-->

<h3 id="securitymetricsorg-quickie-survey-online-credentials">Securitymetrics.org Quickie Survey: Online Credentials</h3>

<p><strong>1. How many online accounts do you manage, in total? How many “sensitive” accounts do you maintain?</strong></p>

<p>By “account” I mean a public or private website, server or network that you log in to, for which you maintain a password or other credential. For example, a password or application entry in an OS X Keychain could be considered an account.</p>

<p>For purposes of this question, “sensitive accounts” means ones that you would consider problematic if they were compromised. Typically, these could be accounts that keep credit card information, manage your 401k details, or contain employment details.</p>

<p>Results (n=51):</p>

<table class="table table-striped"><thead><tr><th>Metric</th><th>All accounts</th><th>Sensitive accounts</th></tr></thead>
<tr><td>Mean</td><td>60.7 accounts</td><td>20.6 accounts</td></tr>
<tr><td>Standard deviation</td><td>55.0</td><td>29.7</td></tr>
<tr><td>Min</td><td>3</td><td>0</td></tr>
<tr><td>First quartile</td><td>23.5</td><td>6</td></tr>
<tr><td>Median</td><td>40</td><td>15</td></tr>
<tr><td>Second quartile</td><td>72.5</td><td>25</td></tr>
<tr><td>Max</td><td>207</td><td>207</td></tr>
<tr><td>Mode</td><td>40</td><td>20</td></tr>
</table>

<p><em>Comments:</em> I draw 3 conclusions from these figures.</p>

<ul>
  <li>First, people have lots of accounts to keep track of – on average.</li>
  <li>That said, the quartiles and median show that respondents skew towards the “conservative case” – that is, they most don’t tend to maintain too many accounts. A few crazy outliers (like me) are pushing the average number up.</li>
  <li>Third, the ratio of sensitive-to-non-sensitive accounts stays fairly constant across quartiles, ranging from 26-38%. In other words: of all of the account passwords people maintain, it’s a fair bet that about a third of them will be “sensitive.”</li>
</ul>

<p>I’d also note that the survey base is self-selected – in the sense that it’s the members of this list. Most of us are professional paranoids, right? Not sure if that means that the average user is worse off than the respondent base (more passwords to keep track of) or better off. Regardless, I’d say it does confirm what I already knew: we’re drowning in passwords. Further insights or armchair-psychology comments welcome.</p>

<p><strong>2. What is your primary coping strategy for managing your online accounts?</strong></p>

<ul>
  <li>I keep all of my passwords the same: 10%</li>
  <li>I write everything down on paper: 12%</li>
  <li>I use a form-filler product, like Apple’s Keychain, and use random passwords 12%</li>
  <li>No particular strategy: 20%</li>
  <li>Other: 47%</li>
</ul>

<p><em>Comments:</em> I can’t draw too many conclusions from the responses to this question, because I asked it badly. Considering that my day job is as an analyst, you’d think I would’ve asked this question in a way that got better answers. :)</p>

<p><strong>3. Do you like the idea of surveying securitymetrics.org members about security practices?</strong></p>

<ul>
  <li>Yes: This is a good idea: 92%</li>
  <li>No: I’ve got enough spam as it is: 8%</li>
</ul>

<p><em>Comments:</em> Everyone seems to like the idea of surveying the membership more often. Cool! I’ve asked mailing list members to suggest ideas for future surveys.</p>

<p><em>Note:</em> I’ve proposed that we spend some time on the subject of community-building at this year’s Mini-Metricon at RSA. More on this later… Betsy Nichols is going to put up a blog entry about Mini-Metricon on the website later today.</p>
                
			</article>
			
			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				</span>
			</div>
 
            <!-- Mailchimp Subscribe Form -->
            
            
            
             <!-- Author Box -->
                				
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" alt="Andrew Jaquith" src="/assets/150504-0e96c06a4c19e295c7925d04c2c6fc0ca8a882bd1fe474f6089764753134c712.jpg">
                         
					</div>
					<div class="col-md-10">		
                        <h5 class="font-weight-bold">Written by Andrew Jaquith <span><a target="_blank" href="https://twitter.com/arj" class="btn btn-outline-success btn-sm btn-round ml-2">Follow</a></span></h5>
						I&rsquo;m <a href="http://www.linkedin.com/in/ajaquith">Andrew Jaquith</a>, a Wall St Managing Director. Previously, I was CTO <a href="http://www.silversky.com">SilverSky</a>, and former analyst with <a href="http://www.forrester.com">Forrester</a> and <a href="http://www.yankeegroup.com">Yankee Group</a>. My interests include security and risk, anything data-related, app development, visualization, good writing and spirited discussion.					
					</div>
				</div>				
                
            
            <!-- Comments -->
            
                <!--  Don't edit anything here. Set your disqus id in _config.yml -->

<div id="comments" class="mt-5">
    <div id="disqus_thread">
    </div>
    <script type="text/javascript">
        var disqus_shortname = 'markerbench'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
</div>
            
            
		</div>
        
        
	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
            <div class="col-md-6 rightborder pl-0">
                <a class="text-dark" href="/blog/2008/01/31/Retired-Comedians-and-Missed-Opportunities/"> Retired Comedians and Missed Opportunities
                
                </a>
            </div>
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/blog/2008/02/19/Every-time-you-perform-arithmetic-operations-on-ordinal-numbers-God-kills-a-kitten/"> “Every time you perform arithmetic operations on ordinal numbers, God kills a kitten”
                
                </a>
            </div>
          
        </div>
    </div>
</div>

    </main>

    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
    <script src="/assets/theme-09a16a9d9cd3e853dee2c3a11353acf3ccf7b0f30b67e0a8d99dd7c987442f11.js" type="text/javascript"></script>

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Markerbench</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>
            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://www.wowthemes.net/mundana-jekyll-theme/"> Mundana Jekyll Theme </a> by <a class="text-dark" target="_blank" href="https://www.wowthemes.net">WowThemes</a>.
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
